<!DOCTYPE html>
<html lang="en-US"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
      <meta charset="UTF-8">

      <meta name="description" content="I’m working on the Roc Toolkit open-source project, a development kit for realtime audio streaming over the network. You can read more about the project in these two articles: 1, 2.
We decided to implement a set of PulseAudio modules that will allow PulseAudio to use Roc as a network transport. Many Linux distros employ …
">
      <meta name="author" content="Victor Gaydov">

      <meta property="og:url" content="http://gavv.net/articles/pulseaudio-under-the-hood/">
      <meta property="og:title" content="
  
    PulseAudio under the hood
  
">
      <meta property="og:description" content="I’m working on the Roc Toolkit open-source project, a development kit for realtime audio streaming over the network. You can read more about the project in these two articles: 1, 2.
We decided to implement a set of PulseAudio modules that will allow PulseAudio to use Roc as a network transport. Many Linux distros employ …
">
      <meta property="og:site_name" content="Victor Gaydov">

      <link rel="canonical" href="https://gavv.net/articles/pulseaudio-under-the-hood/">

      <title>
  
    PulseAudio under the hood
  
</title>

      <meta name="viewport" content="initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="PulseAudio%20under%20the%20hood_files/main.css">
<link rel="stylesheet" href="PulseAudio%20under%20the%20hood_files/prism.css">
<link rel="stylesheet" href="PulseAudio%20under%20the%20hood_files/lightbox.min.css">

<link rel="shortcut icon" href="https://gavv.net/images/favicon.ico">

<link rel="stylesheet" href="PulseAudio%20under%20the%20hood_files/all.min.css">

<link rel="stylesheet" href="PulseAudio%20under%20the%20hood_files/css.css">

<script src="PulseAudio%20under%20the%20hood_files/lightbox-plus-jquery.min.js">
</script>

<script>
    lightbox.option({
      'wrapAround': true,
      'resizeDuration': 0,
      'fadeDuration': 0,
      'imageFadeDuration': 0,
    })
</script>

<script src="PulseAudio%20under%20the%20hood_files/prism.js">
</script>

<script>
var width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
if (width < 880) {
  document.write('\x3Cscript src="/js/hyphenator-loader.js"><\/script>');
}
</script>

    <script src="PulseAudio%20under%20the%20hood_files/embed.js" data-timestamp="1749483286905"></script><script async="" id="dsq_recs_scr" src="PulseAudio%20under%20the%20hood_files/recommendations.js"></script></head>
    <body>
      <header id="header" class="no-print">
  <div id="header-panel">
    <div id="caption">
      Victor Gaydov
    </div>
    <div id="menu">
      <div class="menu-button">
        
          <a href="https://gavv.net/about/" class="menu-item">
        
            About
          </a>
      </div>
      <div class="menu-button">
        
          <a href="https://gavv.net/projects/" class="menu-item">
        
            Projects
          </a>
      </div>
      <div class="menu-button">
        
          <a href="https://gavv.net/articles/" class="menu-item menu-selected">
        
            Articles
          </a>
      </div>
    </div>
    <div id="tray">
      <div>
        <a href="https://github.com/gavv" class="tray-button">
          <i class="fa-brands fa-github"></i>
        </a>
        <a href="https://fosstodon.org/@gavv" class="tray-button">
          <i class="fa-brands fa-mastodon"></i>
        </a>
        <a href="https://gavv.net/articles/index.xml" class="tray-button">
          <i class="rss-button fa fa-rss"></i>
        </a>
      </div>
    </div>
  </div>
</header>

      <div id="content">
        
  <section role="main">
    <article>
      
        
  <header class="post-header">
    <h1 class="post-title">
      
        PulseAudio under the hood
      
    </h1>
    <div class="post-time">
      <time datetime="2017-09-21">
        21 Sep 2017
      </time>
    </div>
    <div class="post-tags">
      
      <div class="post-tag">
        <a href="https://gavv.net/tags/linux">linux</a>
      </div>
      
      <div class="post-tag">
        <a href="https://gavv.net/tags/audio">audio</a>
      </div>
      
      <div class="post-tag">
        <a href="https://gavv.net/tags/networking">networking</a>
      </div>
      
    </div>
  </header>


      
      <section>
        <img src="PulseAudio%20under%20the%20hood_files/features.png" width="480px">
<p><b>Table of contents</b></p>
<nav id="TableOfContents">
  <ul>
    <li><a href="#preface">Preface</a></li>
    <li><a href="#about-pulseaudio">About PulseAudio</a></li>
    <li><a href="#high-level-components">High-level components</a></li>
    <li><a href="#key-abstractions">Key abstractions</a></li>
    <li><a href="#d-bus-api">D-Bus API</a></li>
    <li><a href="#c-api">C API</a></li>
    <li><a href="#protocols-and-networking">Protocols and networking</a></li>
    <li><a href="#device-drivers">Device drivers</a></li>
    <li><a href="#sound-processing">Sound processing</a></li>
    <li><a href="#sample-cache">Sample cache</a></li>
    <li><a href="#stream-management">Stream management</a></li>
    <li><a href="#time-management">Time management</a></li>
    <li><a href="#power-saving">Power saving</a></li>
    <li><a href="#automatic-setup-and-routing">Automatic setup and routing</a></li>
    <li><a href="#desktop-integrations">Desktop integrations</a></li>
    <li><a href="#compatibility-layers">Compatibility layers</a></li>
    <li><a href="#server-internals">Server internals</a></li>
    <li><a href="#module-list">Module list</a></li>
    <li><a href="#gui-tools">GUI tools</a></li>
    <li><a href="#command-line-tools">Command line tools</a></li>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#portability">Portability</a></li>
    <li><a href="#example-setups">Example setups</a></li>
    <li><a href="#example-clients-and-modules">Example clients and modules</a></li>
    <li><a href="#critique">Critique</a></li>
  </ul>
</nav>
<hr>
<h1 id="preface">
  Preface
  <a class="anchor" href="#preface">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>I’m working on the <a href="https://roc-streaming.org/">Roc Toolkit</a>
 open-source project, a development kit for realtime audio streaming 
over the network. You can read more about the project in these two 
articles: <a href="https://gavv.net/articles/new-network-transport/">1</a>, <a href="https://gavv.net/articles/roc-tutorial/">2</a>.</p>
<p>We decided to implement a set of PulseAudio modules that will allow 
PulseAudio to use Roc as a network transport. Many Linux distros employ 
PulseAudio, and their users will be able to improve network service 
quality without changing the workflow. This led me to dig into 
PulseAudio internals and eventually to this document.</p>
<h2 id="motivation">
  Motivation
  <a class="anchor" href="#motivation">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio has <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/">Documentation</a> page covering many specific problems that may be encountered by user and developer. <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/Modules/">Modules</a> page contains a complete list of existing modules with parameters. <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/DBus/">D-Bus API</a> and <a href="https://freedesktop.org/software/pulseaudio/doxygen/index.html">C API</a> are also documented well.</p>
<p>Unfortunately, the available documentation doesn’t give a bird-eye 
view and an explanation of PulseAudio features and design and doesn’t 
cover many implementation details.</p>
<p>In result, the overall picture remains unclear. Advanced 
configuration looks mysterious because one need to understand what 
happens under the hood first. The learning curve for the module writer 
is high too.</p>
<p>This document tries to fill the gap and provide an overview of the 
PulseAudio features, architecture, and internals. More precisely, it has
 three goals:</p>
<ul>
<li>describe the available features</li>
<li>explain their underlying design and important implementation details</li>
<li>provide a starting point for writing clients and server modules</li>
</ul>
<p>It does not provide a detailed reference or tutorial for PulseAudio 
configuration and APIs. Further details can be obtained from the 
official documentation (for configuration and client APIs) and from the 
source code (for internal interfaces).</p>
<h2 id="disclaimer">
  Disclaimer
  <a class="anchor" href="#disclaimer">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>I’m not a PulseAudio developer. This document reflects my personal 
understanding of PulseAudio, obtained from the source code, experiments,
 official wiki, mailing lists, and blog articles. It may be inaccurate. 
Please let me know about any issues.</p>
<p>PulseAudio tends to trigger flame wars, which I believe are 
non-constructive. This document tries to be neutral and provide an 
unbiased overview of the implemented features and design.</p>
<h2 id="thanks">
  Thanks
  <a class="anchor" href="#thanks">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>I’d like to thank my friends and colleagues <a href="https://medium.com/@baranov.mv">Mikhail Baranov</a> and <a href="https://dshil.github.io/">Dmitriy Shilin</a> who read early drafts of the document and provided a valuable feedback.</p>
<p>Also big thanks to <a href="https://www.patreon.com/tanuk">Tanu Kaskinen</a>, a PulseAudio maintainer, who have found and helped to fix dozens of errors.</p>
<p>They all definitely made it better!</p>
<h2 id="meta">
  Meta
  <a class="anchor" href="#meta">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>This document was last updated for PulseAudio 11.1.</p>
<p>Last major update: 21 Oct 2017.</p>
<hr>
<h1 id="about-pulseaudio">
  About PulseAudio
  <a class="anchor" href="#about-pulseaudio">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p><a href="https://www.freedesktop.org/wiki/Software/PulseAudio/">PulseAudio</a>
 is a sound server for POSIX OSes (mostly aiming Linux) acting as a 
proxy and router between hardware device drivers and applications on 
single or multiple hosts.</p>
<p>See details on the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/About/">About</a> page on wiki.</p>
<h2 id="design-goals">
  Design goals
  <a class="anchor" href="#design-goals">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio is designed to meet a number of goals.</p>
<ul>
<li>
<p><strong>Abstraction layer for desktop audio</strong></p>
<p>PulseAudio manages all audio applications, local and network streams,
 devices, filters, and audio I/O. It provides an abstraction layer that 
combines all this stuff together in one place.</p>
</li>
<li>
<p><strong>Programmable behavior</strong></p>
<p>A rich API provides methods for inspecting and controlling all 
available objects and their both persistent and run-time properties. 
This makes it possible to replace configuration files with GUI tools. 
Many desktop environments provide such tools.</p>
</li>
<li>
<p><strong>Automatic setup</strong></p>
<p>PulseAudio is designed to work out of the box. It automatically 
detects and configures local devices and sound servers available in the 
local network. It also implements numerous policies for automatic audio 
management and routing.</p>
</li>
<li>
<p><strong>Flexibility</strong></p>
<p>PulseAudio provides a high flexibility for the user. It’s possible to
 connect any stream of any application to any local or remote device, 
configure per-stream and per-device volumes, construct sound processing 
chains, and more.</p>
</li>
<li>
<p><strong>Extensibility</strong></p>
<p>PulseAudio provides a framework for server extensions, and many 
built-in features are implemented as modules. Non-official third-party 
modules exist as well, however, the upstream doesn’t provide a guarantee
 of a stable API for out-of-tree modules.</p>
</li>
</ul>
<h2 id="feature-overview">
  Feature overview
  <a class="anchor" href="#feature-overview">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The following list gives an idea of the features implemented in PulseAudio.</p>
<ul>
<li>
<p><strong>Protocols and networking</strong></p>
<p>PulseAudio supports a variety of network protocols to communicate with clients, remote servers, and third-party software.</p>
</li>
<li>
<p><strong>Device drivers</strong></p>
<p>PulseAudio supports several backends to interact with hardware 
devices and controls. It supports hotplug and automatically configures 
new devices.</p>
</li>
<li>
<p><strong>Sound processing</strong></p>
<p>PulseAudio implements various sound processing tools, like mixing, 
sample rate conversion, and acoustic echo cancellation, which may be 
employed manually or automatically.</p>
</li>
<li>
<p><strong>Sample cache</strong></p>
<p>PulseAudio implements an in-memory storage for short named batches of
 samples that may be uploaded to the server once and then played 
multiple times.</p>
</li>
<li>
<p><strong>Stream management</strong></p>
<p>PulseAudio manages all input and output streams of all desktop 
applications, providing them such features as clocking, buffering, and 
rewinding.</p>
</li>
<li>
<p><strong>Time management</strong></p>
<p>PulseAudio implements a per-device timer-based scheduler that 
provides clocking in the sound card domain, maintains optimal latency, 
and reduces the probability of playback glitches.</p>
</li>
<li>
<p><strong>Power saving</strong></p>
<p>PulseAudio employs several techniques to reduce CPU and battery usage.</p>
</li>
<li>
<p><strong>Automatic setup and routing</strong></p>
<p>PulseAudio automatically sets parameters of cards, devices, and 
streams, routes streams to devices, and performs other housekeeping 
actions.</p>
</li>
<li>
<p><strong>Desktop integrations</strong></p>
<p>PulseAudio implements several features that integrate it into the desktop environment.</p>
</li>
<li>
<p><strong>Compatibility layers</strong></p>
<p>There are several compatibility layers with other sound systems, so 
that existing applications may automatically run on top of PulseAudio 
without modification.</p>
</li>
</ul>
<h2 id="use-cases">
  Use cases
  <a class="anchor" href="#use-cases">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Here are some practical examples of how PulseAudio features may be used on the desktop:</p>
<ul>
<li>
<p>Smart hotplug handling. For example, automatically setup Bluetooth or
 USB headset when it’s connected, or automatically switch to headphones 
when they’re inserted into the jack.</p>
</li>
<li>
<p>A GUI for easy switching an audio card between various modes like stereo, surround, or S/PDIF.</p>
</li>
<li>
<p>A GUI for easy switching an audio stream to any available audio 
device, like internal speakers, wired headphones, Bluetooth headset, or 
HDMI output.</p>
</li>
<li>
<p>A GUI for making a single application louder than others, or muting 
it, and remembering this decision when the application will appear next 
time.</p>
</li>
<li>
<p>A GUI for routing audio to a remote device available in LAN. For 
example, connecting a browser playing music on a laptop to speakers 
attached to a Raspberry Pi.</p>
</li>
<li>
<p>Automatically routing music or voice from a Bluetooth player or mobile phone to a sound card or Bluetooth speakers or headset.</p>
</li>
<li>
<p>Transparently adding various sound processing tools to a running 
application, for example adding acoustic echo cancellation to a VoIP 
client.</p>
</li>
<li>
<p>Reducing CPU and battery usage by automatically adjusting latency on 
the fly to a maximum value acceptable for currently running 
applications, and by disabling currently unnecessary sound processing 
like resampling.</p>
</li>
<li>
<p>Smart I/O scheduling, which may combine a high latency for playback 
(to avoid glitches and reduce CPU usage) and a low latency for user 
actions like volume changes (to provide smoother user experience).</p>
</li>
<li>
<p>Automatically integrating existing desktop applications into PulseAudio workflow, even if they are not aware of PulseAudio.</p>
</li>
</ul>
<h2 id="problems-and-drawbacks">
  Problems and drawbacks
  <a class="anchor" href="#problems-and-drawbacks">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are several known disadvantages of using PulseAudio, including 
both fundamental issues, and implementation issues that may be resolved 
in the future:</p>
<ul>
<li>additional complexity, overhead, and bugs (more code always means more bugs)</li>
<li>lack of comprehensive documentation</li>
<li>non-intuitive command line tools and configuration</li>
<li>weird features like autospawn and built-in watchdog</li>
<li>higher minimum possible latency</li>
<li>poor quality of service over an unreliable network like 802.11 (WiFi)</li>
<li>no hardware mixing and resampling</li>
<li>no hardware volumes when using ALSA UCM</li>
</ul>
<hr>
<h1 id="high-level-components">
  High-level components
  <a class="anchor" href="#high-level-components">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>The diagram below demonstrates a simplified view of an example PulseAudio setup.</p>
<p>It shows three clients (employing three different APIs), one local 
PulseAudio server, two remote PulseAudio servers (connected via “native”
 and RTP protocols), one remote RTP receiver, ALSA backend, and a set of
 modules required to serve this setup.</p>
<img src="PulseAudio%20under%20the%20hood_files/components.png">
<p>The diagram shows most important PulseAudio components:</p>
<ul>
<li>
<p><strong>libpulse-simple</strong></p>
<p>Client library.</p>
<p>Provides “Simple API” for applications. Implemented as a wrapper around libpulse.</p>
</li>
<li>
<p><strong>libpulse</strong></p>
<p>Client and server library.</p>
<p>Provides “Asynchronous API” for applications. Communicates with the 
server via the “native” protocol over a Unix domain or TCP stream 
socket.</p>
<p>Contains only definitions and code that are part of public API. The 
server also reuses definitions and some code from this library 
internally.</p>
</li>
<li>
<p><strong>libpulsecommon</strong></p>
<p>Client and server library.</p>
<p>Contains parts from libpulsecore which are needed on both client and 
server but can’t be included into libpulse because they are not part of 
public API. For technical reasons, it also contains parts of libpulse.</p>
</li>
<li>
<p><strong>libpulsecore</strong></p>
<p>Server library.</p>
<p>Provides internal API for modules. Contains common environment and generic building blocks for modules.</p>
</li>
<li>
<p><strong>modules</strong></p>
<p>Server extensions.</p>
<p>Many server features are implemented in modules, including network protocols, device drivers, desktop integrations, etc.</p>
</li>
</ul>
<hr>
<h1 id="key-abstractions">
  Key abstractions
  <a class="anchor" href="#key-abstractions">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>This sections discusses the key server-side object types.</p>
<h2 id="devices-and-streams">
  Devices and streams
  <a class="anchor" href="#devices-and-streams">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio is built around devices (sources and sinks) connected to 
streams (source outputs and sink inputs). The diagram below illustrates 
these connections.</p>
<img src="PulseAudio%20under%20the%20hood_files/devices_and_streams.png" width="640px">
<ul>
<li>
<p><strong>Source</strong></p>
<p>A source is an input device. It is an active unit that produces samples.</p>
<p>Source usually runs a thread with its own event loop, generates 
sample chunks, and posts them to all connected source outputs. It also 
implements clocking and maintains latency. The rest of the world usually
 communicates with a source using messages.</p>
<p>The typical source represents an input sound device, e.g. a 
microphone connected to a sound card line input or on a Bluetooth 
headset. PulseAudio automatically creates a source for every detected 
input device.</p>
</li>
<li>
<p><strong>Source output</strong></p>
<p>A source output is a recording stream. It is a passive unit that is connected to a source and consumes samples from it.</p>
<p>The source thread invokes source output when next sample chunk is 
available or parameters are updated. If the source and source output use
 different audio formats, source output automatically converts sample 
format, sample rate, and channel map.</p>
<p>The typical source output represents a recording stream opened by an 
application. PulseAudio automatically creates a source output for every 
opened recording stream.</p>
</li>
<li>
<p><strong>Sink</strong></p>
<p>A sink is an output device. It is an active unit that consumes samples.</p>
<p>Sink usually runs a thread with its own event loop, peeks sample 
chunks from connected sink inputs, and mixes them. It also implements 
clocking and maintains latency. The rest of the world usually 
communicates with a sink using messages.</p>
<p>The typical sink represents an output sound device, e.g. headphones 
connected to a sound card line output or on a Bluetooth headset. 
PulseAudio automatically creates a sink for every detected output 
device.</p>
</li>
<li>
<p><strong>Sink input</strong></p>
<p>A sink input is a playback stream. It is a passive unit that is connected to a sink and produces samples for it.</p>
<p>The sink thread invokes sink input when next sample chunk is needed 
or parameters are updated. If sink and sink input use different audio 
formats, sink input automatically converts sample format, sample rate, 
and channel map.</p>
<p>The typical sink input represents a playback stream opened by an 
application. PulseAudio automatically creates a sink input for every 
opened playback stream.</p>
</li>
</ul>
<h2 id="object-hierarchy">
  Object hierarchy
  <a class="anchor" href="#object-hierarchy">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The diagram below shows the hierarchy of the server-side objects.</p>
<img src="PulseAudio%20under%20the%20hood_files/object_hierarchy.png" width="610px">
<ul>
<li>
<p><strong>Core</strong></p>
<p>The core provides a shared environment for modules. Modules use it to
 find and register objects, install hooks for various events, register 
event loop handlers, etc.</p>
<p>There is only one core instance which is created at startup.</p>
</li>
<li>
<p><strong>Module</strong></p>
<p>A module represents a loadable server extension.</p>
<p>Modules usually implement and register other objects. A module can be
 loaded multiple times with different parameters and so have multiple 
instances.</p>
<p>The typical module implements a network or device discovery, a 
network or hardware device or stream, or a sound processing device. For 
example, PulseAudio loads a new instance of the module-alsa-card for 
every detected ALSA card.</p>
</li>
<li>
<p><strong>Client</strong></p>
<p>A client represents an application connected to the PulseAudio server.</p>
<p>It contains lists of playback and recording streams.</p>
<p>The typical client represents a local application, e.g. a media 
player, or a remote PulseAudio server. PulseAudio automatically creates a
 client for every incoming connection.</p>
</li>
<li>
<p><strong>Card</strong></p>
<p>A card represents a physical audio device, like a sound card or Bluetooth device.</p>
<p>It contains card profiles, device ports, and devices (sources and 
sinks) connected to device ports. It also has a single active card 
profile.</p>
<p>The typical card represents ALSA card or Bluetooth device. PulseAudio
 automatically creates a card for every detected physical device.</p>
</li>
<li>
<p><strong>Card profile</strong></p>
<p>A card profile represents an opaque configuration set for a card.</p>
<p>It defines the backend-specific configuration of the card, and the 
list of currently available devices (sources and sinks) and device 
ports. Only one card profile of a card may be active at the same time. 
The user can switch the active card profile at any time.</p>
<p>The typical card profile represents the sound card mode, e.g. analog 
and digital output, and mono, stereo, and surround mode. PulseAudio 
automatically creates a card profile for every available operation mode 
of a card.</p>
</li>
<li>
<p><strong>Device port</strong></p>
<p>A device port represents a single input or output port on the card.</p>
<p>A single card may have multiple device ports. Different device ports 
of a single card may be used simultaneously via different devices 
(sources and sinks).</p>
<p>The typical device port represents a physical port on a card, or a 
combination of a physical port plus its logical parameters, e.g. one 
output port for internal laptop speakers, and another output port for 
headphones connected via a line out. PulseAudio automatically creates 
device ports for every detected card, depending on the currently active 
card profile.</p>
</li>
<li>
<p><strong>Device</strong></p>
<p>A device represents an active sample producer (input device) or consumer (output device).</p>
<p>A device can have an arbitrary number of streams connected to it. A 
recording stream (source output) can be connected to an input device 
(source). A playback stream (sink input) can be connected to an output 
device (sink).</p>
<p>There are three kinds of devices:</p>
<ul>
<li>
<p><strong>Hardware device</strong></p>
<p>Hardware source or sink is associated with an audio device. Usually, 
it is explicitly associated with a card object, except those limited 
backends that don’t create card objects.</p>
<p>Such sources and sinks contain a subset of device ports provided by 
the device and have a single active device port, from which they will 
read or write samples. The user can switch the active device port of a 
source or sink at any time.</p>
<p>PulseAudio automatically creates one or several pairs of a hardware 
source and sink for every detected card, depending on the currently 
active card profile.</p>
</li>
<li>
<p><strong>Virtual device</strong></p>
<p>Virtual source or sink is not associated with an audio device. It may
 represent a remote network device, a sound processing filter, or 
anything else, depending on the implementation.</p>
<p>PulseAudio may automatically create a pair of virtual source and sink
 for every remote sound card exported by every PulseAudio server in the 
local network.</p>
</li>
<li>
<p><strong>Monitor device</strong></p>
<p>Sink monitor is a special kind of virtual source associated with a sink.</p>
<p>Every sink automatically gets a sink monitor, named as 
“&lt;sink_name&gt;.monitor”. Every time when the sink reads a chunk from
 its sink inputs, it also writes this chunk to the sink monitor.</p>
<p>Typical usage of the sink monitor is capturing all sound that was 
sent to speakers and duplicating it somewhere else. PulseAudio 
automatically creates a sink monitor for every sink.</p>
</li>
</ul>
</li>
<li>
<p><strong>Stream</strong></p>
<p>A stream represents a passive sample consumer (recording stream) or producer (playback stream).</p>
<p>Every stream should be connected to some device. A recording stream 
(source output) should be connected to an input device (source). A 
playback stream (sink input) should be connected to an output device 
(sink).</p>
<p>There are two kinds of streams:</p>
<ul>
<li>
<p><strong>Application stream</strong></p>
<p>An application stream is associated with a client. It is created when
 an application connected to PulseAudio server starts playback or 
recording.</p>
</li>
<li>
<p><strong>Virtual stream</strong></p>
<p>A virtual stream is not associated with a client. It may represent a 
remote network server or anything else, depending on the implementation.</p>
</li>
</ul>
</li>
<li>
<p><strong>Sample cache</strong></p>
<p>The sample cache is an in-memory storage for short named batches of 
samples that may be uploaded to the server once and then played multiple
 times. It is usually used for event sounds.</p>
</li>
</ul>
<hr>
<h1 id="d-bus-api">
  D-Bus API
  <a class="anchor" href="#d-bus-api">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-dbus-protocol</div>
</div>
<p>PulseAudio server-side objects may be inspected and controlled via an
 experimental D-Bus API. Note that it can’t be used for playback and 
recording. These features are available only through the C API.</p>
<p>Unfortunately, the D-Bus API has never left the experimental stage, 
and it has no stability guarantees and is not actively maintained. 
Applications are generally advised to use the C API instead.</p>
<h2 id="buses-and-services">
  Buses and services
  <a class="anchor" href="#buses-and-services">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>D-Bus has several modes of communication:</p>
<ul>
<li>via a system bus (system-wide)</li>
<li>via a session bus (one bus per login session)</li>
<li>peer-to-peer (direct communication between applications)</li>
</ul>
<p>PulseAudio implements several D-Bus services:</p>
<ul>
<li>Device reservation API (on session bus)</li>
<li>Server lookup API (on session bus)</li>
<li>Server API (peer-to-peer)</li>
<li>Server API extensions (peer-to-peer)</li>
</ul>
<h2 id="device-reservation-api">
  Device reservation API
  <a class="anchor" href="#device-reservation-api">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p><a href="http://git.0pointer.net/reserve.git/tree/reserve.txt">Device reservation API</a>
 provides methods for coordinating access to audio devices, typically 
ALSA or OSS devices. It is used to ensure that nobody else is using the 
device at the same time.</p>
<p>If an application needs to use a device directly (bypassing 
PulseAudio), it should first acquire exclusive access to the device. 
When access is acquired, the application may use the device until it 
receives a signal indicating that exclusive access has been revoked.</p>
<p>This API is designed to be generic. It is a small standalone D-Bus 
interface with no dependencies on PulseAudio abstractions, so it may be 
easily implemented by other software.</p>
<h2 id="server-lookup-api">
  Server lookup API
  <a class="anchor" href="#server-lookup-api">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio server API uses peer-to-peer D-Bus mode. In this mode, 
clients communicate directly with the server instead of using a session 
bus, which acts as a proxy. In contrast to the session bus mode, this 
mode permits remote access and has lower latency. However, clients need a
 way to determine the server address before connecting to it.</p>
<p>To solve this problem, PulseAudio server registers <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/DBus/ConnectingToServer/">server lookup interface</a>
 on the session bus. A client should first connect to the session bus in
 order to discover PulseAudio server address and then connect to 
PulseAudio server directly for peer-to-peer communication.</p>
<h2 id="server-api">
  Server API
  <a class="anchor" href="#server-api">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p><a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/DBus/">Server API</a> is available through a peer-to-peer connection to PulseAudio server.</p>
<p>Every object in the hierarchy is identified by a unique path. The 
hierarchy starts with the core object, which has a well-known path. It 
may be used to discover all other objects.</p>
<p>The diagram below shows the most important D-Bus interfaces.</p>
<img src="PulseAudio%20under%20the%20hood_files/dbus.png" width="600px">
<ul>
<li>
<p><strong>Core</strong> - A top-level interface that provides access to all other interfaces.</p>
</li>
<li>
<p><strong>Module</strong> - A loadable server extension.</p>
</li>
<li>
<p><strong>Client</strong> - An application connected to the PulseAudio server.</p>
</li>
<li>
<p><strong>Card</strong> - A physical audio device, like a sound card or Bluetooth device.</p>
</li>
<li>
<p><strong>CardProfile</strong> - An opaque configuration set for a card.</p>
</li>
<li>
<p><strong>DevicePort</strong> - A single input or output port on the card.</p>
</li>
<li>
<p><strong>Device</strong> - The parent interface for Source and Sink.</p>
</li>
<li>
<p><strong>Source</strong> - An input device. May be associated with a card and device port.</p>
</li>
<li>
<p><strong>Sink</strong> - An output device. May be associated with a card and device port.</p>
</li>
<li>
<p><strong>Stream</strong> - May be either a recording stream (source 
output) or playback stream (sink input). Application stream is 
associated with a client.</p>
</li>
<li>
<p><strong>Sample</strong> - A named batch of samples in the sample cache.</p>
</li>
</ul>
<p>In addition to the core interface, PulseAudio modules can register 
custom server API extensions, that are also discoverable through the 
core. Several extensions are available out of the box:</p>
<ul>
<li>
<p><strong>StreamRestore</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-stream-restore</div>
  </div>
<p>Query and modify the database used to store device and stream parameters.</p>
</li>
<li>
<p><strong>Equalizer</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-equalizer-sink</div>
  </div>
<p>Query and modify equalizer sink levers.</p>
</li>
<li>
<p><strong>Ladspa</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-ladspa-sink</div>
  </div>
<p>Query and modify LADSPA sink control ports.</p>
</li>
</ul>
<hr>
<h1 id="c-api">
  C API
  <a class="anchor" href="#c-api">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>PulseAudio provides <a href="https://freedesktop.org/software/pulseaudio/doxygen/index.html">C API</a> for client applications.</p>
<p>The API is implemented in the libpulse and libpulse-simple libraries,
 which communicate with the server via the “native” protocol. There are 
also official bindings for Vala and third-party bindings for other 
languages.</p>
<p>C API is a superset of the D-Bus API. It’s mainly asynchronous, so 
it’s more complex and harder to use. In addition to inspecting and 
controlling the server, it supports recording and playback.</p>
<p>The API is divided into two alternative parts:</p>
<ul>
<li>Asynchronous API (libpulse), complicated but complete</li>
<li>Simple API (libpulse-simple), a simplified synchronous wrapper for the recording and playback subset of the asynchronous API</li>
</ul>
<h2 id="asynchronous-api">
  Asynchronous API
  <a class="anchor" href="#asynchronous-api">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulse</div>
</div>
<p><a href="https://freedesktop.org/software/pulseaudio/doxygen/async.html">Asynchronous API</a> is based on event loop and callbacks.</p>
<p>The diagram below demonstrates the workflow.</p>
<img src="PulseAudio%20under%20the%20hood_files/async_api.png" width="410px">
<ul>
<li>
<p><strong>Main Loop</strong></p>
<p>The first step is creating an instance of one of the available <a href="https://freedesktop.org/software/pulseaudio/doxygen/async.html#mainloop_sec">Main Loop</a> implementations. They differ in a way how the user can run the main loop: directly, in a separate thread, or using Glib.</p>
<p>All communications with the server happen inside the main loop. The 
user should run main loop iterations from time to time. The user can 
either register callbacks that are invoked from the main loop or use 
polling.</p>
<p>For a regular main loop, polling may be performed between iterations.
 For a threaded main loop, polling may be performed after obtaining a 
lock from another thread.</p>
</li>
<li>
<p><strong>Context</strong></p>
<p>The second step is creating a <a href="https://freedesktop.org/software/pulseaudio/doxygen/async.html#context_sec">Context</a> object that represents a connection to the server. The user can set callbacks for context state updates.</p>
</li>
<li>
<p><strong>Stream</strong></p>
<p>When context state becomes ready, the user can create one or multiple <a href="https://freedesktop.org/software/pulseaudio/doxygen/streams.html">Stream</a> objects for playback or recording.</p>
<p>The user can set callbacks for stream state updates and I/O events, 
invoked when the server wants to send or receive more samples. Clocking 
is controlled by the server. Stream also provides several management 
functions, like pause, resume, and volume control.</p>
</li>
<li>
<p><strong>Sample Cache</strong></p>
<p>In addition to regular streams, the user can also use <a href="https://freedesktop.org/software/pulseaudio/doxygen/scache.html">Sample Cache</a> to upload named batches of samples to the server without playing them and start playback later.</p>
</li>
<li>
<p><strong>Introspection</strong></p>
<p>Having a context in a ready state, <a href="https://freedesktop.org/software/pulseaudio/doxygen/introspect.html">Server Query and Control</a>
 API may be used to query and modify various objects on the server. All 
operations are asynchronous. The object hierarchy is similar to D-Bus 
API described above.</p>
</li>
<li>
<p><strong>Property Lists</strong></p>
<p>Every server-side object has a <a href="https://freedesktop.org/software/pulseaudio/doxygen/proplist_8h.html">Property List</a>,
 a map with textual keys and arbitrary textual or binary values. 
Applications and modules may get and set these properties. Various 
modules implement automatic actions based on some properties, like 
routing, volume setup, and autoloading filters.</p>
<p>The typical usage in applications is to provide a property list when 
creating a context (for client properties) and when creating a stream 
(for stream properties). Higher-level frameworks that use PulseAudio 
(like GStreamer) usually do it automatically.</p>
</li>
<li>
<p><strong>Operations</strong></p>
<p>All operations with server-side objects are asynchronous. Many API calls return an <a href="https://freedesktop.org/software/pulseaudio/doxygen/operation_8h.html">Operation</a>
 object which represents an asynchronous request. It may be used to poll
 the request status, set completion callback, or cancel the request.</p>
</li>
<li>
<p><strong>Events</strong></p>
<p>The client can receive two types of events from server:</p>
<ul>
<li>
<p>subscription events</p>
<p><a href="https://freedesktop.org/software/pulseaudio/doxygen/subscribe.html">Events API</a>
 provides methods for subscribing events triggered for the server-side 
objects available through the introspection API. Every event has an 
integer type and arbitrary binary payload.</p>
</li>
<li>
<p>stream events</p>
<p><a href="https://freedesktop.org/software/pulseaudio/doxygen/stream_8h.html#a5690ed098466233860e632abfa61fe50">Stream Events</a>
 are generated to acknowledge the client of the stream state change or 
ask it to do something, e.g. pause the stream. Such event has a textual 
name and arbitrary binary payload.</p>
</li>
</ul>
</li>
</ul>
<h2 id="simple-api">
  Simple API
  <a class="anchor" href="#simple-api">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulse-simple</div>
</div>
<p><a href="https://freedesktop.org/software/pulseaudio/doxygen/simple.html">Simple API</a>
 is a convenient wrapper around the threaded main loop and stream. The 
user just chooses parameters, connects to the server and writes or reads
 samples. All operations are blocking.</p>
<p>Limitations:</p>
<ul>
<li>only single stream per connection is supported</li>
<li>no support for volume control, channel mappings, and events</li>
</ul>
<hr>
<h1 id="protocols-and-networking">
  Protocols and networking
  <a class="anchor" href="#protocols-and-networking">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>PulseAudio server supports a variety of network protocols to 
communicate with clients, remote servers, and third-party software. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/Network/">Network</a> page on wiki.</p>
<p>PulseAudio implements two custom protocols:</p>
<ul>
<li>“native”, a full-featured protocol for most client-server and server-server communications</li>
<li>“simple”, which is rarely useful</li>
</ul>
<p>It also supports several foreign transport and discovery protocols:</p>
<ul>
<li>mDNS (Zeroconf)</li>
<li>RTP/SDP/SAP</li>
<li>RAOP</li>
<li>HTTP</li>
<li>DLNA and Chromecast</li>
<li>ESound</li>
</ul>
<p>And two control protocols:</p>
<ul>
<li>D-Bus API</li>
<li>CLI protocol</li>
</ul>
<h2 id="native-protocol">
  Native protocol
  <a class="anchor" href="#native-protocol">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-native-protocol-{fd,unix,tcp}</div>
</div>
<p>PulseAudio uses a so-called “native” protocol for client-server and 
server-server connections, which works over a Unix domain or TCP stream 
socket. It is a rich, binary, message-oriented, bidirectional, 
asynchronous protocol.</p>
<p>The Asynchronous API described above mostly mirrors the features provided by this protocol.</p>
<p>They are:</p>
<ul>
<li>authentication - provide authentication data for the server</li>
<li>streams - manage server-side stream state and exchange samples</li>
<li>sample cache - manage server-side sample storage</li>
<li>introspection - query and modify server-side objects</li>
<li>events - subscribe server-side object events</li>
<li>extensions - send custom commands to modules</li>
</ul>
<p>There are four message types, each with a header and an optional payload:</p>
<ul>
<li>
<p><strong>packet</strong></p>
<p>Control message. May contain a command (from client to server), a 
reply (from server to client), or an event (from server to client). Each
 message has its own payload type.</p>
</li>
<li>
<p><strong>memblock</strong></p>
<p>Data message. Contains a chunk of samples. In the zero-copy mode 
payload with samples is omitted and the message contains only a header.</p>
</li>
<li>
<p><strong>shmrelease, shmrevoke</strong></p>
<p>Shared pool management message. These messages are used to manage the shared memory pool employed in the zero-copy mode.</p>
</li>
</ul>
<p>With the “native” protocol, the client is clocked by the server. 
Server requests client to send some amount of samples from time to time.</p>
<p>Since the protocol uses stream sockets, it’s not real time. The 
delays introduced by the sender or network cause playback delays on the 
receiver.</p>
<h2 id="zero-copy-mode">
  Zero-copy mode
  <a class="anchor" href="#zero-copy-mode">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>When the “native” protocol is used for client and server on the same host, the zero-copy mode may be employed.</p>
<p>It requires Unix domain socket to be used, and <a href="http://man7.org/linux/man-pages/man3/shm_open.3.html">POSIX shared memory</a> or Linux-specific <a href="http://man7.org/linux/man-pages/man2/memfd_create.2.html">memfd</a>
 to be supported and enabled in PulseAudio. It also requires the server 
and client to run as the same user, for security reasons.</p>
<p>In this mode, chunks are allocated in a shared memory pool and 
communication is done through a shared ring buffer channel that uses a 
shared memory block for data and two file descriptor-based semaphores 
for notifications, on top of POSIX <a href="http://man7.org/linux/man-pages/man2/pipe.2.html">pipe</a> or Linux-specific <a href="http://man7.org/linux/man-pages/man2/eventfd.2.html">eventfd</a>.</p>
<p>To establish communication, the server should send to the client file
 descriptors of the shared memory region and semaphores. The algorithm 
is the following:</p>
<ul>
<li>the server creates an anonymous in-memory file using <code>shm_open</code> (POSIX) or <code>memfd_create</code> (Linux-specific)</li>
<li>the server maps the file to memory using <code>mmap</code> and initializes a memory pool there</li>
<li>the server allocates one block from the pool, initializes the shared ring buffer, and creates semaphores using <code>pipe</code> (POSIX) of <code>eventfd</code> (Linux-specific)</li>
<li>the server transfers file descriptors to the client via a Unix domain socket using <code>sendmsg</code>, which provides special API for this feature</li>
<li>the client receives file descriptors and also maps the file to memory using <code>mmap</code></li>
<li>the server and client now use single shared memory pool and ring buffer</li>
<li>to avoid races, the server and client use mutexes that are placed inside shared memory as well</li>
</ul>
<p>After this, all messages (<code>packet</code>, <code>memblock</code>, <code>shmrelease</code>, <code>shmrevoke</code>) are sent via the shared ring buffer.</p>
<p>When the <code>memblock</code> message is sent in the zero-copy mode,
 it’s payload is omitted. Since the server and client use the same 
shared memory pool, the chunk payload can be obtained from the memory 
pool using the chunk identifier in the chunk header and is not needed to
 be transmitted.</p>
<p>Two additional messages are used in this mode:</p>
<ul>
<li>the peer that received a chunk sends <code>shmrelease</code> when it finishes reading the chunk and wants to return it to the shared memory pool</li>
<li>the peer that has sent a chunk may send <code>shmrevoke</code> when it wants to cancel reading from the chunk</li>
</ul>
<p>To achieve true zero-copy when playing samples, an application should use the <a href="https://freedesktop.org/software/pulseaudio/doxygen/async.html">Asynchronous API</a> and <a href="https://freedesktop.org/software/pulseaudio/doxygen/stream_8h.html#a6cf50cfc4ea8897391941184d74d7dfa">delegate memory allocation</a> to the library. When the zero-copy mode is enabled, memory is automatically allocated from the shared pool.</p>
<h2 id="authentication">
  Authentication
  <a class="anchor" href="#authentication">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>When a client connects to the server via the “native” protocol, the 
server performs several authentication checks in the following order:</p>
<ol>
<li>
<p><strong>auth-anonymous</strong></p>
<p>If the <code>auth-anonymous</code> option is set, then the client is accepted.</p>
</li>
<li>
<p><strong>uid</strong></p>
<p>If a Unix domain socket is used, and the client has the same UID as 
the server, then the client is accepted. This check uses a feature of 
Unix domain sockets that provide a way to securely determine credentials
 of the other side.</p>
</li>
<li>
<p><strong>auth-group</strong></p>
<p>If a Unix domain socket is used, and the <code>auth-group</code> option is set, and the client belongs to the group specified by this option, then the client is accepted.</p>
</li>
<li>
<p><strong>auth-cookie</strong></p>
<p>If the <code>auth-cookie</code> option is set, and the client provided a correct authentication cookie, then the client is accepted.</p>
<p>On start, the server checks a cookie file, usually located at <code>"~/.config/pulse/cookie"</code>.
 If the file isn’t writable, the server reads a cookie from it. 
Otherwise, it generates a new random cookie and writes it to the file. 
Optionally, the server also stores the cookie into the X11 root window 
properties.</p>
<p>Client searches for a cookie in an environment variable, in the X11 
root window properties, in parameters provided by the application, and 
in the cookie file (in this order).</p>
</li>
<li>
<p><strong>auth-ip-acl</strong></p>
<p>If TCP socket is used, and the <code>auth-ip-acl</code> option is set, and client’s IP address belongs to the address whitelist specified in this option, then the client is accepted.</p>
</li>
<li>
<p><strong>reject</strong></p>
<p>If all checks have failed, then the client is rejected.</p>
</li>
</ol>
<h2 id="tunnels">
  Tunnels
  <a class="anchor" href="#tunnels">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-tunnel-{source,sink}</div>
  <div>module-tunnel-{source,sink}-new</div>
</div>
<p>Local applications may be connected with to audio devices using 
tunnel sources and sinks. The diagram below illustrates an example of 
such connections.</p>
<img src="PulseAudio%20under%20the%20hood_files/tunnels.png" width="826px">
<p>Each tunnel connects a single pair of a local device and remote stream:</p>
<ul>
<li>a local tunnel sink is connected to a remote sink input</li>
<li>a local tunnel source is connected to a remote source output</li>
</ul>
<p>Each tunnel acts as a regular PulseAudio client and connects to a 
remote PulseAudio server via the “native” protocol over TCP. Tunnel sink
 creates a playback stream, and tunnel source creates a recording 
stream.</p>
<p>Tunnel devices may be created either manually by the user or automatically if the Zeroconf support is enabled.</p>
<h2 id="mdns-zeroconf">
  mDNS (Zeroconf)
  <a class="anchor" href="#mdns-zeroconf">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-zeroconf-{publish,discover}</div>
  <div>module-bonjour-publish</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Multicast_DNS">mDNS</a> (multicast DNS) protocol, a part of the <a href="https://en.wikipedia.org/wiki/Zero-configuration_networking">Zeroconf</a> protocol stack, resolves names in the local network without using a name server.</p>
<p>PulseAudio may use <a href="https://en.wikipedia.org/wiki/Avahi_(software)">Avahi</a> (free Zeroconf implementation) or <a href="https://en.wikipedia.org/wiki/Bonjour_(software)">Bonjour</a>
 (Apple Zeroconf implementation). If Avahi or Bonjour daemon is running 
and the Zeroconf support is enabled in PulseAudio, every sink and source
 on every PulseAudio server in the local network automatically become 
available on all other PulseAudio servers.</p>
<p>To achieve this, PulseAudio uses automatically configured tunnels:</p>
<ul>
<li>
<p><strong>publishing</strong></p>
<p>PulseAudio server publishes every sink and source as an mDNS service.
 Each published entry contains the server address, device name and type,
 and audio parameters, like sample rate and channel map.</p>
<p>Publishing is implemented for both Avahi (module-zeroconf-publish) and Bonjour (module-bonjour-publish).</p>
</li>
<li>
<p><strong>discovery</strong></p>
<p>PulseAudio server monitors services published on the local network. 
For every detected service, PulseAudio server creates a tunnel sink or 
source connected to the remote device and configured with the parameters
 of that device.</p>
<p>Discovery is implemented only for Avahi (module-zeroconf-discover).</p>
</li>
</ul>
<h2 id="rtpsdpsap">
  RTP/SDP/SAP
  <a class="anchor" href="#rtpsdpsap">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-rtp-{send,recv}</div>
</div>
<p>PulseAudio also has the RTP support. Unlike the “native” PulseAudio 
tunnels, this technology supports multicasting of a single local source 
to any number of remote sinks.</p>
<p>To achieve this, three protocols are used:</p>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Real-time_Transport_Protocol"><strong>RTP</strong></a> (Real-time Transport Protocol)</p>
<p>A transport protocol for delivering audio and video over IP networks.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Session_Description_Protocol"><strong>SDP</strong></a> (Session Description Protocol)</p>
<p>A format for describing multimedia session parameters. Usually used to describe RTP sessions.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Session_Announcement_Protocol"><strong>SAP</strong></a> (Session Announcement Protocol)</p>
<p>A protocol for broadcasting multicast session information. Usually used to send SDP messages.</p>
</li>
</ul>
<p>PulseAudio implements both RTP sender and receiver. They may be used 
together, or with other software that supports RTP, for example VLC, 
GStreamer, FFMpeg, MPLayer, or SoX. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/Network/RTP/">RTP</a> page on wiki.</p>
<p>The diagram below shows an example workflow.</p>
<img src="PulseAudio%20under%20the%20hood_files/rtp.png" width="541px">
<ul>
<li>
<p><strong>RTP sender</strong></p>
<p>RTP sender creates an RTP source output.</p>
<p>Every RTP source output is connected to a single source and 
configured to send RTP packets to a single network address, usually a 
multicast one.</p>
<p>When the RTP source output is created, it broadcasts RTP session 
parameters to the local network using SDP/SAP. When source writes 
samples to RTP source output, source output sends them to preconfigured 
address via RTP. When RTP source output is destroyed, it broadcasts 
goodbye message using SDP/SAP.</p>
</li>
<li>
<p><strong>RTP receiver</strong></p>
<p>RTP receiver listens to SDP/SAP announcements in the local network.</p>
<p>When it receives an announcement for a new RTP session, it creates 
RTP sink input for it. When it receives goodbye message, it destroys the
 appropriate RTP sink input.</p>
<p>Every RTP sink input is connected to single sink and is configured to receive RTP packets from single RTP sender.</p>
<p>When RTP sink input receives an RTP packet, it stores it in the 
queue. When sink requests samples from RTP sink input, RTP sink input 
reads samples from that queue.</p>
</li>
</ul>
<p>RTP sender can’t be clocked by RTP receiver because the sender has no
 feedback from the receiver and there may be multiple receivers for a 
single multicast sender. Since sender and receiver clocks are always 
slightly different, the receiver queue size is slowly drifting. To avoid
 this, RTP receiver adjusts resampler rate on the fly so that samples 
are played a bit slower or faster depending on the queue size.</p>
<p>RTP is a real time protocol. The delays introduced by the sender or 
network cause playback holes on the receiver. Playback is never delayed 
and packets delivered too late are just dropped.</p>
<h2 id="raop-airplay">
  RAOP (AirPlay)
  <a class="anchor" href="#raop-airplay">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-raop-{discover,sink}</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Remote_Audio_Output_Protocol">RAOP</a> (Remote Audio Output Protocol) is a proprietary streaming protocol based on RTP and RTSP, used in Apple AirPlay devices. <a href="https://en.wikipedia.org/wiki/Real-time_Transport_Protocol">RTP</a> is a transport protocol, and <a href="https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol">RTSP</a> is a control protocol.</p>
<p>AirPlay devices use mDNS and are discoverable via Zeroconf. AirPlay 
uses AES encryption, but the RSA keys were extracted from Apple devices,
 and open-source RAOP implementations appeared.</p>
<p>Since version 11.0, PulseAudio has built-in support for RAOP2. 
PulseAudio uses Avahi to receive mDNS RAOP announcements. Every AirPlay 
device in the local network automatically becomes available in 
PulseAudio.</p>
<p>RAOP support consists of two parts:</p>
<ul>
<li>
<p><strong>discovery</strong></p>
<p>PulseAudio server monitors services published on the local network. 
For every detected service, PulseAudio server creates an RAOP sink.</p>
</li>
<li>
<p><strong>sink</strong></p>
<p>Every RAOP sink is connected to a single AirPlay device. It uses RTSP
 to negotiate session parameters and RTP to transmit samples.</p>
</li>
</ul>
<h2 id="http-support">
  HTTP support
  <a class="anchor" href="#http-support">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-http-protocol-{unix,tcp}</div>
</div>
<p>The HTTP support provides two features:</p>
<ul>
<li>
<p><strong>web interface</strong></p>
<p>A simple web interface provides a few bits of information about the server and server-side objects.</p>
</li>
<li>
<p><strong>streaming</strong></p>
<p>It’s possible to receive samples from sources and sink monitors via HTTP. This feature is used for DLNA support.</p>
<p>Every source or sink monitor has a dedicated HTTP endpoint.</p>
<p>When a new HTTP client connects to the endpoint, PulseAudio first 
sends the standard HTTP headers, including the “Content-Type” header 
with the MIME type corresponding to the sample format in use.</p>
<p>After sending headers, PulseAudio creates a new source output 
connected to the source or sink monitor which writes all new samples to 
the HTTP connection. Samples are sent as-is, without any additional 
encoding.</p>
</li>
</ul>
<h2 id="dlna-and-chromecast">
  DLNA and Chromecast
  <a class="anchor" href="#dlna-and-chromecast">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-rygel-media-server</div>
  <div>pulseaudio-dlna</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Digital_Living_Network_Alliance">DLNA</a>
 (Digital Living Network Alliance) is a set of interoperability 
guidelines for sharing digital media among multimedia devices. It 
employs numerous control and transport protocols, including UPnP, RTP, 
and custom HTTP APIs.</p>
<p>Chromecast is a line of digital media players developed by Google. It uses <a href="https://en.wikipedia.org/wiki/Google_Cast">Google Cast</a>, a proprietary protocol stack, based on Google Protocol Buffers and mDNS.</p>
<p>There are two implementations of DLNA and/or Chromecast support:</p>
<ul>
<li>
<p><strong>module-rygel-media-server</strong></p>
<p>PulseAudio can become a DLNA media server so that other DLNA devices 
can discover and read PulseAudio sources. This feature is implemented 
using <a href="https://wiki.gnome.org/Projects/Rygel/">Rygel</a>, a DLNA media server. See details <a href="https://wiki.gnome.org/Projects/Rygel/Pulseaudio">here</a>.</p>
<p>PulseAudio registers a Rygel plugin, which exports PulseAudio sources
 and sink monitors via D-Bus. Every exported source or sink monitor 
includes an HTTP URL that should be used to read samples from 
PulseAudio.</p>
<p>For its part, Rygel publishes exported sources and sink monitors via 
UPnP, and ultimately DLNA clients may see them and read PulseAudio 
streams via HTTP.</p>
</li>
<li>
<p><strong>pulseaudio-dlna</strong></p>
<p>A third-party <a href="https://github.com/masmu/pulseaudio-dlna">pulseaudio-dlna</a>
 project allows PulseAudio to discover and send audio to DLNA media 
renderers and Chromecast devices. Devices published in the local network
 automatically appear as new PulseAudio sinks.</p>
<p>This project is implemented as a standalone daemon written in Python.
 The daemon creates a null sink for every discovered remote device, 
opens the sink monitor associated with it, reads samples from the 
monitor, performs necessary encoding, and sends samples to the remote 
device.</p>
<p>The communication with the PulseAudio server is done via the D-Bus API (to query and configure server objects) and <code>parec</code> tool (to receive samples from a sink monitor).</p>
</li>
</ul>
<h2 id="esound">
  ESound
  <a class="anchor" href="#esound">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-esound-protocol-{unix,tcp}</div>
</div>
<p>PulseAudio server may be accessed via the protocol used in <a href="https://en.wikipedia.org/wiki/ESound">Enlightened Sound Daemon</a>.</p>
<p>The documentation says that it supports playback, recording, and 
control commands, so switching to PulseAudio should be transparent for 
applications that are using ESound.</p>
<h2 id="simple">
  Simple
  <a class="anchor" href="#simple">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-simple-protocol-{unix,tcp}</div>
</div>
<p>The “simple” protocol is used to send or receive raw PCM samples from
 the PulseAudio server without any headers or meta-information.</p>
<p>The user should configure the sample format and the source and sink to use. Then the user may use tools like <code>netcat</code> to send PCM samples over a Unix domain or TCP socket.</p>
<h2 id="cli">
  CLI
  <a class="anchor" href="#cli">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-cli-protocol-{unix,tcp}</div>
  <div>module-cli</div>
</div>
<p>PulseAudio server implements its own <a href="http://manpages.ubuntu.com/manpages/man5/pulse-cli-syntax.5.html">CLI</a> protocol.</p>
<p>It is a simple text protocol that provides various commands to inspect and control the server:</p>
<ul>
<li>status commands - list and inspect server-side objects</li>
<li>module management - load, unload, and inspect modules</li>
<li>moving streams - move streams to devices</li>
<li>killing clients and streams - remove clients and streams</li>
<li>volume commands - setup volumes of devices and streams</li>
<li>configuration commands - setup parameters of devices and device ports</li>
<li>property lists - setup property lists of devices and streams</li>
<li>sample cache - add, remove, or play samples in the server-side sample cache</li>
<li>log and debug commands - configure logging, dump server configuration, etc</li>
<li>meta commands - include and conditional directives</li>
</ul>
<p>The same syntax may be used in several places:</p>
<ul>
<li>in PulseAudio configuration files</li>
<li>over a Unix domain or TCP socket (module-cli-protocol-{unix,tcp})</li>
<li>over the controlling TTY of the server (module-cli)</li>
</ul>
<p>The TTY version requires that the server should be started in foreground mode in a terminal. The <code>pacmd</code> tool uses the CLI protocol over a Unix domain socket.</p>
<hr>
<h1 id="device-drivers">
  Device drivers
  <a class="anchor" href="#device-drivers">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>PulseAudio has several backends that implement audio I/O and device 
management. The diagram below illustrates backend-specific components.</p>
<img src="PulseAudio%20under%20the%20hood_files/device_drivers.png" width="566px">
<ul>
<li>
<p><strong>card</strong></p>
<p>A card represents a physical audio device, like a sound card or 
Bluetooth device. It contains card profiles, device ports, and devices. 
It has a single active card profile.</p>
</li>
<li>
<p><strong>card profile</strong></p>
<p>A card profile represents an opaque configuration set of a card, like
 an analog or digital mode. It defines the backend-specific 
configuration of the card, and the list of currently available device 
ports and devices.</p>
</li>
<li>
<p><strong>device port</strong></p>
<p>A device port represents a single input or output port on the card, 
like internal speakers or external line-out. Multiple device ports may 
belong to a card.</p>
</li>
<li>
<p><strong>device</strong></p>
<p>A device (source or sink) represents an active sample producer or 
consumer. A device is usually associated with a card and a set of device
 ports. It has a single active device port. Multiple devices may belong 
to a card.</p>
</li>
</ul>
<p>Every backend should implement the following features of a source or sink:</p>
<ul>
<li>reading or writing samples to the device</li>
<li>maintaining latency</li>
<li>providing clocking in the device time domain</li>
</ul>
<p>Currently, the only full-featured backends are ALSA and Bluetooth, 
which implement all object types listed above. Other backends provide 
only sources and sinks.</p>
<h2 id="alsa-backend">
  ALSA backend
  <a class="anchor" href="#alsa-backend">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-udev-detect</div>
  <div>module-alsa-{card,source,sink}</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Advanced_Linux_Sound_Architecture">ALSA</a>
 (Advanced Linux Sound Architecture) is a Linux kernel component 
providing device drivers for sound cards, and a user space library 
(libasound) interacting with the kernel drivers. It provides a <a href="http://www.alsa-project.org/alsa-doc/alsa-lib/">rich high-level API</a> and hides hardware-specific stuff.</p>
<p>ALSA backend in PulseAudio automatically creates PulseAudio cards, card profiles, device ports, sources, and sinks:</p>
<ul>
<li>
<p>PulseAudio card is associated with an ALSA card.</p>
</li>
<li>
<p>PulseAudio card profile is associated with a configuration set for an
 ALSA card. It defines a subset of ALSA devices belonging to a card, and
 so the list of available device ports, sources, and sinks.</p>
</li>
<li>
<p>PulseAudio device port is associated with a configuration set for an 
ALSA device. It defines a list of active inputs and outputs of the card 
and other device options.</p>
</li>
<li>
<p>PulseAudio source and sink are associated with an ALSA device. When a
 source or sink is connected to a specific device port, they together 
define an ALSA device and its configuration.</p>
</li>
</ul>
<p>The concrete meaning of PulseAudio card profile and device ports 
depends on whether the ALSA UCM is available for an ALSA card or not 
(see below).</p>
<p>PulseAudio sources and sinks for ALSA devices implement a timer-based
 scheduler that manages latency and clocking. It is discussed later in a
 separate section.</p>
<h2 id="alsa-device-hierarchy">
  ALSA device hierarchy
  <a class="anchor" href="#alsa-device-hierarchy">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>ALSA uses a device hierarchy that is different from the PulseAudio hierarchy. See <a href="http://delogics.blogspot.ru/2014/11/understanding-alsa-device-subdevice-and.html">this post</a> for an overview.</p>
<p>The ALSA hierarchy has three levels:</p>
<ul>
<li>
<p><strong>card</strong></p>
<p>ALSA card represents a hardware or virtual sound card. Hardware cards
 are backed by kernel drivers, while virtual cards are implemented 
completely in user space plugins.</p>
</li>
<li>
<p><strong>device</strong></p>
<p>ALSA card contains at least one playback or capture device. A device 
is something that is capable of processing single playback or recording 
stream. All devices or a card may be used independently and in parallel.
 Typically, every card has at least one playback device and one capture 
device.</p>
</li>
<li>
<p><strong>subdevice</strong></p>
<p>ALSA device contains at least one subdevice. All subdevices of the 
same device share the same playback or recording stream. For playback 
devices, subdevices are used to represent available slots for hardware 
mixing. Typically, there is no hardware mixing, and every device has a 
single subdevice.</p>
</li>
</ul>
<p>ALSA device is identified by a card number and device number. 
PulseAudio by default interacts only with hardware ALSA devices. 
PulseAudio currently doesn’t use hardware mixing and so don’t employ 
multiple subdevices even if they’re available.</p>
<h2 id="alsa-kernel-interfaces">
  ALSA kernel interfaces
  <a class="anchor" href="#alsa-kernel-interfaces">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Each ALSA device has a corresponding device entry in the <code>"/dev/snd"</code> directory. Their meta-information may be discovered through the <code>"/proc/asound"</code> directory. See some details in <a href="http://www.sabi.co.uk/Notes/linuxSoundALSA.html">this post</a>.</p>
<p>The diagram below shows the device entries involved when PulseAudio is running.</p>
<img src="PulseAudio%20under%20the%20hood_files/alsa_kernel_interfaces.png" width="421px">
<p>Five device entry types exist:</p>
<ul>
<li>
<p><strong>pcm</strong> - for recording or playing samples</p>
</li>
<li>
<p><strong>control</strong> - for manipulating the internal mixer and routing of the card</p>
</li>
<li>
<p><strong>midi</strong> - for controlling the MIDI port of the card, if any</p>
</li>
<li>
<p><strong>sequencer</strong> - for controlling the built-in sound synthesizer of the card, if any</p>
</li>
<li>
<p><strong>timer</strong> - to be used in pair with the sequencer</p>
</li>
</ul>
<p>PulseAudio interacts only with the pcm and control device entries.</p>
<p>Every ALSA card and device may have a set of kcontrols (kenrnel 
control elements) associated with it. The kernel provides generic 
operations for manipulating registered kcontrols using ioctl on the 
corresponding device entry.</p>
<p>A kcontrol has the following properties:</p>
<ul>
<li>
<p><strong>name</strong> - a string identifier of the kcontrol</p>
</li>
<li>
<p><strong>index</strong> - a numerical identifier of the kcontrol</p>
</li>
<li>
<p><strong>interface</strong> - determines what entity this kcontrol is 
associated with, e.g. “card” (for card kcontrols), “pcm” (for pcm device
 kcontrols) or “mixer” (for control device kcontrols)</p>
</li>
<li>
<p><strong>type</strong> - determines the type of the kcontrol members, e.g. “boolean”, “integer”, “enumerated”, etc.</p>
</li>
<li>
<p><strong>members</strong> - contain the kcontrol values; a kcontrol can have multiple members, but all of the same type</p>
</li>
<li>
<p><strong>access attributes</strong> - determine various kcontrol access parameters</p>
</li>
</ul>
<p>A kcontrol typically represent a thing like a volume control (allows 
to adjust the sound card internal mixer volume), a mute switch (allows 
to mute or unmute device or channel), a jack control (allows to 
determine whether something is plugged in, e.g. into an HDMI or a 3.5mm 
analog connector), or some device-specific option.</p>
<p>The concrete set of the available kcontrols are defined by the sound 
card driver, though drivers try to provide similar kcontrols. The driver
 usually just exposes all available hardware controls, and it’s up to 
the user space to provide a unified hardware-independent layer on top of
 them. This responsibility rests on the ALSA UCM and PulseAudio.</p>
<h2 id="alsa-user-space-interfaces">
  ALSA user space interfaces
  <a class="anchor" href="#alsa-user-space-interfaces">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>ALSA provides numerous user space interfaces to interact with ALSA 
cards and devices and their properties. See libasound documentation: <a href="http://www.alsa-project.org/alsa-doc/alsa-lib/index.html">1</a>, <a href="http://www.alsa-project.org/alsa-doc/alsa-lib/modules.html">2</a>.</p>
<p>The diagram below provides an overview of the components involved when PulseAudio is running.</p>
<img src="PulseAudio%20under%20the%20hood_files/alsa_user_space_interfaces.png" width="608px">
<p>Here is the list of involved ALSA interfaces:</p>
<ul>
<li>
<p><strong>PCM</strong></p>
<p><a href="http://www.alsa-project.org/alsa-doc/alsa-lib/pcm.html">PCM</a> interface implements methods for playback and recording on top of the pcm device entry.</p>
<p>Applications can setup the per-device kernel-side ring buffer 
parameters, write or read samples to the buffer, and issue flow control 
operations.</p>
<p>This interface is used in most ALSA applications.</p>
</li>
<li>
<p><strong>CTL</strong></p>
<p><a href="http://www.alsa-project.org/alsa-doc/alsa-lib/control.html">CTL</a> interface implements low-level methods for accessing kcontrols on top of the kernel ioctl API.</p>
<p>Applications can inspect, read, and write CTL elements, which are mapped one-to-one to the kernel-side kcontrols.</p>
<p>This interface is usually not used directly.</p>
</li>
<li>
<p><strong>HCTL</strong></p>
<p><a href="http://www.alsa-project.org/alsa-doc/alsa-lib/hcontrol.html">HCTL</a> interface implements a caching layer on top of the CTL interface.</p>
<p>Applications can inspect, read, and write HCTL elements, mapped 
one-to-one to the CTL elements, and in addition, can set per-element 
callbacks for various events.</p>
<p>This interface is usually not used directly.</p>
</li>
<li>
<p><strong>Mixer</strong></p>
<p><a href="http://www.alsa-project.org/alsa-doc/alsa-lib/mixer.html">Mixer</a> interface implements a higher-level management layer above the HCTL interface.</p>
<p>It provides a framework for managing abstract mixer elements. A mixer
 element is, generally speaking, a set of logically grouped HCTL 
elements. Applications register custom mixer element classes and 
implement a custom mapping of the HCTL elements to the mixer elements.</p>
<p>This interface provides a generic and rather complex asynchronous 
API. In most cases, applications may use the Simple Mixer interface 
instead.</p>
</li>
<li>
<p><strong>Simple Mixer</strong></p>
<p><a href="http://www.alsa-project.org/alsa-doc/alsa-lib/group___simple_mixer.html">Simple Mixer</a> interface implements a mixer element class for the Mixer and provides a simple and less abstract synchronous API on top of it.</p>
<p>A Simple Mixer element represents a logical group of the related 
kcontrols. An element may have the following attributes, mapped 
internally to the corresponding HCTL elements: a volume control, a mute 
switch, or an enumeration value. Each attribute may be playback, 
capture, or global. Each attribute may be also per-channel or apply to 
all channels.</p>
<p>This interface is supposed to be used by applications that want to 
control the mixer and do not need the complex and generic Mixer API.</p>
</li>
<li>
<p><strong>UCM</strong></p>
<p><a href="http://www.alsa-project.org/alsa-doc/alsa-lib/group__ucm.html">UCM</a> (Use Case Manager) interface implements high-level configuration presets on top of the Mixer interface.</p>
<p>Applications can describe their use-cases by selecting one of the 
available presets instead of configuring mixer elements manually. The 
UCM then performs all necessary configuration automatically, hiding 
machine-specific details and complexity of the Mixer interface.</p>
</li>
</ul>
<h2 id="alsa-routing">
  ALSA routing
  <a class="anchor" href="#alsa-routing">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>ALSA cards often have multiple inputs and outputs. For example, a 
card may have an analog output for internal speaker, an analog output 
for 3.5mm headphone connector, an analog input for internal microphone, 
an analog input for 3.5mm microphone connector, and an HDMI input and 
output.</p>
<p>ALSA interfaces represent this in the following way:</p>
<ul>
<li>
<p>an ALSA card contains separate ALSA devices for HDMI and analog audio</p>
</li>
<li>
<p>an ALSA device has separate pcm device entries in <code>"/dev/snd"</code> for playback and capture</p>
</li>
<li>
<p>an ALSA card has a control device entry in <code>"/dev/snd"</code> with various kcontrols allowing to determine what’s plugged in and configure input and output routing</p>
</li>
</ul>
<p>The following kcontrols may be used:</p>
<ul>
<li>
<p><strong>jack controls</strong></p>
<p><a href="https://01.org/linuxgraphics/gfx-docs/drm/sound/designs/jack-controls.html">Jack controls</a>
 may be used to determine what’s plugged in. Drivers usually create jack
 controls for physical connectors, however, details may vary.</p>
<p>For example, headphone and microphone connectors may be represented 
with a single jack control or two separate jack controls. An internal 
speaker or microphone can be sometimes represented with a jack control 
as well, despite the fact that there is no corresponding physical 
connector.</p>
</li>
<li>
<p><strong>mute controls</strong></p>
<p>Mute controls may be used to mute and unmute a single input or 
output. Drivers usually create mute controls for every available input 
and output of a card.</p>
</li>
<li>
<p><strong>route controls</strong></p>
<p>An enumeration control may be sometimes provided to choose the active input or output.</p>
</li>
</ul>
<p>Different drivers provide different sets of kcontrols, and it’s up to
 the user space to build a unified hardware-independent layer on top of 
them. PulseAudio is able to do it by itself, or alternatively may rely 
on ALSA UCM.</p>
<p>In both cases, PulseAudio goal is to probe what inputs and outputs 
are available and map them to device ports somehow. However, details 
vary depending on whether UCM is in use or not.</p>
<h2 id="alsa-cards-with-ucm">
  ALSA cards with UCM
  <a class="anchor" href="#alsa-cards-with-ucm">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>ALSA Use Case Manager aims two goals:</p>
<ul>
<li>
<p>Abstract the applications which configure ALSA devices from the complexity of the Mixer interface.</p>
</li>
<li>
<p>Make these applications portable across numerous embedded and mobile 
devices, by moving the machine-specific part to configuration files.</p>
</li>
</ul>
<p>UCM lets applications to operate with such high-level operations as 
“setup this device to play HiFi music via an external line-out” or 
“setup that device to capture voice for a phone call via an internal 
microphone”.</p>
<p>UCM then looks into the local configuration files and maps such 
use-case description to the concrete values of mixer elements. These 
files may be part of the UCM package or may be provided by a device 
vendor.</p>
<p>An application provides the UCM with three strings:</p>
<ul>
<li>
<p><strong>ucm verb</strong></p>
<p>Defines the main operation mode of an ALSA device, e.g. “HiFi” or “Voice”. Only one UCM verb may be active at the same time.</p>
</li>
<li>
<p><strong>ucm modifier</strong></p>
<p>Defines a supplementary operation mode of an ALSA device, e.g. 
“PlayMusic” or “PlayTone”. Available UCM modifiers are defined by 
currently active UCM verb. Zero or multiple UCM modifiers may be active 
at the same time.</p>
</li>
<li>
<p><strong>ucm device</strong></p>
<p>Defines configuration of active inputs and outputs of an ALSA device,
 e.g. “Speaker” or “Headset”. Available UCM devices are defined by 
currently active UCM verb. Zero or multiple UCM devices may be active at
 the same time.</p>
</li>
</ul>
<p>A combination of one UCM verb, zero or multiple UCM modifiers, and 
one or multiple UCM devices define what ALSA device to use and how to 
configure its mixer elements.</p>
<p>When UCM is available for a card, PulseAudio automatically employs it.</p>
<p>The diagram below illustrates relations between PulseAudio and ALSA 
objects when the UCM is active. Some diagrams and details are also 
available on Linaro Wiki: <a href="https://wiki.linaro.org/WorkingGroups/Middleware/Multimedia/Specs/1111/AudioIntegration/UCMPulseAudio/Analyzation">1</a>, <a href="https://wiki.linaro.org/WorkingGroups/Middleware/Multimedia/Specs/1111/AudioIntegration/UCMPulseAudio/PlanMaterials">2</a>, <a href="https://wiki.linaro.org/WorkingGroups/Middleware/Multimedia/Specs/1111/AudioIntegration/UCMPulseAudio">3</a>.</p>
<div>
  <a data-lightbox="alsa_ucm" href="https://gavv.net/articles/pulseaudio-under-the-hood/diagrams/alsa_ucm.png">
    <img src="PulseAudio%20under%20the%20hood_files/alsa_ucm.png">
  </a>
</div>
<p>The mapping of the PulseAudio object hierarchy to the ALSA object hierarchy is the following:</p>
<ul>
<li>
<p>PulseAudio card is associated with the UCM interface of an ALSA card. One PulseAudio card is created for every ALSA card.</p>
</li>
<li>
<p>PulseAudio card profile is associated with a UCM verb. For every 
card, one PulseAudio profile is created for every UCM verb available for
 the card.</p>
</li>
<li>
<p>PulseAudio source is associated with the PCM interface of a capture 
ALSA device. For every card, one PulseAudio source is created for every 
available capture ALSA device that is associated with a UCM verb, UCM 
modifier, or UCM device available in the currently active card profile.</p>
</li>
<li>
<p>PulseAudio sink is associated with the PCM interface of a playback 
ALSA device. For every card, one PulseAudio sink is created for every 
playback ALSA device that is associated with a UCM verb, UCM modifier, 
or UCM device available in the currently active card profile.</p>
</li>
<li>
<p>PulseAudio device port is associated with a combination of a UCM 
modifier and UCM devices. For every source or sink, one PulseAudio 
device port is created for every possible valid combination of zero or 
one UCM modifier and one or multiple UCM devices.</p>
<p>A valid combination includes:</p>
<ul>
<li>only UCM modifiers and devices that are enabled by currently active card profile</li>
<li>only UCM modifiers and devices that are associated with the ALSA device of the source or sink</li>
<li>only non-mutually exclusive UCM devices</li>
</ul>
<p>Every UCM modifier is mapped to a PulseAudio role. The UCM modifier 
of a device port is actually enabled only when there is at least one 
source output or sink input connected to the source or sink of the 
device port, which has a “media.role” property equal to the UCM 
modifier’s role.</p>
</li>
</ul>
<p>This is how the mapping is used:</p>
<ul>
<li>
<p>The card defines what ALSA card is used, and so what profiles are available.</p>
</li>
<li>
<p>The currently active card profile of the card defines what UCM verb is used, and so what sources and sinks are available.</p>
</li>
<li>
<p>The source or sink defines what ALSA device is used, and so what device ports are available.</p>
</li>
<li>
<p>The currently active device port of the source or sink defines what 
UCM modifier and UCM devices are used. Whether the UCM modifier is 
enabled depends on the roles of currently connected source outputs or 
sinks inputs.</p>
</li>
<li>
<p>The currently active UCM verb, UCM modifier, and UCM devices define 
what card inputs and outputs are active, what device options are set, 
and what volume controls are used.</p>
</li>
</ul>
<h2 id="alsa-cards-wo-ucm">
  ALSA cards w/o UCM
  <a class="anchor" href="#alsa-cards-wo-ucm">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Besides the UCM support, PulseAudio has its own configuration system 
on top of the ALSA Mixer. It was developed before UCM appeared. It is 
used when the UCM is not available for a card.</p>
<p>Mixer configuration is described in custom PulseAudio-specific configuration files. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Backends/ALSA/Profiles/">Profiles</a> page on wiki.</p>
<p>Configuration files define the following objects:</p>
<ul>
<li>
<p><strong>profile set</strong></p>
<p>Provides available profiles for an ALSA card. Contains a list of profiles.</p>
<p>Physically it is a <code>.conf</code> file under the <code>"/usr/share/pulseaudio/alsa-mixer/profile-sets"</code> directory.</p>
</li>
<li>
<p><strong>profile</strong></p>
<p>Represents a configuration set for an ALSA card. Contains a list of mappings.</p>
<p>Every mapping defines a playback or capture ALSA device that is 
available when this profile is active, and the configuration sets 
available for each device.</p>
<p>Physically it is a <code>[Profile]</code> section in the profile file.</p>
</li>
<li>
<p><strong>mapping</strong></p>
<p>Represents a playback or capture ALSA device. Contains:</p>
<ul>
<li>
<p><strong>device strings</strong></p>
<p>Device string is a pattern used to match an concrete ALSA device belonging to the ALSA card. First matched device is used.</p>
</li>
<li>
<p><strong>channel map</strong></p>
<p>Channel mapping defines what channels are used for the ALSA device.</p>
</li>
<li>
<p><strong>input/output paths</strong></p>
<p>Mapping contains multiple input and output paths that represent alternative configuration sets for the ALSA device.</p>
<p>Every input or output path defines a single configuration set, which 
provides an ALSA mixer path and settings for ALSA mixer elements 
accessible through that path.</p>
</li>
</ul>
<p>Physically it is a <code>[Mapping]</code> section in the profile file.</p>
</li>
<li>
<p><strong>path</strong></p>
<p>Represents a configuration set for a single capture or playback ALSA device. Contains a list of elements, and a list of jacks.</p>
<p>Every element or jack defines an ALSA mixer element and how it should
 be used when the configuration set defined by this path is active.</p>
<p>Physically it is a <code>.conf</code> file under the <code>"/usr/share/pulseaudio/alsa-mixer/paths"</code> directory.</p>
</li>
<li>
<p><strong>jack</strong></p>
<p>Represents an ALSA mixer element for a jack that should be used for 
probing. Contains identifier of the ALSA mixer element and its expected 
state (plugged or unplugged).</p>
<p>Every jack is probed and its state is compared with the expected one.
 This probing is used to activate only those paths that are actually 
available.</p>
<p>Physically it is a <code>[Jack]</code> section in the path file.</p>
</li>
<li>
<p><strong>element</strong></p>
<p>Represents an ALSA mixer element and defines how it should be handled. Contains:</p>
<ul>
<li>
<p><strong>element id</strong></p>
<p>Defines the name of the ALSA mixer element.</p>
</li>
<li>
<p><strong>volume policy</strong></p>
<p>Defines how to handle the volume of the ALSA mixer element. It may be
 either ignored, unconditionally disabled, unconditionally set to a 
constant, or merged into the value of PulseAudio volume slider.</p>
</li>
<li>
<p><strong>switch value</strong></p>
<p>Defines how to handle the value of a switch ALSA mixer element. It 
may be either ignored, unconditionally set to a constant, used for 
muting and unmuting, or made selectable by the user via an option.</p>
</li>
<li>
<p><strong>enumeration value</strong></p>
<p>Defines how to handle the value of enumeration ALSA mixer element. It
 may be either ignored, or made selectable by the user via an option.</p>
</li>
<li>
<p><strong>options</strong></p>
<p>Every option defines one alternative value of a switch or enumeration
 ALSA mixer element. This value is made selectable by the user.</p>
</li>
</ul>
<p>Physically it is an <code>[Element]</code> section in the path file.</p>
</li>
<li>
<p><strong>option</strong></p>
<p>Represents one alternative value of a switch or enumeration ALSA 
mixer element. Contains identifier of the ALSA mixer element and its 
value.</p>
<p>Physically it is an <code>[Option]</code> section in the path file.</p>
</li>
</ul>
<p>When UCM is not available for a card, PulseAudio uses <a href="https://www.freedesktop.org/software/systemd/man/udev.html">Udev rules</a> to select an appropriate profile set for the card:</p>
<ul>
<li>
<p>PulseAudio installs Udev rules that match known audio card devices by vendor and product identifiers and set <code>PULSE_PROFILE_SET</code> property for them. The property contains a name of a profile set <code>.conf</code> file.</p>
</li>
<li>
<p>When PulseAudio configures a new ALSA card that has no UCM support, it reads the <code>PULSE_PROFILE_SET</code>
 property set by Udev rules and loads the appropriate profile set file. 
The file defines how to create and configure card profiles, device 
ports, sources, and sinks.</p>
</li>
<li>
<p>If an ALSA card was not matched by Udev rules and the <code>PULSE_PROFILE_SET</code> property was not set, PulseAudio uses default profile set which contains some reasonable configuration for most cards.</p>
</li>
</ul>
<p>The diagram below illustrates relations between PulseAudio and ALSA objects when UCM is not used.</p>
<div>
  <a data-lightbox="alsa_no_ucm" href="https://gavv.net/articles/pulseaudio-under-the-hood/diagrams/alsa_no_ucm.png">
    <img src="PulseAudio%20under%20the%20hood_files/alsa_no_ucm.png">
  </a>
</div>
<p>The mapping of the PulseAudio object hierarchy to the ALSA object hierarchy is the following:</p>
<ul>
<li>
<p>PulseAudio card is associated with an ALSA card and a profile set 
defined in configuration files. One PulseAudio card is created for every
 ALSA card, and one profile set is selected for every card.</p>
</li>
<li>
<p>PulseAudio card profile is associated with a profile defined in 
configuration files. For every card, one PulseAudio profile is created 
for every profile in the profile set of the card.</p>
</li>
<li>
<p>PulseAudio source is associated with a mapping defined in 
configuration files, and with the PCM interface of the capture device 
matched by the device mask of the mapping. For every card, one 
PulseAudio source is created for every mapping in the currently active 
profile.</p>
</li>
<li>
<p>PulseAudio sink is associated with a mapping defined in configuration
 files, and with the PCM interface of the playback device matched by the
 device mask of the mapping. For every card, one PulseAudio sink is 
created for every mapping in the currently active profile.</p>
</li>
<li>
<p>PulseAudio device port is associated with a combination of a path and
 options defined in configuration files. For every source or sink, one 
PulseAudio device port is created for every possible combination of one 
path and a subset of all options of all elements of this path.</p>
</li>
</ul>
<p>This is how the mapping is used:</p>
<ul>
<li>
<p>The card defines what ALSA card is used and what profile set is used, and so what profiles are available.</p>
</li>
<li>
<p>The currently active card profile of the card defines what mappings are available, and so what sources and sinks are available.</p>
</li>
<li>
<p>The source or sink defines what ALSA device is used, and what mapping is used, and so what device ports are available.</p>
</li>
<li>
<p>The currently active device port of the source or sink defines what 
path is used, what jacks are probed, what elements are used for getting 
and setting volume and how, and what combination of options of elements 
of the path is used.</p>
</li>
<li>
<p>The currently active elements, their volume policies, and their 
options define how to configure ALSA mixer elements of the ALSA device.</p>
</li>
</ul>
<h2 id="bluetooth-backend">
  Bluetooth backend
  <a class="anchor" href="#bluetooth-backend">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-bluetooth-discover</div>
  <div>module-bluez5-{discover,device}</div>
  <div>module-bluez4-{discover,device}</div>
</div>
<p>PulseAudio supports <a href="https://en.wikipedia.org/wiki/Bluetooth">Bluetooth</a>, a wireless protocol stack for exchanging data over short distances. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/Bluetooth/">Bluetooth</a> page for details. PulseAudio relies on two backends for Bluetooth support:</p>
<ul>
<li><a href="http://www.bluez.org/">BlueZ</a> (PulseAudio supports versions 4 and 5, but we discuss only version 5)</li>
<li><a href="https://en.wikipedia.org/wiki/OFono">oFono</a> (for HFP support)</li>
</ul>
<p>Bluetooth specification defines numerous <a href="https://en.wikipedia.org/wiki/List_of_Bluetooth_profiles">Bluetooth profiles</a>
 which may be supported by a device. Each profile describes the 
protocols, codecs, and device roles to be used. A Bluetooth device may 
support a subset of defined profiles and roles. Note that Bluetooth 
profiles and roles are different from the PulseAudio card profiles and 
stream roles.</p>
<p>PulseAudio supports three Bluetooth profiles:</p>
<ul>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/List_of_Bluetooth_profiles#Advanced_Audio_Distribution_Profile_.28A2DP.29">A2DP</a></strong> (Advanced Audio Distribution Profile)</p>
<p>Profile for high-quality audio streaming. Usually used to stream music.</p>
<p>The roles of the two connected A2DP devices are:</p>
<ul>
<li>Source role (SRC) - the device that sends audio</li>
<li>Sink role (SNK) - the device that receives audio</li>
</ul>
<p>PulseAudio supports both roles. For every discovered A2DP device, two options are available:</p>
<ul>
<li>for SRC device, the server may create a single PulseAudio source which acts as an SNK device</li>
<li>for SNK device, the server may create a single PulseAudio sink which acts as an SRC device</li>
</ul>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/List_of_Bluetooth_profiles#Headset_Profile_.28HSP.29">HSP</a></strong> (Headset Profile)</p>
<p>Profile for phone-quality audio playback and recording. Usually used for phone calls.</p>
<p>The roles of the two connected HSP devices are:</p>
<ul>
<li>Headset role (HS) - the device with the speakers and microphone, e.g. a headset</li>
<li>Audio Gateway role (AG) - the device that serves as a gateway to an 
external service, e.g. a mobile phone connected to a cellular network</li>
</ul>
<p>PulseAudio supports both roles. It can communicate with a headset or 
be a headset itself for other device. For every discovered HS or AG 
device, the server may create a pair of PulseAudio source and sink which
 together act as an AG or HS device.</p>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/List_of_Bluetooth_profiles#Hands-Free_Profile_.28HFP.29">HFP</a></strong> (Hands-Free Profile)</p>
<p>Provides all features of HSP plus some additional features for managing phone calls.</p>
<p>The roles of the two connected HFP devices are:</p>
<ul>
<li>Hands-Free Unit role (HF) - the device with the speakers and microphone, e.g. a portable navigation device</li>
<li>Audio Gateway role (AG) - the device that serves as a gateway to an 
external service, e.g. a mobile phone connected to a cellular network</li>
</ul>
<p>PulseAudio supports both roles. It can communicate with a HF unit or 
be a HF unit itself for other device. For every discovered HF or AG 
device, the server may create a pair of PulseAudio source and sink which
 together act as an AG or HF device.</p>
</li>
</ul>
<p>PulseAudio card profile is associated with a Bluetooth profile and role. The following card profiles are available:</p>
<ul>
<li>
<p>High Fidelity Playback (A2DP Sink) - the PulseAudio card will provide
 a single PulseAudio source which acts as an A2DP SNK device</p>
</li>
<li>
<p>High Fidelity Capture (A2DP Source) - the PulseAudio card will provide a single PulseAudio sink which acts as an A2DP SRC device</p>
</li>
<li>
<p>Headset Head Unit (HSP/HFP) - the PulseAudio card will provide a pair
 PulseAudio source and sink which together act as an HF device</p>
</li>
<li>
<p>Headset Audio Gateway (HSP/HFP) - the PulseAudio card will provide a 
pair PulseAudio source and sink which together act as an AG device</p>
</li>
</ul>
<p>Bluetooth backend listens to BlueZ and oFono events on D-Bus and 
automatically creates PulseAudio cards, sources, and sinks for all 
discovered Bluetooth devices.</p>
<p>The mapping of the PulseAudio object hierarchy to the Bluetooth object hierarchy is the following:</p>
<ul>
<li>
<p>PulseAudio card is associated with a Bluetooth device. One PulseAudio card is created for every discovered Bluetooth device.</p>
</li>
<li>
<p>PulseAudio card profile is associated with a Bluetooth profile and 
role. One of the predefined PulseAudio card profiles created for every 
available operation mode supported by the Bluetooth device.</p>
</li>
<li>
<p>One PulseAudio source and/or one PulseAudio sink is created for every
 PulseAudio card depending on the currently active card profile.</p>
</li>
<li>
<p>One PulseAudio device port is created for every PulseAudio source or sink.</p>
</li>
</ul>
<p>This is how the mapping is used:</p>
<ul>
<li>
<p>The card defines what Bluetooth device is used and what profiles are available.</p>
</li>
<li>
<p>The currently active card profile defines what Bluetooth profile and 
role are used, and so what transport protocols and codecs are used.</p>
</li>
</ul>
<h2 id="jack-backend">
  JACK backend
  <a class="anchor" href="#jack-backend">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-jackdbus-detect</div>
  <div>module-jack-{source,sink}</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/JACK_Audio_Connection_Kit">JACK</a>
 (JACK Audio Connection Kit) is a professional sound server that 
provides realtime low-latency connections between applications and 
hardware. Like PulseAudio, JACK may work on top of several backends, 
including ALSA.</p>
<p>However, their design goals are different. See comments from <a href="http://0pointer.de/blog/projects/when-pa-and-when-not.html">PulseAudio authors</a> and <a href="http://jackaudio.org/faq/pulseaudio_and_jack.html">JACK authors</a>:</p>
<ul>
<li>
<p>PulseAudio design is focused on consumer audio for desktop and 
mobile. It offers seamless device switching, automatic setup of hardware
 and networking, and power saving. It can’t guarantee extremely low 
latency. Instead, it usually adjusts latency dynamically to provide 
lower battery usage and better user experience even on cheap hardware.</p>
</li>
<li>
<p>JACK design is focused on professional audio hardware and software. 
It offers the lowest possible latency and may connect applications 
directly to devices or each other. It doesn’t try to provide the smooth 
desktop experience to the detriment of performance or configurability 
and is targeted to advanced users.</p>
</li>
</ul>
<p>There are three alternative options to use PulseAudio and JACK on the same system:</p>
<ul>
<li>Use them for different sound cards. JACK can ask  PulseAudio to release an ALSA card via <a href="http://git.0pointer.net/reserve.git/tree/reserve.txt">device reservation API</a>.</li>
<li>Suspend PulseAudio when JACK is running using <a href="http://manpages.ubuntu.com/manpages/en/man1/pasuspender.1.html">pasuspender</a> tool.</li>
<li>Configure PulseAudio to use JACK backend instead of ALSA.</li>
</ul>
<p>JACK backend for PulseAudio monitors when JACK is started using the 
JACK D-Bus API, and then creates one source and sink that read and write
 samples to JACK.</p>
<p>PulseAudio uses two threads for a JACK source or sink: one realtime 
thread for the JACK event loop, and another for the PulseAudio one. The 
reason for an extra thread is that it’s not possible to add custom event
 sources to the JACK event loop, hence PulseAudio event loop can’t be 
embedded into it. The extra thread costs extra latency, especially if 
PulseAudio is not configured to make its threads realtime using rtkit.</p>
<h2 id="other-backends">
  Other backends
  <a class="anchor" href="#other-backends">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The following backends are available but have limited functionality:</p>
<ul>
<li>
<p><strong>OSS</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-detect</div>
    <div>module-oss</div>
  </div>
<p><a href="https://en.wikipedia.org/wiki/Open_Sound_System">OSS</a> 
(Open Sound System) is an older interface for making and capturing sound
 in Unix and Unix-like operating systems. Nowadays it is superseded by 
ALSA on Linux but is used on some other Unix systems. Many systems, 
including Linux and various *BSD variants, provide a compatibility layer
 for OSS applications.</p>
<p>OSS backend implements source and sink for OSS devices. Each one is connected to a single device, usually <code>/dev/dspN</code>. At startup, PulseAudio can automatically create a sink and source for every available OSS device.</p>
</li>
<li>
<p><strong>Solaris</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-detect</div>
    <div>module-solaris</div>
  </div>
<p>Solaris backend implements source and sink for <code>/dev/audio</code>
 device available in Solaris and some *BSD variants, also known as “Sun 
audio” or “Sunau” and originally appeared in SunOS. This device supports
 the <a href="https://en.wikipedia.org/wiki/Au_file_format">Au file format</a>.</p>
<p>At startup, PulseAudio can automatically create one sink and one source for <code>/dev/audio</code> device if it is present.</p>
</li>
<li>
<p><strong>CoreAudio</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-coreaudio-{detect,device}</div>
  </div>
<p><a href="https://en.wikipedia.org/wiki/Core_Audio">CoreAudio</a> is a low-level API for dealing with sound in Apple’s MacOS and iOS operating systems.</p>
<p>CoreAudio backend monitors available devices and automatically 
creates card, sink, and source for every detected device. No card 
profiles and device ports are implemented.</p>
</li>
<li>
<p><strong>WaveOut</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-detect</div>
    <div>module-waveout</div>
  </div>
<p>WaveOut backend implements source and sink for the legacy Win32 WaveIn/WaveOut interfaces. They are part of <a href="https://en.wikipedia.org/wiki/Windows_legacy_audio_components">MultiMedia Extensions</a> introduced in Windows 95 and still supported in recent Windows versions (with some issues).</p>
<p>Each source or sink is connected to a single device. At startup, 
PulseAudio can automatically create one sink and one source for the 
first available device if it is running on Windows.</p>
</li>
<li>
<p><strong>ESound</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-esound-sink</div>
  </div>
<p>ESound backend implements a sink acting as a client for <a href="https://en.wikipedia.org/wiki/ESound">Enlightened Sound Daemon</a>. It doesn’t implement source. The documentation recommends avoiding using this sink because of latency issues.</p>
<p>Note that PulseAudio server is also able to emulate ESound server.</p>
</li>
</ul>
<h2 id="hotplug-support">
  Hotplug support
  <a class="anchor" href="#hotplug-support">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-{udev,jackbus,coreaudio}-detect</div>
  <div>module-{bluetooth,bluez5,bluez4}-discover</div>
</div>
<p>Hotplug is currently implemented for the following backends:</p>
<ul>
<li>ALSA (using libudev)</li>
<li>Bluetooth (using BlueZ)</li>
<li>JACK (using D-Bus JACK API)</li>
<li>CoreAudio</li>
</ul>
<p>In particular, PulseAudio uses <a href="https://www.freedesktop.org/software/systemd/man/libudev.html">libudev</a>
 to detect ALSA cards (both with and without UCM support). The server 
creates Udev monitor and filters events for sound card devices:</p>
<ul>
<li>when a new device is inserted, the server creates a card, card profiles, device ports, sources, and sinks, as described above</li>
<li>when the device is removed, all these objects are removed as well</li>
</ul>
<h2 id="hardware-controls">
  Hardware controls
  <a class="anchor" href="#hardware-controls">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio server has support for hardware controls. The user should 
manually specify a sink, and the server will forward volume up/down and 
mute requests to it.</p>
<p>Two types of controls are supported:</p>
<ul>
<li>
<p><strong>IR remote control</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-lirc</div>
  </div>
<p>Infrared remote controls are handled using <a href="https://en.wikipedia.org/wiki/LIRC">LIRC</a> (Linux Infrared Remote Control).</p>
</li>
<li>
<p><strong>Multimedia buttons</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-mmkbd-evdev</div>
  </div>
<p>Multimedia buttons available on some keyboards are handled using <a href="https://en.wikipedia.org/wiki/Evdev">evdev</a>, a generic input event interface in the Linux kernel, usually used in programs like X server and Wayland.</p>
</li>
</ul>
<hr>
<h1 id="sound-processing">
  Sound processing
  <a class="anchor" href="#sound-processing">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>PulseAudio implements various sound processing tools. Some of them 
are enabled automatically when necessary (like sample rate conversion), 
and others should be explicitly configured by the user (like echo 
cancellation).</p>
<h2 id="resampler">
  Resampler
  <a class="anchor" href="#resampler">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p>Every source, sink, source output, and sink input may use its own audio parameters:</p>
<ul>
<li>sample format (e.g. 32-bit floats in native endian)</li>
<li>sample rate (e.g. 44100Hz)</li>
<li>channel map (e.g. two channels for stereo)</li>
</ul>
<p>Source output and sink input are responsible for performing all 
necessary conversions when they are invoked by source or sink. To 
achieve this, they configure resampler with appropriate input and output
 parameters and then run it frame-by-frame.</p>
<p>When resampler is configured, it tries to select an optimal conversion algorithm for requested input and output parameters:</p>
<ul>
<li>chooses sample rate conversion method</li>
<li>chooses the working sample format for sample rate conversion (some methods benefit from using a higher precision)</li>
<li>calculates channel mapping (taking into account channel names and meaning)</li>
</ul>
<p>For every frame, resampler performs the following steps:</p>
<ul>
<li>converts frame from input sample format to working sample format</li>
<li>maps input channels to output channels</li>
<li>converts sample rate from input rate to output rate</li>
<li>if LFE channel (subwoofer) is used, applies the <a href="https://en.wikipedia.org/wiki/Linkwitz%E2%80%93Riley_filter">LR4 filter</a></li>
<li>converts frame from working sample format to output sample format</li>
</ul>
<p>Each step is performed only if it’s needed. For example, if input 
sample format and working sample format are the same, no conversion is 
necessary.</p>
<p><a href="https://en.wikipedia.org/wiki/Sample_rate_conversion">Sample rate conversion</a> usually operates at fixed input and output rates. When a client creates a stream, it may enable <em>variable rate</em> mode. In this case, input or output rate may be updated on the fly by explicit client request.</p>
<p>The user can specify what method to use in the server configuration 
files. You can find a comparison of hardware and some software resampler
 methods in <a href="http://archimago.blogspot.ru/2015/10/measurements-look-at-linux-audio-alsa.html">this post</a>.</p>
<p>The following methods are supported:</p>
<ul>
<li>
<p><strong>speex</strong></p>
<p>Fast resampler from <a href="https://speex.org/">Speex</a> library. If PulseAudio was built with speex support, used by default.</p>
</li>
<li>
<p><strong>ffmpeg</strong></p>
<p>Fast resampler from <a href="https://www.ffmpeg.org/">FFmpeg</a> library. If PulseAudio was built without speex support, and variable rate mode is not requested, used by default.</p>
</li>
<li>
<p><strong>src</strong></p>
<p>Slower but high-quality resampler from <a href="http://www.mega-nerd.com/SRC/">Secret Rabbit Code</a> (libsamplerate) library. Used in some PulseAudio modules.</p>
</li>
<li>
<p><strong>sox</strong></p>
<p>Slower but high-quality resampler from <a href="http://sox.sourceforge.net/">SoX</a> library. Not used by default.</p>
</li>
<li>
<p><strong>trivial</strong></p>
<p>Built-in low-quality implementation used as a fallback when 
PulseAudio was built without speex support, and ffmpeg can’t be used 
because variable rate mode was requested.</p>
<p>Instead of interpolation, it uses <a href="https://en.wikipedia.org/wiki/Decimation_(signal_processing)">decimation</a> (when downsampling) or duplication (when upsampling).</p>
</li>
<li>
<p><strong>copy</strong></p>
<p>No-op implementation used when input and output sample rates are the same.</p>
</li>
<li>
<p><strong>peaks</strong></p>
<p>Pseudo resampler that finds peaks. It is enabled when a client 
requests peak detection mode. Instead of interpolation, it calculates 
every output sample as a maximum value in the corresponding window of 
input samples.</p>
<p>This mode is usually used in GUI applications like <code>pavucontrol</code> that want to display volume level.</p>
</li>
</ul>
<h2 id="mixing-and-volumes">
  Mixing and volumes
  <a class="anchor" href="#mixing-and-volumes">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulse</div>
  <div>libpulsecore</div>
</div>
<p>When multiple sink inputs are connected to one sink, sink 
automatically mixes them, taking into account per-channel volume 
settings. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Volumes/">Volumes</a> and <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/WritingVolumeControlUIs/">Writing Volume Control UIs</a> pages.</p>
<p>Every source, sink, sink input, and source output has its own 
per-channel volume level that may be controlled via both C API and D-Bus
 API.</p>
<p>A way how the sink and sink input volumes are combined is determined by the <em>flat volumes</em> mode (inspired by Windows Vista):</p>
<ul>
<li>
<p>When flat volumes are enabled, the volume of the sink is always the 
maximum volume of all sink inputs connected to it. When the sink input 
volume is updated, the sink volume is recalculated too. When the sink 
volume is updated, all sink input volumes are scaled equally.</p>
</li>
<li>
<p>When flat volumes are disabled, each sink input has its own volume, 
considered to be relative to the volume of the sink to which it is 
connected.</p>
</li>
</ul>
<p>This mode may be enabled per-sink or globally (default in many distros).</p>
<p>There are two kinds of volumes:</p>
<ul>
<li>hardware volumes are used for hardware sources and sinks that support it</li>
<li>software volumes are used everywhere else, in particular for 
hardware sources and sinks that don’t support hardware volume, and for 
sink input mixing</li>
</ul>
<p>Volumes span from 0% to 100%, which are respectively the silence and the maximum volume that the sound hardware is capable of.</p>
<p>Software volumes use the cubic scale. Hardware volumes generally use 
an unspecified scale. However, volumes of hardware sources and sinks 
that have the <em>decibel volume</em> flag and volumes of all sink inputs may be converted to and from the decibel scale using dedicated API functions.</p>
<p>Finally, virtual source and sinks that are attached to a master source or sink usually use <em>volume sharing</em> mode. When it is enabled, the source or sink always uses the same volume as its master.</p>
<h2 id="volume-range">
  Volume range
  <a class="anchor" href="#volume-range">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulse</div>
  <div>libpulsecore</div>
  <div>module-alsa-{source,sink}</div>
</div>
<img src="PulseAudio%20under%20the%20hood_files/volumes.png" width="490px">
<p>The device volume range is virtually divided into the three subranges:</p>
<ul>
<li><code>[muted, base]</code></li>
<li><code>[base, norm]</code></li>
<li><code>[norm, norm * n]</code></li>
</ul>
<p>The points on the boundaries are the following:</p>
<ul>
<li>
<p><strong>“muted”</strong></p>
<p>Constant. Complete silence (0%, -inf dB).</p>
</li>
<li>
<p><strong>“base”</strong></p>
<p>Determined dynamically for every device. Defines backend-specific 
default volume that may be not as loud as the “norm” volume. May be 
equal to the “norm” volume.</p>
<p>Mapped to the volume where the analog output is at some kind of 
normalized, pre-defined voltage level. For S/PDIF cards set to the 
volume where the output PCM samples are unscaled.</p>
</li>
<li>
<p><strong>“norm”</strong></p>
<p>Constant. Maximum hardware volume of a card (100%, 0 dB). If a card includes a proper amplifier, this volume may be very loud.</p>
<p>For cards with an amplifier, volumes below this point employ hardware
 amplification, and volumes above this point employ digital (software) 
amplification.</p>
<p>For cards without an amplifier, digital amplification is always used, both for volumes below and above this point.</p>
<p>For cards without the decibel volume flag, volumes above this point are internally truncated the “norm” volume.</p>
</li>
<li>
<p><strong>“norm * n”</strong></p>
<p>Maximum volume that a GUI allows to set, e.g. “norm * 2”.</p>
<p>A GUI uses a maximum volume above the “norm” to let the user to employ additional digital amplification.</p>
<p>This may be useful, for example, if the device is under-powered or 
the audio content has been mastered with too low volume. However it may 
cause distortion.</p>
</li>
</ul>
<h2 id="passthrough">
  Passthrough
  <a class="anchor" href="#passthrough">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p>By default, PulseAudio uses uncompressed PCM everywhere. However, 
some input and output devices support various compressed audio 
encodings.</p>
<p>PulseAudio provides <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/Passthrough/">passthrough mode</a>
 that may be enabled for a source, sink, source output, and sink input. 
With this mode, samples may be sent in the device-specific compressed 
encoding.</p>
<p>When a client creates a stream, it may enable passthrough mode. In 
this case, passthrough mode is enabled for corresponding source output 
or sink input.</p>
<p>Note that a passthrough source output may be connected only to a 
passthrough source, and a passthrough sink input may be connected only 
to a passthrough sink.</p>
<p>Currently, only several <a href="https://en.wikipedia.org/wiki/S/PDIF">S/PDIF</a> (IEC61937) encodings are supported:</p>
<ul>
<li>IEC61937 DTS</li>
<li>IEC61937 AC3</li>
<li>IEC61937 EAC3</li>
<li>IEC61937 MPEG</li>
<li>IEC61937 MPEG2 AAC</li>
</ul>
<p>PulseAudio server doesn’t automatically detect actual encodings 
supported by hardware. However, PulseAudio client can manually enable or
 disable encodings for every source and sink using introspection API. 
The server stores a list of enabled encodings for every source and sink 
in a database, so these settings are persistent.</p>
<p>The user can enable encodings via <code>pavucontrol</code> GUI. Other
 applications may check which encodings are enabled for a source or sink
 and use passthrough mode if they support one of the enabled encodings.</p>
<h2 id="virtual-devices-and-streams">
  Virtual devices and streams
  <a class="anchor" href="#virtual-devices-and-streams">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Pulseaudio provides several sound processing tools implemented as 
virtual devices (sources and sinks) and virtual streams (source outputs 
and sink inputs).</p>
<ul>
<li>
<p><strong>sink monitor</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>libpulsecore</div>
  </div>
<p>Sink monitor reads samples written to the sink.</p>
<p>Every sink automatically gets an associated sink monitor. Every time 
when the sink reads a chunk from its sink inputs, it writes this chunk 
to the sink monitor.</p>
<p>The sink monitor is a sink-to-source-output adapter.</p>
</li>
<li>
<p><strong>loopback</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-loopback</div>
  </div>
<p>Loopback forwards audio from a source to a sink.</p>
<p>Loopback is implemented as a pair of a source output and sink input 
and a queue in between. Source and sink may have different clocks. To 
deal with it, loopback adjusts resampler rate on the fly to maintain 
fixed latency calculated from the queue size.</p>
<p>Loopback is a source-to-sink adapter.</p>
</li>
<li>
<p><strong>null source and sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-null-{source,sink}</div>
  </div>
<p>The null sink silently drops all data from the sink inputs. The null source writes silence to the connected source outputs.</p>
<p>As any other sink, the null sink has an associated sink monitor, 
which can be used to read all data written to the sink. Hence, the null 
sink together with its sink monitor is a sink-input-to-source-output 
adapter.</p>
</li>
<li>
<p><strong>combine sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-combine-sink</div>
  </div>
<p>This sink combines multiple sinks into one.</p>
<p>All data written to this sink is forwarded to all connected sinks. 
Combine sink creates a sink input for every connected sink and 
duplicates incoming data to every sink input.</p>
<p>Different sinks may have different clocks. To deal with it, combine 
sink adjusts sample rate for every sink input, which performs 
resampling, which may be quite CPU intensive. By default, the “trivial” 
rate conversion method is used.</p>
</li>
<li>
<p><strong>sine source and sink input</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-sine-source</div>
    <div>module-sine</div>
  </div>
<p>Sine source and sine sink input generate a sine wave with the preconfigured frequency.</p>
<p>Sine source may be connected to source outputs (e.g. application 
recording stream or an RTP sender). Sine sink input may be connected to a
 sink (e.g. ALSA sink or tunnel sink).</p>
</li>
<li>
<p><strong>pipe source and sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-pipe-{source,sink}</div>
  </div>
<p>Pipe source or sink reads or writes samples to a preconfigured file on disk. This file may be a named pipe (FIFO).</p>
</li>
</ul>
<h2 id="filter-devices">
  Filter devices
  <a class="anchor" href="#filter-devices">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Filter sources and sinks are a special category of virtual devices. 
Such source or sink has a special virtual source output or sink input 
connected to another, master source or sink.</p>
<p>There are two kinds of filters:</p>
<ul>
<li>
<p><strong>regular filters</strong></p>
<p>Such filter creates one virtual source or sink and one virtual source
 output or sink input connected to the master source or sink.</p>
</li>
<li>
<p><strong>group filters</strong></p>
<p>Such filter creates a pair of connected virtual source and sink and a
 pair of virtual source output and sink input, connected to a pair of 
master source and sink.</p>
</li>
</ul>
<p>PulseAudio treats filter devices specially in several cases:</p>
<ul>
<li>
<p><strong>thread sharing</strong></p>
<p>Unlike regular sources and sinks, filter sources and sinks don’t have
 a dedicated thread. They are running inside the thread of the master 
source or sink.</p>
</li>
<li>
<p><strong>volume sharing</strong></p>
<p>The filter source or sink always uses the same volume as its master source or sink.</p>
</li>
<li>
<p><strong>autoloading</strong></p>
<p>The filter source or sink may be automatically loaded and connected 
to a stream based on stream properties set by an application.</p>
</li>
<li>
<p><strong>routing rules</strong></p>
<p>Automatic routing rules have special cases for autoloaded filter sources and sinks.</p>
</li>
</ul>
<h2 id="regular-filters">
  Regular filters
  <a class="anchor" href="#regular-filters">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Several regular filters are available:</p>
<ul>
<li>
<p><strong>remap source and sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-remap-{source,sink}</div>
  </div>
<p>Remap source and sink act as a proxy for a master source or sink, 
performing statically configured channel remapping on top of it.</p>
</li>
<li>
<p><strong>equalizer sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-equalizer-sink</div>
  </div>
<p>This sink acts as a proxy for a master sink, implementing <a href="https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method">STFT OLA</a>-based digital equalizer on top of it. The equalizer may be configured on the fly via a D-Bus interface.</p>
</li>
<li>
<p><strong>virtual surround sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-virtual-surround-sink</div>
  </div>
<p>This sink acts as a proxy for a master sink, implementing performing a <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a> with a prerecorded <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRIR</a> WAV file to emulate surround sound when using headphones.</p>
</li>
<li>
<p><strong>virtual source and sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-virtual-{source,sink}</div>
  </div>
<p>The source code of the virtual source and sink may be used as a reference when writing a new filter source or sink.</p>
<p>Virtual source reads data from the master source and writes it to the
 connected source outputs. Virtual sink reads data from the connected 
sink inputs and writes it to the master sink.</p>
</li>
</ul>
<h2 id="echo-cancellation-filter">
  Echo cancellation filter
  <a class="anchor" href="#echo-cancellation-filter">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-echo-cancel</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Echo_suppression_and_cancellation">Acoustic echo cancellation</a>
 (AEC) is implemented as a group filter. It creates a connected pair of 
virtual source and sink, each acting as a proxy for a master source and 
sink.</p>
<p>Here is how it works:</p>
<ul>
<li>
<p>When data is sent to the virtual sink, it is forwarded to the master 
sink. Additionally, the virtual sink stores a frame of recently written 
samples.</p>
</li>
<li>
<p>When data is read from the virtual source, it is forwarded from the 
master source. Additionally, the echo is canceled using the sample frame
 stored in the virtual sink.</p>
</li>
</ul>
<img src="PulseAudio%20under%20the%20hood_files/aec.png" width="331px">
<p>Several AEC engines are implemented:</p>
<ul>
<li>
<p><strong>speex</strong></p>
<p>Acoustic echo cancellation using <a href="https://speex.org/">Speex</a> library.</p>
</li>
<li>
<p><strong>webrtc</strong></p>
<p>Acoustic echo cancellation using an <a href="https://www.freedesktop.org/software/pulseaudio/webrtc-audio-processing/">adapted copy</a> of <a href="https://en.wikipedia.org/wiki/WebRTC">WebRTC</a> implementation from Google Chromium. It also supports <a href="https://arunraghavan.net/2016/06/beamforming-in-pulseaudio/">beamforming</a>.</p>
</li>
<li>
<p><strong>adrian</strong></p>
<p>Acoustic echo cancellation NLMS-pw algorithm, <a href="http://www.andreadrian.de/echo_cancel/">originally implemented</a> in <a href="http://andreadrian.de/intercom/">VoIP Intercom</a> by Andre Adrian. The implementation uses <a href="https://gstreamer.freedesktop.org/data/doc/orc/">Orc</a> code generator.</p>
</li>
<li>
<p><strong>null</strong></p>
<p>No-op engine.</p>
</li>
</ul>
<h2 id="ladspa-plugin-sink">
  LADSPA plugin sink
  <a class="anchor" href="#ladspa-plugin-sink">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-ladspa-sink</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/LADSPA">LADSPA</a> (Linux 
Audio Developer’s Simple Plugin API) is a standard API for plugins 
implementing audio filters and audio signal processing effects.</p>
<p>PulseAudio implements LADSPA support as a filter sink. Each LADSPA 
sink loads single LADSPA plugin from a shared library. Plugin parameters
 may be configured when the sink is created or via D-Bus API on the fly.</p>
<p>A concrete example may be found in <a href="https://www.bfccomputing.com/dynamic-range-compression-for-pulseaudio/">this post</a>, which demonstrates how to configure PulseAudio to use an LADSPA plugin for <a href="https://en.wikipedia.org/wiki/Dynamic_range_compression">Dynamic Range Compression</a>.</p>
<p>Note that <a href="https://en.wikipedia.org/wiki/LV2">LADSPA Version 2</a> (LV2) standard exists, but it’s not supported in PulseAudio.</p>
<h2 id="constructing-chains">
  Constructing chains
  <a class="anchor" href="#constructing-chains">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Modules add new functionality to the server by implementing sources, 
source outputs, sinks, and sink inputs. The user then may combine them 
into a chain. However, only two types of direct connections are allowed:</p>
<ul>
<li>source output to a source</li>
<li>sink input to a sink</li>
</ul>
<p>When this is not enough, the elements of a chain have to be connected indirectly using one of the available adapters.</p>
<p>The table below summarizes all possible direct and indirect 
connections. A table row defines from where to read the samples. A table
 column defines to where to write the samples.</p>
<p>The combinations not listed in the table aren’t possible. It’s not 
possible to read samples from a source output and write samples to a 
sink input and source.</p>
<table>
  <tbody><tr>
    <th></th>
    <th>to source output</th>
    <th>to sink</th>
  </tr>
  <tr>
    <th>from source</th>
    <td>directly</td>
    <td>loopback</td>
  </tr>
  <tr>
    <th>from sink</th>
    <td>sink monitor</td>
    <td>sink monitor + loopback</td>
  </tr>
  <tr>
    <th>from sink input</th>
    <td>null sink + sink monitor</td>
    <td>directly</td>
  </tr>
</tbody></table>
<hr>
<h1 id="sample-cache">
  Sample cache
  <a class="anchor" href="#sample-cache">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p>The sample cache is an in-memory storage for short named batches of 
samples that may be uploaded to the server once and then played multiple
 times. It is usually used for event sounds.</p>
<p>Clients may create, remove, and play the sample cache entries using several protocols:</p>
<ul>
<li>“native”</li>
<li>ESound</li>
<li>D-Bus</li>
<li>CLI</li>
</ul>
<p>There are several different methods of uploading samples to the sample cache:</p>
<ul>
<li>
<p><strong>from stream</strong></p>
<p>The client connects an existing playback stream to a sample cache 
entry. All samples written to the stream will be temporarily sent to the
 sample cache entry instead of the sink input associated with the 
stream.</p>
<p>This method is used in the “native” protocol.</p>
</li>
<li>
<p><strong>from payload</strong></p>
<p>The client directly sends the samples to a sample cache entry.</p>
<p>This method is used in the D-Bus and ESound protocols.</p>
</li>
<li>
<p><strong>from file</strong></p>
<p>The client asks the server to load the samples from an audio file on disk.</p>
<p>The file can be either loaded immediately or lazily. In the latter 
case, the server loads the file in memory only when it should be played,
 and automatically unloads it if it wasn’t used for some period of time.</p>
<p>This method is used in the CLI protocol.</p>
</li>
</ul>
<p>When the client asks the server to play a sample cache entry, the 
server creates a new sink input that reads samples from the entry.</p>
<p>Depending on the protocol, the client may also provide additional 
properties for the new sink input, which, among other things, may be 
used by the routing algorithm to choose a sink to connect it to.</p>
<hr>
<h1 id="stream-management">
  Stream management
  <a class="anchor" href="#stream-management">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-native-protocol-{fd,unix,tcp}</div>
</div>
<p>Clients which use the “native” protocol should create playback or 
recording streams in order to exchange samples with the server. Every 
stream is associated with a sink input or source output on the server, 
which may be connected to a sink or source. The client and server then 
exchange asynchronous commands and chunks of samples through the stream.</p>
<p>The diagram below illustrates the logical data flow from an 
application to a sound card. Note that it doesn’t reflect that the 
client application, the native protocol module, the sink module, and the
 sound card are actually separate execution threads connected via 
queues.</p>
<div>
  <a data-lightbox="dataflow" href="https://gavv.net/articles/pulseaudio-under-the-hood/diagrams/dataflow.png">
    <img src="PulseAudio%20under%20the%20hood_files/dataflow.png">
  </a>
</div>
<h2 id="stream-types">
  Stream types
  <a class="anchor" href="#stream-types">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are three types of the “native” protocol streams:</p>
<ul>
<li>
<p><strong>recording stream</strong></p>
<p>A recording stream has a corresponding source output that is 
connected to a source. Samples are sent from the server to client. 
Control commands are sent in both directions.</p>
</li>
<li>
<p><strong>playback stream</strong></p>
<p>A playback stream has a corresponding sink input that is connected to
 a sink. Samples are sent from the client to server. Control commands 
are sent in both directions.</p>
</li>
<li>
<p><strong>upload stream</strong></p>
<p>An upload stream has a corresponding sink input that is connected to a
 sample cache entry. Samples are sent from the client to server. Control
 commands are sent in both directions.</p>
</li>
</ul>
<h2 id="client-to-server">
  Client to server
  <a class="anchor" href="#client-to-server">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The client may send the following stream commands to the server:</p>
<ul>
<li>
<p><strong>write or read samples</strong></p>
<p>The client sends (for playback streams) or receives (for recording 
streams) a chunk of samples. When sending samples, the client may either
 append samples to the stream or specify a seek offset and overwrite 
previously sent samples.</p>
</li>
<li>
<p><strong>stream properties</strong></p>
<p>The client gets or sets various stream properties, including <code>timing info</code> (current position and latency), <code>sample spec</code> (sample size, sample rate, and number of channels), <code>channel map</code> (bitmask of enabled channels), <code>format info</code> (stream encoding, PCM or hardware-specific like S/PDIF), <code>buffer attributes</code> (maximum and target buffer size, prebuffering size, request size), and <code>stream proplist</code> (list of arbitrary named properties).</p>
</li>
<li>
<p><strong>stream state</strong></p>
<p>The client gets current stream state. A stream may be playing (uncorked) or paused (corked).</p>
</li>
<li>
<p><strong>pause and resume</strong></p>
<p>The client corks (pauses) or uncorks (resumes) the stream. The stream
 is paused as soon as possible without waiting the full latency period.</p>
</li>
<li>
<p><strong>prebuffering</strong></p>
<p>The client sends <code>prebuf</code> command (start prebuffering) or <code>trigger</code>
 command (stop prebuffering). When prebuffering is started for a 
playback stream, the stream is paused until the server-side buffer 
accumulates required amount of samples. When the server receives enough 
samples from the client, it automatically starts the stream and disables
 prebuffering.</p>
</li>
<li>
<p><strong>flush</strong></p>
<p>The client drops all samples from the server-side stream buffer.</p>
</li>
<li>
<p><strong>drain</strong></p>
<p>The client asks the server to inform it when the server reads all 
samples from the server-side buffer of playback stream and it becomes 
empty and an underflow occurs. When this happens, all samples sent by 
the client are already sent to the sink, though they probably didn’t 
reach the sound card yet.</p>
</li>
</ul>
<h2 id="server-to-client">
  Server to client
  <a class="anchor" href="#server-to-client">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The client may register callbacks for the following stream commands from server:</p>
<ul>
<li>
<p><strong>request to write or read</strong></p>
<p>The server requests the client to send (for playback streams) or 
receive (for recording streams) more samples. The client should send or 
receive samples only when requested by the server to be clocked by the 
sound card timer.</p>
</li>
<li>
<p><strong>underflow and overflow</strong></p>
<p>The server acknowledges the client that an underflow (underrun) or 
overflow (overrun) occurred. Underflow occurs when trying to read from 
an empty stream buffer. Overflow occurs when trying to write to a full 
stream buffer.</p>
</li>
<li>
<p><strong>stream started</strong></p>
<p>The server acknowledges the client that the stream was automatically started after prebuffering or underrun.</p>
</li>
<li>
<p><strong>stream suspended</strong></p>
<p>The server acknowledges the client that the device the stream is 
connected to was suspended. When all streams connected to a source or 
sink remain paused for some period of time, the source or sink is 
suspended to save power.</p>
</li>
<li>
<p><strong>stream moved</strong></p>
<p>The server acknowledges the client that the stream was moved to another source or sink.</p>
</li>
<li>
<p><strong>stream event</strong></p>
<p>The server may send custom events to the client with a textual name 
and arbitrary binary payload. Currently, three event types exist: <code>request-cork</code> (the client should pause stream), <code>request-uncork</code> (the client should unpause stream), <code>format-lost</code> (the stream was moved to another source or sink that doesn’t support encoding being used currently).</p>
</li>
</ul>
<h2 id="buffering">
  Buffering
  <a class="anchor" href="#buffering">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The diagram below shows what buffers are used to transfer sample chunks from the client to the sound card. See also <a href="http://voices.canonical.com/david.henningsson/2014/11/21/pulseaudio-buffers-and-protocol/">this post</a> for an overview of buffering in PulseAudio.</p>
<img src="PulseAudio%20under%20the%20hood_files/buffers.png">
<p>The following buffers are employed:</p>
<ul>
<li>
<p><strong>device buffer</strong></p>
<p>The sink reads samples from connected sink inputs and stores them 
into the device buffer. In the case of an ALSA sink, this is the 
kernel-side ALSA ring buffer. The sound card reads samples from it.</p>
<p>The size of this buffer is equal to the minimum target stream buffer 
size among of the all sink inputs connected to sink. This is so to have 
the largest possible latency still meeting the requirements of all 
clients.</p>
</li>
<li>
<p><strong>render queue</strong></p>
<p>The sink input returns samples from its render queue. When the render
 queue doesn’t contain enough samples, the sink input pops a chunk from 
the stream buffer, converts it to the format requested by sink using 
resampler, and pushes to the render queue.</p>
<p>The size of this buffer is equal to the size of device buffer (for 
samples that were already passed to the sink, needed for rewinding) plus
 a zero or small amount of samples (for samples that were not yet passed
 to the sink, appearing only when the stream provided a larger chunk 
than requested and only a part of the chunk was read).</p>
</li>
<li>
<p><strong>resampler buffer</strong></p>
<p>Depending on the resampling method, resampler may introduce its own buffering between the stream buffer and render queue.</p>
<p>The size of this buffer is zero or one chunk.</p>
</li>
<li>
<p><strong>stream buffer</strong></p>
<p>The server pushes chunks received from client to the stream buffer.</p>
<p>The server maintains the stream buffer size near to the target stream
 buffer size requested by the client via the buffer attributes. Server 
achieves this by adjusting the number of samples requested from client 
each time. Therefore, the properly written client should try to respond 
with the requested number of samples, in average.</p>
<p>The size of this buffer is equal to the size of render queue (for 
samples that were already passed to the render queue, needed for 
rewinding) plus some amount of samples (for samples that were not yet 
passed to the render queue, needed to achieve the target latency).</p>
</li>
<li>
<p><strong>socket buffer(s)</strong></p>
<p>When the client sends a chunk to the server, the chunk may pass through zero, one, or two sockets buffers:</p>
<ul>
<li>
<p>If a TCP socket is used, both client and server have their own kernel-side socket buffers. Two socket buffers in total.</p>
</li>
<li>
<p>If a Unix domain socket is used, the client and server share the same kernel-side socket buffer. One socket buffer in total.</p>
</li>
<li>
<p>If a Unix domain socket is used, and the zero-copy mode is enabled, 
the client and server use the same user space shared memory pool, so 
that no kernel-side buffer is used. In this case, the client allocates a
 chunk and sends its ID to the server, and the server pushes it to the 
stream buffer. Zero socket buffers in total.</p>
</li>
</ul>
<p>The size of this buffer(s) is determined only by network or scheduling delay. The <a href="https://en.wikipedia.org/wiki/Nagle's_algorithm">Nagle’s algorithm</a> is disabled by PulseAudio for TCP sockets and is never used for Unix domain sockets.</p>
</li>
<li>
<p><strong>client buffer</strong></p>
<p>The client performs no buffering except a single chunk that it’s going to send to the server.</p>
<p>So the size of this buffer is no more than one chunk.</p>
</li>
</ul>
<h2 id="rewinding">
  Rewinding
  <a class="anchor" href="#rewinding">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Rewinding is a process of overwriting existing samples in buffers 
instead of appending to them. Implementation details are described in 
the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Rewinding/">Rewinding</a> page.</p>
<p>It is an important feature that is necessary to combine a higher 
latency for playback (to reduce glitches) with a lower latency for 
out-of-band requests like pause and volume changes (to improve user 
experience).</p>
<p>For example, the volume change should be applied immediately even 
when the playback latency is 2s. To achieve this, all buffers that 
contain samples with an older volume value are rewound and refilled.</p>
<h2 id="rewinding-and-buffers">
  Rewinding and buffers
  <a class="anchor" href="#rewinding-and-buffers">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Rewinding works by moving read and write pointers of the ring buffers.</p>
<img src="PulseAudio%20under%20the%20hood_files/rewind_buffers.png" width="660px">
<p>The three buffers are employed:</p>
<ul>
<li>
<p><strong>device buffer</strong></p>
<p>Normally, the sound card moves the read pointer forward, and the sink
 moves the write pointer forward. On rewind, the sink moves the write 
pointer backward. The read pointer can’t be moved backward because the 
sound card has already played the samples.</p>
</li>
<li>
<p><strong>render queue</strong></p>
<p>Normally, the sink moves the read pointer forward, and the sink input
 moves the write pointer forward. On rewind, the sink input moves the 
read pointer backward, so that the sink can re-read required amount of 
samples.</p>
<p>To support this, the render queue always keeps some amount of samples
 before the read pointer, equal to the size of the device buffer.</p>
</li>
<li>
<p><strong>stream buffer</strong></p>
<p>Normally, the sink input moves the read and write pointers forward. On rewind, the read pointer is moved backward.</p>
<p>To support this, the stream buffer always keeps some amount of 
samples before the read pointer, equal to the size of the render queue, 
which includes the size of the device buffer.</p>
</li>
</ul>
<h2 id="rewind-processing">
  Rewind processing
  <a class="anchor" href="#rewind-processing">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>A rewind is separated into two parts:</p>
<ul>
<li>rewind request</li>
<li>rewind processing</li>
</ul>
<p>Rewind request may be issued on a sink input or sink. Sink input 
always propagates rewind requests to the sink. Rewind processing always 
starts from the sink and then goes down to all connected sink inputs.</p>
<p>The diagram below illustrates the overall algorithm.</p>
<img src="PulseAudio%20under%20the%20hood_files/rewind_flow.png" width="660px">
<p>The steps are:</p>
<ul>
<li>
<p>If a rewind was requested for a sink input, it is propagated to the sink. If a rewind was requested for a sink, it is processed.</p>
</li>
<li>
<p>The sink processes the rewind. It moves back the write pointer of the
 device buffer as much as possible. The rewind request may be truncated 
if some samples were already played and can’t be rewound.</p>
</li>
<li>
<p>The sink asks all connected sink inputs to process the rewind and passes them the number of samples actually rewound.</p>
</li>
<li>
<p>The sink input processes the rewind. First, it moves back the read 
pointer of the render queue. If it’s not enough, it also moves back the 
read pointer of the stream buffer.</p>
</li>
</ul>
<h2 id="rewind-requests">
  Rewind requests
  <a class="anchor" href="#rewind-requests">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Here is the list of cases when a rewind request is issued on a sink input or sink:</p>
<ul>
<li>
<p><strong>stream write with a non-zero seek offset</strong></p>
<p>The client explicitly overwrites previously written samples by 
specifying a seek offset. Buffers are rewound to overwrite unplayed 
samples.</p>
</li>
<li>
<p><strong>stream flushed</strong></p>
<p>The client explicitly drops buffered samples. Buffers are rewound to drop unplayed samples.</p>
</li>
<li>
<p><strong>sink input corked or uncorked</strong></p>
<p>The sink input is paused or unpaused via the stream or introspection 
API or by an automatic rule. When the stream is paused, buffers are 
rewound to drop unplayed samples. When the stream is unpaused, buffers 
are rewound to start playing newly available samples immediately.</p>
</li>
<li>
<p><strong>sink input volume changed or muted</strong></p>
<p>The sink input volume is changed or muted via the stream or 
introspection API, by the sink, or by an automatic rule. Buffers are 
rewound to apply the new volume immediately.</p>
</li>
<li>
<p><strong>sink input removed or moved to another sink</strong></p>
<p>The sink input is removed or moved due to client disconnect, via the 
introspection API, or by an automatic rule. Buffers are rewound to drop 
unplayed samples of the stream.</p>
</li>
<li>
<p><strong>sink input underrun ended</strong></p>
<p>The sink input was in underrun and finally had provided the samples. 
Buffers are rewound to overwrite silence with newly available samples 
and play them immediately.</p>
</li>
<li>
<p><strong>sink volume changed or muted</strong></p>
<p>The sink volume is changed or muted via the introspection API, by 
hardware, or by an automatic rule. Buffers are rewound to apply the new 
volume immediately.</p>
</li>
<li>
<p><strong>sink latency decreased</strong></p>
<p>The sink decreases its latency due to a new stream connection with a 
lower latency requirement or on a watermark decrease. Buffers are 
rewound to shrink the device buffer.</p>
</li>
<li>
<p><strong>sink parameters changed</strong></p>
<p>The parameters of a virtual sink (like equalizer sink) are changed. Buffers are rewound to apply new parameters immediately.</p>
</li>
</ul>
<h2 id="moving-streams">
  Moving streams
  <a class="anchor" href="#moving-streams">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>At any time, a sink input or source output may be moved to another sink or source.</p>
<p>The move may be initiated explicitly by any application (typically 
via the mixer GUI like pavucontrol) or automatically by the routing 
policy (typically when a device is inserted or removed).</p>
<p>When the stream is moved, a rewind is requested to drop its samples from the sink or source it was previously connected to.</p>
<h2 id="synchronized-streams">
  Synchronized streams
  <a class="anchor" href="#synchronized-streams">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>When a client creates a stream, it may configure it to be synchronized with another stream.</p>
<p>PulseAudio guarantees that all streams synchronized together always 
go sample-by-sample. To achieve this, it automatically propagates 
control commands issued on a stream (like pause and resume) to all 
synchronized streams.</p>
<p>It’s currently not possible to move a synchronized stream to another device.</p>
<h2 id="monitoring">
  Monitoring
  <a class="anchor" href="#monitoring">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The client can monitor existing devices and streams:</p>
<ul>
<li>
<p>To monitor a source, the client just connects a recording stream to the source.</p>
</li>
<li>
<p>To monitor a source output, the client connects a recording stream to
 the source to which the source output is connected to. This is enough 
because all source outputs connected to the same source get the same 
data.</p>
</li>
<li>
<p>To monitor a sink, the client connects a recording stream to the corresponding sink monitor.</p>
</li>
<li>
<p>To monitor a sink input, the client connects a recording stream to the corresponding sink monitor and sets the <a href="https://freedesktop.org/software/pulseaudio/doxygen/stream_8h.html#a2d4b414edaa95ed08ed7e3b321a208d0">monitor stream</a> of the recording stream to the identifier of the specific sink input to be monitored.</p>
</li>
</ul>
<hr>
<h1 id="time-management">
  Time management
  <a class="anchor" href="#time-management">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>Playback and recording are driven by a per-device timer-based scheduler that provides clocking and maintains optimal latency.</p>
<p>The diagram below illustrates the process. It shows the path of 
samples from an application (on the left) to a sound card (on the right)
 when using the “native” protocol.</p>
<div>
  <a data-lightbox="sequence" href="https://gavv.net/articles/pulseaudio-under-the-hood/diagrams/sequence.png">
    <img src="PulseAudio%20under%20the%20hood_files/sequence.png">
  </a>
</div>
<h2 id="clocking">
  Clocking
  <a class="anchor" href="#clocking">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are no two devices with equal clocks. One of them is always 
slightly faster and another is slightly slower. This applies both to a 
pair of computers, as well as to a pair of separately clocked devices on
 the same computer.</p>
<p>Since sound cards have their own clocks, an application can’t use a 
CPU timer to send samples to the sound card. Instead, the application 
should be clocked by the sound card, i.e. use a timer that runs in the 
sound card time domain.</p>
<p>In PulseAudio, clocking is provided by sources and sinks. Hardware 
source or sink runs a thread that writes or read samples to source 
outputs or sink inputs using a timer synchronized with the sound card.</p>
<h2 id="clocking-and-native-protocol">
  Clocking and native protocol
  <a class="anchor" href="#clocking-and-native-protocol">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-native-protocol-{fd,unix,tcp}</div>
</div>
<p>Every application stream running over the “native” protocol is 
clocked by the source or sink to which it is connected. Every client 
stream has an associated source output (for recording streams) or sink 
input (for playback streams) on the server.</p>
<p>When source writes samples to the source output, source output 
forwards them to the client stream. When sink reads samples from the 
sink input, sink input requests desired amount of samples from the 
client stream.</p>
<p>When the asynchronous API is used, a callback is invoked when the 
server requests more samples. The callback should respond with a desired
 amount of samples. When the simple API is used, the client is blocked 
until the server requests more samples.</p>
<h2 id="clocking-and-rtp">
  Clocking and RTP
  <a class="anchor" href="#clocking-and-rtp">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-rtp-recv</div>
</div>
<p>An RTP sender can’t be clocked by an RTP receiver because the sender 
has no feedback from the receiver and there may be multiple receivers 
for a single multicast sender. In result, the receiver queue size is 
slowly but constantly increasing or decreasing.</p>
<p>Sooner or later, it will cause an underrun (the next sample to play 
is not received yet) or an overrun (the received sample is dropped 
because it came too early and the queue is still full). The user will 
hear glitches.</p>
<p>When I was running a demo sender and receiver on two computers, the 
clock difference was about 0.0055%. This means that every hour the first
 timer outruns the second by approximately 200 milliseconds. In other 
words, if the latency is about 200 ms, the playback has to be restarted 
every hour.</p>
<p>To prevent this, PulseAudio RTP receiver adjusts resampler rate on the fly to maintain constant queue size:</p>
<ul>
<li>
<p>when the queue size becomes too high, the rate is slightly increased,
 the samples are played a bit faster, and after a while, the queue size 
decreases</p>
</li>
<li>
<p>when the queue size becomes too low, the rate is slightly decreased, 
the samples are played a bit slower, and after a while, the queue size 
increases</p>
</li>
</ul>
<p>To prevent oscillations, an exponentially weighted average of the 
estimated rate is used. To prevent latency jumps, the rate is updated 
gradually with small steps. Algorithm details are described <a href="https://github.com/pulseaudio/pulseaudio/blob/master/src/modules/rtp/module-rtp-recv.c#L314">in the source code</a>.</p>
<h2 id="latency">
  Latency
  <a class="anchor" href="#latency">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">  <div class="flex_th">component</div>
  <div>libpulse</div>
  <div>libpulsecore</div>
  <div>module-native-protocol-{fd,unix,tcp}</div>
  <div>module-alsa-{source,sink}</div>
</div>
<p>Simply speaking, latency is the delay between sound being played and 
heard, or between being emitted and recorded. More accurately, we are 
interested in the delay between a client application and an analog input
 or output.</p>
<p>When a playback stream is connected to a sink, the following happens:</p>
<ul>
<li>on server request, the client sends samples to the server via a socket or a shared ring buffer</li>
<li>the sink input gets an I/O event, reads samples and writes them to the stream buffer</li>
<li>on timer tick, PulseAudio sink reads samples from the sink input 
stream buffer, performs resampling and mixing, and writes result to the 
ALSA ring buffer</li>
<li>on timer tick, the DMA reads samples from the ALSA ring buffer and writes it to the sound card</li>
<li>the sound card passes samples to the codec, which writes the result to the analog output</li>
</ul>
<p>The same is happening with a recording stream, in reverse order:</p>
<ul>
<li>the codec reads samples from the analog input and passes them to the DMA</li>
<li>the DMA writes the samples to the ALSA ring buffer</li>
<li>on timer tick, PulseAudio source reads samples from the ALSA ring buffer and writes them to source outputs</li>
<li>source output reads samples from the stream buffer and sends them to the client</li>
<li>the client gets and I/O event and reads samples</li>
</ul>
<p>Accordingly, the latency can’t be less than the sum of:</p>
<ul>
<li>the time to transmit a chunk of samples (or a chunk header in the zero-copy mode) between the client and the server</li>
<li>the time to process samples, e.g. the time to resample and mix 
chunks from all sink inputs connected to the sink in case of a playback 
stream</li>
<li>the number of queued samples in the ALSA ring buffer</li>
<li>the time to transmit a chunk of samples via DMA</li>
<li>the time to encode or decode samples in the sound card codec</li>
</ul>
<p>PulseAudio can control the stream buffer size and the ALSA ring 
buffer size. The rest components can’t be controlled and are determined 
by the hardware capabilities. To achieve the target overall latency, 
PulseAudio measures the current overall latency and then adjusts the 
buffer sizes accordingly.</p>
<p>The driver and hardware latency, which includes the size of the ALSA 
ring buffer, the DMA delay, and the sound card codec delay, is measured 
using the <a href="https://www.kernel.org/doc/html/v4.10/sound/designs/timestamping.html">ALSA PCM timestamping</a>.</p>
<p>On the diagram above, the minimum possible latency is shown as a 
vertical bar between the two red lines. Since the client application, 
the PulseAudio network thread, the PulseAudio device thread, and the 
sound card run in parallel and may be clocked independently, there are 
additional small delays caused by scheduler jitter and non-synchronous 
timers. So in practice, the minimum possible latency will be a bit 
higher than shown in the diagram.</p>
<p>Note that compared to bare ALSA, PulseAudio increases the minimum possible latency:</p>
<ul>
<li>the client, the network thread, and the device thread are separate processes, so there are additional context switches</li>
<li>the client communicates with the server via IPC, so there are non-zero round trip times (even in the zero-copy mode)</li>
</ul>
<p>On the other hand, PulseAudio may operate at a lower latency than a 
naive implementation of an ALSA client based on select/poll, due to its 
advanced timer-based scheduler (see below).</p>
<h2 id="controlling-latency">
  Controlling latency
  <a class="anchor" href="#controlling-latency">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Every source, sink, source output, and sink input has its own 
latency. PulseAudio server controls all of them and can adjust latency 
on the fly to reach the minimum acceptable value that causes no 
glitches. This value may depend on things like hardware capacity and 
current system load. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/LatencyControl/">LatencyControl</a> page.</p>
<p>An application can set its own latency requirements for a stream, using these four parameters of the stream buffer:</p>
<ul>
<li><code>maxlength</code> - the maximum number of bytes in buffer</li>
<li><code>tlength</code> - the desired number of bytes in buffer, i.e. the target latency</li>
<li><code>prebuf</code> - the minimum number of bytes to be accumulated in buffer before starting the stream, i.e. the start threshold</li>
<li><code>minreq</code> - the minimum number of bytes to be requested from client each time</li>
</ul>
<p>For every stream, PulseAudio server maintains a constant latency, depending on <em>adjust latency</em> mode that may be enabled per-stream by an application:</p>
<ul>
<li>
<p>If adjust latency mode is disabled, <code>tlength</code> specifies 
target size of the stream buffer. PulseAudio server requests or sends 
samples to the client in such way that there is always about <code>tlength</code> bytes in the stream buffer.</p>
</li>
<li>
<p>If adjust latency mode is enabled, <code>tlength</code> specifies 
desired size of the stream buffer plus the device buffer. Device buffer 
size is controlled by the source or sink implementation. Let’s call it <code>dlength</code>.</p>
<p>In the case of ALSA source or sink, <code>dlength</code> corresponds 
to the driver and hardware latency, which includes the size of the ALSA 
ring buffer, the DMA delay, and the sound card codec delay.</p>
<p>In other words, in this mode <code>tlength</code> specifies the desired latency between the client and the sound card. To reach it, PulseAudio server does two things:</p>
<ul>
<li>
<p>adjusts the <code>dlength</code> to be the minimum <code>tlength</code> value among of the all sink inputs connected to sink</p>
</li>
<li>
<p>requests or sends samples to the client in such way that there is always about <code>tlength - dlength</code> bytes in the stream buffer</p>
</li>
</ul>
</li>
</ul>
<p>Note that the actual stream latency may be higher than requested by 
an application. PulseAudio automatically increases the latency depending
 on hardware and OS scheduler constraints. In particular, the latency is
 increased in case of frequent ALSA underruns to avoid glitches.</p>
<h2 id="measuring-latency">
  Measuring latency
  <a class="anchor" href="#measuring-latency">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio provides clients with the stream <a href="https://freedesktop.org/software/pulseaudio/doxygen/structpa__timing__info.html">timing info</a>, containing the stream latency divided into the three components:</p>
<ul>
<li>
<p><strong>transport latency</strong></p>
<p>The time required to send a sample from the client to the server-side
 stream buffer. Set to the half of the round trip time, calculated on 
the client when it sends the timing info request and receives the 
response.</p>
</li>
<li>
<p><strong>stream buffer latency</strong></p>
<p>The size of the server-side stream buffer. Equal to the difference between the write index and read index.</p>
<p>For playback stream, the client advances the write index and the sink
 advances the read index. For recording stream, the source advances the 
write index and the client advances the read index.</p>
</li>
<li>
<p><strong>sink or source latency</strong></p>
<p>The latency reported by sink or source. In general, it represents delay between the stream buffer and the sound card.</p>
<p>For ALSA devices, it is approximately equal to the size of the chunk 
queue between the sink and sink input, plus the size of the ALSA ring 
buffer, plus the DMA delay, plus the sound card codec delay.</p>
</li>
</ul>
<p>Timing info may be retrieved manually by application or automatically when the <em>auto timing update</em> flag is set for a stream.</p>
<p>The client library uses timing info to calculate two values:</p>
<ul>
<li>
<p><strong>stream time</strong></p>
<p><a href="https://freedesktop.org/software/pulseaudio/doxygen/stream_8h.html#a9b1caba84c7a5c90efdbcaed31e9dfca">Stream time</a> is the timestamp of the sample being currently played or recorded on the sound card:</p>
<ul>
<li>playback streams: <code>stream_time = streambuf_read_index - sink_latency + transport_latency</code></li>
<li>recording streams: <code>stream_time = streambuf_write_index + source_latency + transport_latency</code></li>
</ul>
<p>The transport latency is added to take into account the number of 
samples played or recorded on the server during the time elapsed between
 the timing info was sent from the server and received on the client.</p>
</li>
<li>
<p><strong>stream latency</strong></p>
<p><a href="https://freedesktop.org/software/pulseaudio/doxygen/stream_8h.html#aa521efcc16fe2abf0f8461462432ac16">Stream latency</a>
 is the difference between the timestamps of the last sample sent or 
received by the client and the sample being currently played or recorded
 on the sound card:</p>
<ul>
<li>playback streams: <code>stream_latency = streambuf_write_index - stream_time</code></li>
<li>recording streams: <code>stream_latency = stream_time - streambuf_read_index</code></li>
</ul>
<p>Or, equally:</p>
<ul>
<li>playback streams: <code>stream_latency = streambuf_latency + sink_latency - transport_latency</code></li>
<li>recording streams: <code>stream_latency = streambuf_latency + source_latency - transport_latency</code></li>
</ul>
</li>
</ul>
<p>Usually, the stream time value is not used directly in the 
calculation above. Instead, it is postprocessed before reporting to the 
application or calculating the stream latency:</p>
<ul>
<li>
<p>unless the <em>not monotonic</em> flag is set for the stream, the client ensures that the stream time never steps back;</p>
</li>
<li>
<p>if the <em>interpolate timing</em> flag is set for the stream, the client interpolates and smooths the stream time between timing info updates.</p>
</li>
</ul>
<p>Actual calculations in the source code differ from the formulas above in two details:</p>
<ul>
<li>
<p>on signed overflows, negative values may be truncated to zero or reported separately;</p>
</li>
<li>
<p>when recording stream is connected to a monitor source, the latency 
of the monitored sink is taken into account in addition to the latency 
of monitor source.</p>
</li>
</ul>
<h2 id="latency-and-backends">
  Latency and backends
  <a class="anchor" href="#latency-and-backends">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Non-ALSA backends generally don’t support adjusting device buffer 
size. An application can determine if a source or sink supports it by 
checking the <em>dynamic latency</em> flag of the device.</p>
<p>Some backends, including Bluetooth devices, don’t provide accurate 
information about the actual latency. This information is important for 
some applications, notably for the <a href="https://en.wikipedia.org/wiki/Lip_sync">lip sync</a> in video players.</p>
<p>To workaround problems with such backends, the user can manually set the <em>latency offset</em>
 for a device port, which is zero by default. When a source or sink is 
connected to a device port, the latency offset of the device port is 
added to the latency (device buffer size) reported by the source or 
sink.</p>
<h2 id="alsa-challenges">
  ALSA challenges
  <a class="anchor" href="#alsa-challenges">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>When a program uses ALSA, the program writes or reads samples from 
the ALSA ring buffer, and the sound card reads or writes samples from 
that buffer at a timer tick. The user can configure the <em>buffer size</em> (number of samples in whole ring buffer) and the <em>period size</em> (a.k.a. <em>fragment size</em>, the number of samples written or read per one timer tick).</p>
<p>As noted above, the program should be clocked by the sound card. 
Traditionally, this is achieved by either blocking on ALSA write or read
 operation until the sound card updates the ring buffer, or waiting for 
it using poll or select.</p>
<p>This way, the whole process is driven by the sound card timer, which 
is good. However, two problems may be encountered when developing ALSA 
client:</p>
<ul>
<li>
<p>If poll or select is used, there is always a short delay between the 
moment when the sound card reads from the ring buffer and the program 
writes next chunk of samples to it. The time is spent for a context 
switch from the kernel space to the user space, returning from poll or 
select, issuing the next write call, and finally doing one more context 
switch from the user space to the kernel space.</p>
<p>On low period sizes, this delay can cause glitches. The delay may be 
avoided if the program uses blocking write instead of poll or select, 
but this doesn’t allow to do I/O multiplexing, which may be necessary.</p>
</li>
<li>
<p>It’s not easy to guess optimal buffer size and period size because 
they depend on the latency, hardware, CPU, and average system load. When
 default parameters chosen by ALSA doesn’t play well enough, the client 
programming becomes more tricky.</p>
</li>
</ul>
<h2 id="timer-based-scheduler-tsched">
  Timer-based scheduler (tsched)
  <a class="anchor" href="#timer-based-scheduler-tsched">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-alsa-{source,sink}</div>
</div>
<p>PulseAudio addresses these challenges by doing its own <a href="http://0pointer.de/blog/projects/pulse-glitch-free.html">timer-based scheduling</a>,
 also known as the glitch-free playback model. It was introduced in 
0.9.11 version and inspired by audio systems in Windows Vista and MacOS.</p>
<p>With the timer-based scheduling, PulseAudio is able to fulfill the two requirements at the same time:</p>
<ul>
<li>
<p>the server usually doesn’t introduce glitches by itself even with low latency values</p>
</li>
<li>
<p>the client doesn’t bother about the ring buffer parameters and 
advanced timing techniques and can just request the latency it needs</p>
</li>
</ul>
<p>Without it, the glitches were more common when using PulseAudio with 
low latency values, and applications had to either switch to a higher 
latency or use ALSA directly.</p>
<p>Here are its basic ideas:</p>
<ul>
<li>
<p><strong>Timer</strong></p>
<p>Instead of using poll or select on ALSA device, PulseAudio configures
 its own timer and uses poll or select on the timer. To avoid glitches, 
PulseAudio timer is configured to fire some time before the sound card 
timer, so that PulseAudio has enough time to write next chunk of samples
 to the ring buffer.</p>
</li>
<li>
<p><strong>Synchronization</strong></p>
<p>PulseAudio monitors the size of ALSA ring buffer and adjusts the 
timer to be synchronous with the sound card timer. To avoid 
oscillations, the sleep period for the PulseAudio timer is updated 
smoothly.</p>
</li>
<li>
<p><strong>Watermark</strong></p>
<p>PulseAudio maintains a watermark for the number of unread bytes in 
the ring buffer. When this number becomes lower than the watermark, or 
(in the worst case) an underrun occurs, the watermark is increased. When
 this number becomes higher than the watermark, the watermark is
decreased.</p>
<p>The key point of the watermark is that PulseAudio can detect and 
prevent an underrun until it really happens. The watermark value affects
 two things:</p>
<ul>
<li>
<p>The delta between PulseAudio timer and ALSA timer. The higher is the 
watermark, the greater the delta, so that PulseAudio has more time to 
fill the ring buffer before ALSA timer tick happens.</p>
</li>
<li>
<p>The latency. When the watermark becomes too high, the latency is 
increased. When the watermark becomes low again, the latency is 
decreased back. This means that the actual latency may be higher than 
requested.</p>
</li>
</ul>
</li>
<li>
<p><strong>Interrupts</strong></p>
<p>On every tick of the sound card timer, an interrupt (a.k.a. period 
wakeup) is generated, and the process blocked on ALSA write or poll is 
woken up. PulseAudio doesn’t need this, so it tries to reduce the number
 of interrupts to lower CPU usage.</p>
<p>To reduce the number of interrupts, the buffer size and period size 
(a.k.a. fragment size) are set as large as supported by hardware. 
Typical values are 2s buffer size and 1s or 0.5s period size.</p>
<p>If supported by the sound card driver, interrupts for the sound card 
timer are disabled at all. This only works with recent ALSA versions and
 some drivers. This should work at least with recent Intel drivers.</p>
</li>
<li>
<p><strong>Rewinding</strong></p>
<p>To provide instant reaction on user-input, PulseAudio uses the ALSA 
feature of buffer rewriting. Whenever an application performs a seek, 
pause, or writes more samples, PulseAudio rewrites the ring buffer with 
actual data.</p>
</li>
<li>
<p><strong>Clocking</strong></p>
<p>When a sink reads samples from a sink input, the sink input requests 
more samples from the application. The process is driven by the sink 
timer, which is kept synchronous with the sound card timer. The samples 
are requested some time before they should be written to the ring 
buffer, so the application has time to receive the request and response 
with a chunk of samples.</p>
</li>
</ul>
<p>The timer-based scheduler may be enabled or disabled globally or per-sink. By default, it is automatically enabled when:</p>
<ul>
<li>the card is a real hardware device</li>
<li>the card driver supports mmap</li>
<li>the card driver doesn’t use double buffering (batch mode), so that the real device buffer may be updated at any time</li>
<li>PulseAudio is not running under a virtual machine (yes, it has some code to detect this)</li>
</ul>
<hr>
<h1 id="power-saving">
  Power saving
  <a class="anchor" href="#power-saving">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>Several techniques are used to save the power. Some benchmarks may be found in <a href="http://linux-tipps.blogspot.ru/2011/04/power-performance-of-pulseaudio-alsa.html">this post</a>.</p>
<h2 id="device-states">
  Device states
  <a class="anchor" href="#device-states">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-suspend-on-idle</div>
</div>
<p>A source or sink may be in one of the following states:</p>
<ul>
<li>
<p><strong>RUNNING</strong></p>
<p>Actively performing I/O. There are non-paused connected streams.</p>
</li>
<li>
<p><strong>IDLE</strong></p>
<p>Actively performing I/O, but there are no non-paused connected 
streams. In the case of a sink, zero samples are written to the device. 
In the case of a source, recorded samples are dropped.</p>
</li>
<li>
<p><strong>SUSPENDED</strong></p>
<p>Not performing any I/O.</p>
</li>
</ul>
<p>The source or sink is marked idle when there are no non-paused 
connected streams. If it remains in this state for some time, it may be 
suspended. When a non-paused stream appears again, the source or sink is
 resumed.</p>
<h2 id="reducing-interrupts">
  Reducing interrupts
  <a class="anchor" href="#reducing-interrupts">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-alsa-{source,sink}</div>
</div>
<p>The less frequently sound card interrupts occur, the less frequently the driver wakes up, the less power is used.</p>
<p>When the timer-based scheduler is used, PulseAudio reduces the number
 of sound card interrupts or completely disables them if it’s supported 
by the driver.</p>
<h2 id="default-latency">
  Default latency
  <a class="anchor" href="#default-latency">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-native-protocol-{fd,unix,tcp}</div>
</div>
<p>The higher is latency, the less frequently the server wakes up, the less power is used.</p>
<p>High latency may be set for a stream automatically:</p>
<ul>
<li>
<p>If an application uses PulseAudio and didn’t specify the latency, 
PulseAudio automatically selects the default value, which is high, 
typically 2s.</p>
</li>
<li>
<p>If an application uses GStreamer, the user can configure GStreamer to
 select a high latency automatically for applications which media role 
is “music”, as described in <a href="https://arunraghavan.net/2011/05/more-pulseaudio-power-goodness/">this post</a>. However, GStreamer doesn’t do it by default.</p>
<p>GStreamer uses PulseAudio as a backend and is a backend itself for 
many applications and higher-level media frameworks like Xine and 
Phonon. See details <a href="http://tuxradar.com/content/how-it-works-linux-audio-explained">here</a>.</p>
</li>
</ul>
<hr>
<h1 id="automatic-setup-and-routing">
  Automatic setup and routing
  <a class="anchor" href="#automatic-setup-and-routing">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>PulseAudio automatically restores parameters for cards, devices, and 
streams, routes streams to devices, and performs other housekeeping 
actions.</p>
<p>Some examples:</p>
<ul>
<li>
<p>When a new card, device, or stream appears, the server should restore previously configured parameters.</p>
</li>
<li>
<p>When a card or device appears, the server may move existing streams 
to it. When a card or device disappears, the server may move existing 
streams to another device.</p>
</li>
<li>
<p>When a client creates a new stream, the server should route it to some device.</p>
</li>
<li>
<p>When a client creates a new stream, the server may perform some 
automatic setup depending on stream properties, like autoloading sound 
processing tools, or silencing less important streams.</p>
</li>
</ul>
<h2 id="databases">
  Databases
  <a class="anchor" href="#databases">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p>PulseAudio uses an embedded database to store parameters and routing rules persistently. It supports three database backends:</p>
<ul>
<li><a href="http://www.gnu.org.ua/software/gdbm/">gdbm</a> (GNU dbm)</li>
<li><a href="https://tdb.samba.org/">tdb</a></li>
<li>“simple” (built-in hashtable-based implementation)</li>
</ul>
<p>Two separate databases are involved in routing:</p>
<ul>
<li>restoration database</li>
<li>device manager database</li>
</ul>
<h2 id="stream-roles">
  Stream roles
  <a class="anchor" href="#stream-roles">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p>The routing and automatic setup are heavily based on roles. A role is
 a short string describing media type, like “video”, “music”, or 
“phone”.</p>
<p>Roles are used in several places:</p>
<ul>
<li>every stream may have the “media.role” property provided by application</li>
<li>every device may have the “device.intended_roles” property provided by the device backend</li>
<li>the restoration database may contain per-role routing rules</li>
<li>the device manager database may contain per-role priority lists of routing rules</li>
</ul>
<h2 id="stream-groups">
  Stream groups
  <a class="anchor" href="#stream-groups">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p>Two streams belong to the same group if they have the same group 
identifier. PulseAudio checks the following stream properties and uses 
the first available one as the group identifier of the stream:</p>
<ul>
<li>“media.role” (e.g. “video”, “music”, or “phone”)</li>
<li>“application.id” (e.g. “org.gnome.Totem”)</li>
<li>“application.name” (e.g. “Totem Music Player”)</li>
<li>“media.name” (e.g. “Pink Floyd - Astronomy Domine”)</li>
</ul>
<p>If none of these properties are present in the stream property list, 
PulseAudio uses the default group identifier, which is the same for all 
streams.</p>
<p>Note that more generic properties are preferred over more specific ones.</p>
<p>Stream groups are used in two places:</p>
<ul>
<li>in the restoration database</li>
<li>when autoloading group filters</li>
</ul>
<h2 id="stream-routing">
  Stream routing
  <a class="anchor" href="#stream-routing">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-stream-restore</div>
  <div>module-device-manager</div>
  <div>module-intended-roles</div>
</div>
<p>Routing is a process of choosing to which device (source or sink) to 
connect a new stream (source output or sink input). A good routing 
overview can be found in <a href="http://colin.guthr.ie/2010/02/this-is-the-route-to-hell/">this post</a>.</p>
<p>Routing consists of several steps:</p>
<ul>
<li>
<p><strong>application device</strong></p>
<p>If the client has specified a device name, the stream is routed to that device.</p>
<p>This usually means that the user has configured the application to 
use a specific device. Some applications may provide a command line 
option or GUI for that.</p>
</li>
<li>
<p><strong>module-stream-restore</strong></p>
<p>Otherwise, module-stream-restore (enabled by default) checks the 
restoration database. If there is a stored device for the stream group, 
and the device is currently available, the stream is routed to that 
device.</p>
<p>This means that the user had moved a stream from the same stream 
group to that device earlier, and this decision was remembered. The 
restoration database is updated only when the user manually moves a 
stream via GUI. It’s not affected by automatic routing.</p>
</li>
<li>
<p><strong>module-device-manager</strong></p>
<p>Otherwise, module-device-manager (enabled in KDE) checks its per-role
 and global priority lists of devices. If there is a non-empty priority 
list for the stream role or non-empty global priority list, and there 
are currently available devices in the list, the stream is routed to the
 first such device.</p>
<p>This means that the user has configured some priority lists to be used for a role or globally. KDE provides a GUI for that.</p>
</li>
<li>
<p><strong>module-intended-roles</strong></p>
<p>Otherwise, module-intended-roles (enabled by default) searches for a 
device which intended role list contains the stream role. If such device
 exists, the stream is routed to that device.</p>
<p>In other words, a per-role default device is used. The intended role 
list of a device is provided by the device backend. It can also be set 
manually when creating the device.</p>
</li>
<li>
<p><strong>fallback device</strong></p>
<p>Otherwise, the stream is routed to the fallback device.</p>
<p>Fallback source and fallback sink may be changed by the user.</p>
</li>
</ul>
<p>Some side notes:</p>
<ul>
<li>
<p>After the user had once manually moved a stream, there is an entry 
for it in the restoration database, and the other routing steps are 
never executed again for this stream. Some GUI tools provide a function 
of removing routing rules from the restoration database.</p>
</li>
<li>
<p>When the user moves a stream to another device, all streams of the 
same stream group are immediately moved as well, and a routing rule for 
the whole group is stored in the restoration database.</p>
</li>
<li>
<p>When the user updates device manager routing rules, existing streams 
are immediately re-routed according to the new routing rules.</p>
</li>
<li>
<p>When the user changes the fallback source or sink, nothing happens. 
The new fallback device will be used only when a new stream is routed.</p>
</li>
</ul>
<h2 id="restoration-database">
  Restoration database
  <a class="anchor" href="#restoration-database">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-{card,device,stream}-restore</div>
  <div>module-default-device-restore</div>
</div>
<p>Three parameter categories are stored in the restoration database:</p>
<ul>
<li>active profiles of cards</li>
<li>volume/mute settings and active ports of devices</li>
<li>volume/mute settings and routing rules (stream group name plus device name)</li>
</ul>
<p>There is a separate module responsible for managing every category:</p>
<ul>
<li>module-card-restore</li>
<li>module-device-restore</li>
<li>module-stream-restore</li>
</ul>
<p>The modules implement two functions:</p>
<ul>
<li>
<p>all modules monitor server objects and read or write appropriate category of parameters to the restoration database</p>
</li>
<li>
<p>some modules also provide a protocol and API extension that enables 
client applications to read, write, and monitor the corresponding 
category of parameters in the database</p>
</li>
</ul>
<p>Each module monitors both existing and new objects:</p>
<ul>
<li>
<p>When an appropriate parameter of an existing object is changed by the
 user, the module stores the object ID, parameter ID, and parameter 
value into the database.</p>
</li>
<li>
<p>When a new object (card, device, or stream) appears, the module 
checks if there are stored parameters for this object, and restores 
them, if any.</p>
</li>
</ul>
<p>What is used as the object ID depends on the object type:</p>
<ul>
<li>for cards and devices (sources and sinks), the card or device name is used</li>
<li>for streams, the stream group identifier is used, computed from the stream properties</li>
</ul>
<p>The outcome of using stream group as the object ID is the following:</p>
<ul>
<li>
<p>All streams with the same role share the same volume and routing 
settings. For example, all music streams or all notification streams.</p>
</li>
<li>
<p>All streams without a role, but belonging to the same application, 
share the same volume and routing settings. For example, all instances 
of a music player.</p>
</li>
<li>
<p>All streams without a role and an application identifier, but with 
the same media name, share the same volume and routing settings. For 
example, all music players which are playing the same file and have 
specified media name in the same format.</p>
</li>
<li>
<p>All streams that didn’t specify any of the above properties share the same volume and routing settings.</p>
</li>
</ul>
<p>When the user changes the stream volume or moves it to another 
device, all other streams with the shared volume and routing are 
automatically updated or moved as well.</p>
<p>Besides the three modules described above, the 
module-default-device-restore saves (on a timer event) and restores (on 
start) the fallback source and sink. These two device names are stored 
in two text files instead of the restoration database.</p>
<h2 id="device-manager-database">
  Device manager database
  <a class="anchor" href="#device-manager-database">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-device-manager</div>
</div>
<p>The module-device-manager was developed for KDE, which uses it by 
default and provides a GUI tool for manipulating priority lists. The 
module performs three functions:</p>
<ul>
<li>
<p>Maintains a database with multiple priority lists of devices: one 
priority list per role, and one default priority list. The priority list
 may contain both devices that are available currently or were available
 in the past.</p>
</li>
<li>
<p>Implements routing. When a new stream can’t be routed using the 
restoration database, device manager checks if there is a non-empty 
priority list for the stream role or non-empty default priority list. If
 a non-empty priority list is found, the first currently available 
device from the priority list is used.</p>
</li>
<li>
<p>Provides a protocol and API extension that provides methods for inspecting and manipulating priority lists.</p>
</li>
</ul>
<h2 id="device-intended-roles">
  Device intended roles
  <a class="anchor" href="#device-intended-roles">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-intended-roles</div>
</div>
<p>The module-intended-roles implements a kind of automatic per-role fallback device.</p>
<p>This is how it works:</p>
<ul>
<li>
<p>When server backend creates a device, it may specify a list of roles 
intended for the device via the “device.intended_roles” property. This 
property may be also autodetected from the device form factor.</p>
</li>
<li>
<p>When a client creates a context or a stream, it may specify its role 
via the “media.role” property. This property may be also autodetected 
from the application desktop file.</p>
</li>
<li>
<p>When server routes a new stream, it searches for a device which has 
the stream role in its intended role list. The fallback device is always
 checked first, so if its intended roles match, it takes priority over 
other devices.</p>
</li>
</ul>
<p>The intended role list is set for several types of sources and sinks:</p>
<ul>
<li>
<p><strong>alsa</strong></p>
<p>ALSA backend sets intended roles of sources and sinks if the ALSA card supports UCM.</p>
<p>For every source or sink, PulseAudio computes intended roles from the
 UCM modifiers associated with the device ports of the source or sink. 
Every UCM modifier name is converted to a PulseAudio role name.</p>
</li>
<li>
<p><strong>bluetooth</strong></p>
<p>Bluetooth backend sets intended roles of sources and sinks depending on the device form factor.</p>
</li>
<li>
<p><strong>raop</strong></p>
<p>RAOP module unconditionally sets “music” role for RAOP sinks.</p>
</li>
<li>
<p><strong>aec</strong></p>
<p>Acoustic echo cancellation module unconditionally sets “phone” role for its sources and sinks.</p>
</li>
</ul>
<h2 id="priority-routing-proposal">
  Priority routing proposal
  <a class="anchor" href="#priority-routing-proposal">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p>As discussed in the <a href="https://colin.guthr.ie/2010/02/this-is-the-route-to-hell/">blog post</a>
 mentioned above, existing routing scheme is non-intuitive, and its 
steps are quite isolated and inconsistent in several aspects:</p>
<ul>
<li>
<p>If all applications will correctly provide stream roles, it will not 
be possible to move a single stream to another device without affecting 
all streams with the same role. Per-application stream moving works so 
far only because most applications don’t specify stream roles, and 
PulseAudio uses application ID instead.</p>
</li>
<li>
<p>When a stream is moved manually or device manager rules are updated, 
existing streams are re-routed. When the fallback device is updated, 
existing streams are not affected, however.</p>
</li>
<li>
<p>When configuring device manager, it’s clear that there are per-role 
device lists. When moving a stream, it’s not clear what rules will be 
overwritten (per-stream, per-role, per-application, or something else).</p>
</li>
<li>
<p>Device manager routing rules contain device lists with both available
 and unavailable devices, and the first available device is used. 
Restoration database routing rules contain only a single device, and the
 rule is used only when the device is available.</p>
</li>
<li>
<p>Restoration database rules override the device manager rules, but 
this is not obvious. The overrides may suddenly appear or disappear 
depending on whether the corresponding device is currently available and
 what meta-information is provided by an application.</p>
</li>
</ul>
<p>In result, it’s hard for the user to figure it out how and why the routing works. A non-implemented <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/RFC/PriorityRouting/">PriorityRouting</a> proposal exists, aimed to make routing more consistent and transparent:</p>
<ul>
<li>priority lists are moved to the PulseAudio core and become first-class objects</li>
<li>all existing routing steps are reworked to operate on top of these priority lists</li>
<li>modules may implement routing policies by registering or manipulating priority lists</li>
<li>the user can inspect and configure priority lists using GUI tools</li>
</ul>
<h2 id="third-party-routing-modules">
  Third-party routing modules
  <a class="anchor" href="#third-party-routing-modules">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Some projects implement their own PulseAudio modules that replace or modify default routing scheme:</p>
<ul>
<li>
<p><strong><a href="https://www.tizen.org/about/devices/vehicle-infotainment">Tizen IVI</a></strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-murphy-ivi</div>
  </div>
<p>Tizen IVI (Linux for In-Vehicle Infotainment) project uses <a href="https://01.org/murphy">Murphy</a>, a resource policy daemon that manages things like audio, video and network connections. The <a href="https://github.com/otcshare/pulseaudio-module-murphy-ivi">pulseaudio-module-murphy-ivi</a> PulseAudio module implements routing using Murphy as a backend.</p>
<p>See <a href="http://cdn.download.tizen.org/misc/media/conference2013/slides/TDC2013-Audio_Management_for_Tizen_IVI.pdf">these slides</a> for some details.</p>
</li>
<li>
<p><strong><a href="http://iot.bzh/">IoT.bzh</a></strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>agl-audio-plugin</div>
  </div>
<p>IoT.bzh (Linux for Internet Of Things) project uses <a href="https://gerrit.automotivelinux.org/gerrit/gitweb?p=staging/agl-audio-plugin.git;a=summary">agl-audio-plugin</a>
 PulseAudio module. It was forked from the Tizen IVI PulseAudio module 
and is its simplified version that doesn’t need Murphy and uses either a
 JSON configuration file or its own embedded configuration.</p>
<p>There are also <a href="http://iot.bzh/download/public/2016/audio/AGL-PulseAudio-Audio-Routing.pdf">some slides</a> with details.</p>
</li>
<li>
<p><strong><a href="https://wiki.merproject.org/wiki/Nemo">Nemo</a></strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-meego-*</div>
    <div>module-policy-enforcement</div>
    <div>module-stream-restore-nemo</div>
  </div>
<p>Nemo is a Linux distribution for mobile devices. It uses <a href="https://en.wikipedia.org/wiki/Mer_(software_distribution)">Mer</a> (a <a href="https://en.wikipedia.org/wiki/MeeGo">MeeGo</a> fork). It implements several PulseAudio modules with custom routing, volume, and permission policies.</p>
<p>See details on <a href="https://wiki.merproject.org/wiki/Nemo/Audio">their wiki</a>.</p>
</li>
<li>
<p><strong><a href="https://sailfishos.org/">Sailfish OS</a></strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-droid-*</div>
  </div>
<p>Sailfish OS a mobile OS based on <a href="https://wiki.merproject.org/wiki/Nemo">Nemo</a> and <a href="https://en.wikipedia.org/wiki/Mer_(software_distribution)">Mer</a>. It uses <a href="https://github.com/mer-hybris/pulseaudio-modules-droid">pulseaudio-modules-droid</a>, which implements several PulseAudio modules allowing PulseAudio to work on top of the Android Audio HAL.</p>
<p>See details on <a href="http://events.linuxfoundation.org/sites/events/files/slides/elc_telephony_piirainen_0.pdf">these slides</a>.</p>
</li>
</ul>
<h2 id="autodetecting-properties">
  Autodetecting properties
  <a class="anchor" href="#autodetecting-properties">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
  <div>module-augment-properties</div>
</div>
<p>When an application connects to the server, a new client object is 
created on the server. The C API automatically sets the 
“application.process.binary” property of the client, which contains the 
name of the application executable.</p>
<p>When a new client is created, PulseAudio may automatically find the <a href="https://specifications.freedesktop.org/desktop-entry-spec/desktop-entry-spec-latest.html">desktop file</a> for the application and use it to compute some client properties. The desktop file is usually searched in the <code>"/usr/share/desktop"</code> directory and should have the same name as the application executable.</p>
<p>The two desktop file entries are used:</p>
<ul>
<li>
<p><strong>X-PulseAudio-Properties</strong></p>
<p>This entry may define arbitrary properties for the client.</p>
</li>
<li>
<p><strong>Categories</strong></p>
<p>This may be used to compute the “media.role” property for the client.
 Currently, the “Game” category is mapped to the “game” role, and the 
“Telephony” category is mapped to the “phone” role.</p>
</li>
</ul>
<p>When a new stream is created, it inherits all properties from the 
client, including the ones that were detected from the desktop file. If 
the stream has its own properties, they override the client properties.</p>
<p>Sources and sinks may have “device.form_factor” property. Form factor
 is a short string describing device type, e.g. “handset”, “headset”, 
“speaker”, or “microphone”.</p>
<p>If a new device has an empty intended role list property, but 
non-empty form factor property, in some cases PulseAudio may 
automatically compute intended roles from the form factor. Currently, 
“headset”, “handset”, and “hands-free” form factors are converted to the
 “phone” role.</p>
<p>Currently, the device form factor is set in two places:</p>
<ul>
<li>
<p>Bluetooth backend computes device form factor from the device class.</p>
</li>
<li>
<p>If Udev rules can match the sound card model, the <code>SOUND_FORM_FACTOR</code> property is attached to the device. The server reads this property during source or sink initialization.</p>
</li>
</ul>
<h2 id="autoloading-filters">
  Autoloading filters
  <a class="anchor" href="#autoloading-filters">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-filter-{heuristics,apply}</div>
</div>
<p>PulseAudio may automatically load and setup filter sources and sinks 
based on the stream properties. An application specifies what sound 
processing tools it wants for a stream, and the server performs all 
necessary configuration.</p>
<p>This mechanism is based on a convention used for all filter sources and sinks:</p>
<ul>
<li>every filter is implemented in a separate module</li>
<li>the module name is the filter name</li>
<li>every module creates one source, or one sink, or one paired source and sink</li>
<li>every module accepts the “master” or the “source_master” and “sink_master” arguments</li>
</ul>
<p>Three stream properties are employed:</p>
<ul>
<li>
<p><strong>“filter.want”</strong></p>
<p>The name of the filter to load. The server is allowed to ignore the filter when it thinks it’s unreasonable.</p>
</li>
<li>
<p><strong>“filter.apply”</strong></p>
<p>The name of the filter to load. The server unconditionally loads the filter. Overrides “filter.want”.</p>
</li>
<li>
<p><strong>“filter.suppress”</strong></p>
<p>The name of the filter <em>not</em> to load. The server doesn’t load 
the filter, even if it’s specified in “filter.want” or “filter.apply”. 
Useful when “filter.want” or “filter.apply” property is set 
automatically, but application wants to disable it.</p>
</li>
</ul>
<p>The autoloading support is divided into two modules:</p>
<ul>
<li>
<p><strong>module-filter-heuristics</strong></p>
<p>Tracks when a stream is created or moved.</p>
<p>If the “filter.apply” is unset and the “filter.want” property is set 
and the specified filter should not be ignored, sets the “filter.apply” 
property to the value of the “filter.want” property.</p>
<p>Currently, the only filter that may be ignored is “echo-cancel”. It 
is ignored if the “device.intended_roles” property of the stream device 
contains the “phone” role.</p>
</li>
<li>
<p><strong>module-filter-apply</strong></p>
<p>Tracks when a stream is created, moved, or stream properties are updated.</p>
<p>If the “filter.apply” is set and not disabled by the 
“filter.suppress” property, and the filter is not loaded yet, the module
 does the following:</p>
<ul>
<li>
<p>Checks if this is a group filter, which uses a paired source and sink
 and therefore requires a paired source output and sink input. In this 
case, the filter is loaded only if the paired stream exists as well.</p>
<p>Two streams are considered paired if they have the same 
“filter.apply” property and the same stream group identifier. Currently,
 the only group filter is “echo-cancel”.</p>
</li>
<li>
<p>Checks if the user did specify additional module paraterer via the “filter.apply.&lt;filter_name&gt;.parameters” property.</p>
</li>
<li>
<p>Loads the “module-&lt;filter_name&gt;” module. The “master” or the 
“source_master” and “sink_master” arguments are set to the name of the 
source or sink to which the stream or paired streams are currently 
connected. If the user did specify additional parameters, they are also 
passed to the module.</p>
</li>
<li>
<p>Finds the newly loaded filter source or sink.</p>
</li>
<li>
<p>Moves the stream or paired streams to the filter source or sink.</p>
</li>
</ul>
</li>
</ul>
<h2 id="automatic-actions">
  Automatic actions
  <a class="anchor" href="#automatic-actions">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Several modules implement various housekeeping actions that are performed automatically.</p>
<ul>
<li>
<p><strong>switch on port available</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-switch-on-port-available</div>
  </div>
<p>When the availability of device ports changes, automatically switch the active device port and card profile.</p>
<p>The device port is switched when the currently active device port 
becomes unavailable, or a higher priority device port becomes available.
 The active card profile may be also changed if necessary. The 
priorities of ports and profiles are defined by their backend.</p>
<p>To avoid unwanted switches, the module tracks manual port and profile
 changes made by the user and uses some heuristics to determine what 
profile is preferred for every port, and what port is preferred for 
every profile.</p>
<p>In practice, this module is only relevant for ALSA cards. When the 
user plugs in something using the analog jack on a sound card, that 
typically makes some port available and may also make another port 
unavailable. Similarly, when the user unplugs something from an analog 
jack, that typically makes some port unavailable and may also make 
another port available.</p>
</li>
<li>
<p><strong>switch on connect</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-switch-on-connect</div>
  </div>
<p>When a new source or sink appears, automatically set it as the fallback device and move all active streams to it.</p>
</li>
<li>
<p><strong>rescue streams</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-rescue-streams</div>
  </div>
<p>When a source or sink disappears, automatically move streams connected to it to another working source or sink.</p>
</li>
<li>
<p><strong>always sink</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-always-sink</div>
  </div>
<p>Ensure that there is always at least one non-filter sink. When all sinks disappear, automatically load the null sink.</p>
</li>
<li>
<p><strong>role ducking and corking</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-role-{ducking,cork}</div>
  </div>
<p>When an “important” stream is started, automatically duck (lower the 
volume) or cork (mute and request a pause) active streams. When the 
important stream finishes, unduck or uncork other active streams.</p>
<p>Whether a stream is important or not is determined by its 
“media.role” property. By default, streams with the “phone” role are 
considered important.</p>
</li>
<li>
<p><strong>allow passthrough</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-allow-passthrough</div>
  </div>
<p>When a new passthrough stream is moved to an existing sink, 
automatically create a null sink and move all other running streams to 
that null sink. When the passthrough finishes, move the streams back and
 remove the null sink.</p>
<p>The reason for such behavior is that passthrough streams are 
incompatible with regular PCM streams and they can’t be connected to the
 same sink at the same time. Therefore, if the user moves a passthrough 
stream to a sink, all other streams should be temporary disconnected.</p>
</li>
<li>
<p><strong>position event sounds</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-position-event-sounds</div>
  </div>
<p>When a stream is created or its properties are changed, adjust the 
volume balance of the stream depending on its on-screen 2-D coordinated 
provided by the application.</p>
<p>First, PulseAudio checks that:</p>
<ul>
<li>
<p>the “media.role” property of the stream is set to “event”, which is usually true for GUI event sounds</p>
</li>
<li>
<p>the “event.id” property of the stream is not set to one of the known 
identifiers of test sounds used in some volume control dialogs</p>
</li>
<li>
<p>whether the “event.mouse.hpos” and “event.mouse.vpos”, or “window.hpos” and “window.vpos” properties are set for the stream</p>
</li>
</ul>
<p>If all of the above is true, PulseAudio adjusts the volume balance of the stream:</p>
<ul>
<li>the horizontal position defines the balance between the “left” and “right” channels</li>
<li>the vertical position defines the balance between the “front” and “rear” channels</li>
</ul>
<p>Thus, the GUI events on the screen are virtually mapped to a horizontal plane around the user.</p>
</li>
<li>
<p><strong>match volumes</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-match</div>
  </div>
<p>When a new stream appears, automatically update its volume based on 
its name and a preconfigured match table, by default loaded from <code>"~/.pulse/match"</code>. Each line contains a regular expression to match the stream name and the volume to set.</p>
</li>
</ul>
<h2 id="role-based-configuration-of-alsa-devices">
  Role-based configuration of ALSA devices
  <a class="anchor" href="#role-based-configuration-of-alsa-devices">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-alsa-card</div>
</div>
<p>When a stream is moved to or from an ALSA source or sink which employs UCM, the stream role may affect ALSA mixer configuration:</p>
<ul>
<li>
<p>Every ALSA device port is associated with zero or one UCM modifier.</p>
</li>
<li>
<p>Every UCM modifier is mapped to a PulseAudio role.</p>
</li>
<li>
<p>The UCM modifier of a device port is enabled when there is at least 
one source output or sink input connected to the source or sink of the 
device port, which has the “media.role” property equal to the UCM 
modifier role.</p>
</li>
</ul>
<h2 id="automated-setup-of-bluetooth-devices">
  Automated setup of Bluetooth devices
  <a class="anchor" href="#automated-setup-of-bluetooth-devices">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-bluetooth-policy</div>
</div>
<p>PulseAudio provides two features that automate the setup of Bluetooth
 devices that support A2DP (usually used for music) and HSP (usually 
used for phone calls):</p>
<ul>
<li>
<p><strong>automatic profile switch</strong></p>
<p>Automatically switch between A2DP and HSP profiles (if both are 
available for a device), depending on the roles of the currently active 
streams.</p>
<p>When a new stream is created and its “media.role” property is set to 
“phone” or unset (depending on the “auto_switch” module parameter), 
switch the card profile to HSP, to make the bluetooth source available 
when the regular stream routing logic is executed. When all such source 
outputs disappear, switch the card profile to A2DP.</p>
</li>
<li>
<p><strong>automatic playback</strong></p>
<p>Automatically route and play audio from A2DP and HSP/HFP AG sources.</p>
<p>When a new A2DP or HSP/HFP AG source is created for a BLuetooth card, do the following:</p>
<ul>
<li>create a loopback, i.e. a pair of a source output and sink input connected with a queue</li>
<li>connect the loopback source output to the source</li>
<li>set the “media.role” property of the loopback sink input to “music” 
(for A2DP) or “phone” (for HSP/HFP), which may be used for routing</li>
<li>let PulseAudio route the loopback sink input to some sink</li>
</ul>
</li>
<li>
<p><strong>automatic capture</strong></p>
<p>Automatically route and record audio from HSP/HFP AG sinks.</p>
<p>When a new HSP/HFP AG sink is created for a Bluetooth card, do the following:</p>
<ul>
<li>create a loopback, i.e. a pair of a source output and sink input connected with a queue</li>
<li>connect the loopback sink input to the sink</li>
<li>set the “media.role” property of the loopback source output to “phone”, which may be used for routing</li>
<li>let PulseAudio route the loopback source output to some source</li>
</ul>
</li>
</ul>
<hr>
<h1 id="desktop-integrations">
  Desktop integrations
  <a class="anchor" href="#desktop-integrations">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>This section describes features that integrate PulseAudio into the desktop environment. Some details are also available on the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Desktops/">Desktops</a> page on wiki.</p>
<h2 id="autospawn-and-autoexit">
  Autospawn and autoexit
  <a class="anchor" href="#autospawn-and-autoexit">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio server usually starts and exits automatically:</p>
<ul>
<li>
<p><strong>autospawn</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>libpulse</div>
  </div>
<p>When a client tries to communicate to the server via libpulse, the 
server is started automatically if it’s not started yet. Due to this 
feature, when the user kills the server but an active client exists, the
 server will be automatically started again. This feature can be 
disabled in the client configuration file.</p>
</li>
<li>
<p><strong>autoexit</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>libpulsecore</div>
  </div>
<p>When there are no connected clients during some period of time, the 
server automatically exits. However, the automatic exit may be prevented
 by one of the session management modules.</p>
</li>
</ul>
<h2 id="session-management">
  Session management
  <a class="anchor" href="#session-management">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are several session management modules that prevent the server from exiting during the lifetime of a desktop session:</p>
<ul>
<li>
<p><strong>systemd-logind</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-systemd-login</div>
  </div>
<p>This module monitors <a href="https://freedesktop.org/wiki/Software/systemd/logind/">logind</a>
 events via D-Bus API. It creates a fake PulseAudio client for every new
 login session of the current user (determined by UID), and removes it 
when the session ends. These fake clients keep the server opened until 
the last user session ends.</p>
</li>
<li>
<p><strong>ConsoleKit</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-console-kit</div>
  </div>
<p>This module is the same but monitors <a href="https://www.freedesktop.org/wiki/Software/ConsoleKit/">ConsoleKit</a> events.</p>
</li>
<li>
<p><strong>XSMP</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-x11-xsmp</div>
  </div>
<p>This module connects to the <a href="https://en.wikipedia.org/wiki/X_session_manager">X session manager</a> via <a href="https://www.x.org/releases/X11R7.7/doc/libSM/xsmp.html">XSMP</a> protocol. It creates a fake client and removes it when the current X11 session ends.</p>
</li>
</ul>
<h2 id="x11-publishing">
  X11 publishing
  <a class="anchor" href="#x11-publishing">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-x11-publish</div>
</div>
<p>PulseAudio server may publish its address and credentials via the X11
 root window properties. These properties may be then read by clients 
running on the same X display, including remote clients that use SSH X 
forwarding.</p>
<p>The properties can be read using this command:</p>
<pre><code>$ xprop -root | grep PULSE
</code></pre>
<p>Here is the list:</p>
<ul>
<li>
<p><strong><code>PULSE_COOKIE</code></strong></p>
<p>An authentication cookie that may be used by clients to connect to 
the server. All clients that have access to the current X11 display will
 be able to connect to PulseAudio server too.</p>
</li>
<li>
<p><strong><code>PULSE_ID</code></strong></p>
<p>Server ID in form of <code>"server_uid@machine_id/server_pid"</code>.</p>
</li>
<li>
<p><strong><code>PULSE_SERVER</code></strong></p>
<p>Space-separated list of server sockets, e.g. <code>"{machine_id}unix:/socket/path tcp:hostname:port"</code>. The server automatically updates this property when sockets are opened or closed.</p>
</li>
<li>
<p><strong><code>PULSE_SESSION_ID</code></strong></p>
<p>The value of <code>$XDG_SESSION_ID</code> environment variable when the server was started. Omitted if the variable wasn’t set.</p>
</li>
<li>
<p><strong><code>PULSE_SOURCE</code>, <code>PULSE_SINK</code></strong></p>
<p>These properties are optional. They may be set to a source or sink 
name provided by the user via the module arguments when starting the 
server. Clients will use these source and sink as defaults.</p>
</li>
</ul>
<h2 id="x11-events">
  X11 events
  <a class="anchor" href="#x11-events">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio can interact with X11 events in two ways:</p>
<ul>
<li>
<p><strong>Bell</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-x11-bell</div>
  </div>
<p>The core X11 protocol allows clients to ring a bell, imitating <a href="https://en.wikipedia.org/wiki/Bell_character">analogous feature</a> of the TTY. <a href="https://en.wikipedia.org/wiki/X_keyboard_extension">XKB</a> extends this by supporting multiple named bells and providing an API for controlling bells and handling bell events.</p>
<p>PulseAudio is able to intercept XKB bell event and play a preconfigured sample from the sample cache.</p>
</li>
<li>
<p><strong>Cork</strong></p>
  <div class="flex_table">
    <div class="flex_th">component</div>
    <div>module-x11-cork-request</div>
  </div>
<p>PulseAudio can automatically cork (mute and request a pause) all 
active streams when a more important stream appears, and uncork them 
when it disappears.</p>
<p>This is implemented as special cork-request and uncork-request events
 sent from the server to clients. However, many PulseAudio clients don’t
 subscribe and handle server events, and their streams become muted but 
not paused.</p>
<p>As a workaround, PulseAudio can artificially synthesize X11 media key
 events along with the cork or uncork request, as if the pause or play 
multimedia keyboard button was pressed. Some applications will handle 
these events and pause/resume playback. This scheme is known to be 
buggy, however.</p>
</li>
</ul>
<h2 id="realtimekit-rtkit">
  RealtimeKit (rtkit)
  <a class="anchor" href="#realtimekit-rtkit">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulsecore</div>
</div>
<p><a href="http://git.0pointer.net/rtkit.git/">RealtimeKit</a> provides
 a D-Bus API that allows user processes to enable the realtime 
scheduling policy without the root privileges. Basically, it’s a D-Bus, 
rootless, policy-based replacement for the <a href="http://man7.org/linux/man-pages/man2/sched_setscheduler.2.html"><code>sched_setscheduler</code></a> POSIX call, plus a watchdog. Some details are available in <a href="http://0pointer.de/blog/projects/rtkit.html">this post</a>.</p>
<p>PulseAudio may use it to enable <a href="http://man7.org/linux/man-pages/man7/sched.7.html"><code>SCHED_RR</code></a>
 policy for some of its threads. Threads under this policy preempt any 
other threads on the system except the other realtime threads.</p>
<p>The realtime policy is enabled for the sink and source threads, 
including the ALSA sink thread that runs the timer-based scheduler. This
 helps to handle low latency values because when it’s time to provide 
samples for the ALSA driver, PulseAudio will not be delayed even if 
there are other starved processes.</p>
<h2 id="gnome-registry-gconf">
  GNOME registry (GConf)
  <a class="anchor" href="#gnome-registry-gconf">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-gconf</div>
  <div>gconf-helper</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/GConf">GConf</a> is a system 
used by the GNOME desktop environment for storing configuration settings
 for the desktop and applications. GConf is currently deprecated and is 
replaced with GSettings and dconf, but is still available in distros.</p>
<p>PulseAudio monitors the <code>"/system/pulseaudio/modules"</code> GConf directory that should have the following layout:</p>
<pre tabindex="0"><code>system
└── pulseaudio
    └── modules             # root directory monitored by PulseAudio
        └── &lt;foobar&gt;        # subdirectory with an arbitrary name
            ├── enabled     # contains a boolean, true if this subdirectory is enabled
            ├── name0       # contains a string with module name
            ├── args0       # contains a string with module arguments
            ├── name1       # contains a string with module name
            ├── args1       # contains a string with module arguments
            └── ...
</code></pre><p>For every subdirectory, PulseAudio automatically loads 
modules when new entries appear (up to ten entries currently) and 
unloads them when they disappear or “enabled” entry is set to false. The
 monitoring is implemented in <code>gconf-helper</code> tool which is the part of the PulseAudio package.</p>
<p>This feature is used in the <code>paprefs</code> GUI which may be 
used to configure and enable some non-default modules like RTP sender 
and receiver. The advantage of this approach is that these settings are 
stored persistently in the GConf database.</p>
<hr>
<h1 id="compatibility-layers">
  Compatibility layers
  <a class="anchor" href="#compatibility-layers">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>There are several compatibility layers with other sound systems, so 
that existing applications may automatically run on PulseAudio without 
modification.</p>
<h2 id="emulate-alsa">
  Emulate ALSA
  <a class="anchor" href="#emulate-alsa">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>alsa-plugins</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Advanced_Linux_Sound_Architecture">ALSA</a>
 implements both kernel drivers and a user space library (libasound) 
with a high-level API for applications. This library supports <a href="http://alsa.opensrc.org/ALSA_plugins">plugins</a>, which implement virtual ALSA devices.</p>
<p>Plugins are usually used for two things:</p>
<ul>
<li>to add software sound processing on top of a real device, when it’s missed in hardware (e.g. resampling or mixing)</li>
<li>to redirect sound to another sound systems (e.g. PulseAudio and JACK)</li>
</ul>
<p>The user manually enables plugins and sets the default device in ALSA <a href="https://www.alsa-project.org/main/index.php/Asoundrc">configuration files</a>, regularly <code>"~/.asoundrc"</code> or <code>"/etc/asound.conf"</code>.</p>
<p>Note that plugins are the feature of libasound. They work entirely in
 user space inside a process that uses the library and has opened a 
virtual device. ALSA is not a sound server and doesn’t run a daemon 
process, unlike PulseAudio and JACK.</p>
<p>PulseAudio ships with an ALSA plugin that implements the “pulse” 
virtual device. In Linux distros that use PulseAudio, this device is 
usually configured as the default device for ALSA applications.</p>
<img src="PulseAudio%20under%20the%20hood_files/alsa_compat.png" width="555px">
<p>This is how it works:</p>
<ul>
<li>
<p>When an ALSA application opens the “pulse” device, a new PulseAudio 
stream is created, and playback or recording is redirected to PulseAudio
 server.</p>
</li>
<li>
<p>PulseAudio server does its usual business: routing, mixing, adjusting volume, etc.</p>
</li>
<li>
<p>If the stream was routed to a local sound card, and PulseAudio uses 
ALSA backend, the stream goes to libasound again. This time, however, a 
hardware device for the appropriate sound card is used instead of the 
virtual “pulse” device.</p>
</li>
<li>
<p>Finally, libasound asks the kernel space ALSA driver to write or read samples from the sound card ring buffer.</p>
</li>
</ul>
<h2 id="emulate-oss">
  Emulate OSS
  <a class="anchor" href="#emulate-oss">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>ossp</div>
  <div>padsp</div>
</div>
<p><a href="https://en.wikipedia.org/wiki/Open_Sound_System">OSS</a> is 
an aged interface for making and capturing sound in Unix. OSS creates a 
character device for every sound card, and applications may open device,
 write or read samples, and perform control commands with ioctl.</p>
<p><a href="https://github.com/libfuse/osspd">ossp</a> (OSS Proxy) 
provides a full-featured OSS emulation on top of ALSA or PulseAudio. It 
has a modular architecture consisting of the main daemon and backends:</p>
<ul>
<li>
<p><strong>osspd</strong></p>
<p>osspd daemon creates and serves OSS devices using <a href="https://lwn.net/Articles/308445/">CUSE</a> (Character Device in User Space). It forwards sound to a backend.</p>
</li>
<li>
<p><strong>ossp-aslap</strong></p>
<p>ossp-aslap backend forwards sound to ALSA using libasound.</p>
</li>
<li>
<p><strong>ossp-padsp</strong></p>
<p>ossp-padsp backend forwards sound to PulseAudio using libpulse.</p>
</li>
</ul>
<p>Well, quite enough forwarding:</p>
<img src="PulseAudio%20under%20the%20hood_files/ossp.png" width="241px">
<p>In addition, PulseAudio provides the <a href="http://manpages.ubuntu.com/manpages/en/man1/padsp.1.html">padsp</a> wrapper. It intercepts standard library functions using the <code>LD_PRELOAD</code> trick:</p>
<ul>
<li>
<p>when the wrapped application tries to open an OSS device, it gets a fake file descriptor</p>
</li>
<li>
<p>when the application issues an operation on that fake descriptor 
(e.g. write, stat, or ioctl), the wrapper handles it and forwards sound 
or control commands to PulseAudio</p>
</li>
</ul>
<p>The same approach is used in aoss (OSS to ALSA), esddsp (OSS to 
ESound), and artsdsp (OSS to arts sound server). It works for some 
applications but is known to be incomplete.</p>
<p>Note that ALSA driver also provides in-kernel OSS emulation. However,
 it’s not aware of the user space stuff, including libasound virtual 
devices, and therefore can’t be used to forward sound to PulseAudio. See
 <a href="http://alsa.opensrc.org/OSS_emulation">OSS emulation</a> on ALSA wiki.</p>
<h2 id="emulate-esound">
  Emulate ESound
  <a class="anchor" href="#emulate-esound">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>module-esound-protocol-{unix,tcp}</div>
  <div>module-esound-compat-{spawnpid,spawnfd}</div>
</div>
<p>Like PulseAudio, <a href="https://en.wikipedia.org/wiki/ESound">Enlightened Sound Daemon</a> is a sound server accessed by applications via a socket. This daemon was being used in GNOME before it switched to PulseAudio.</p>
<p>For seamless migration, PulseAudio server provides two features:</p>
<ul>
<li>
<p>A module that implements the ESound protocol and emulates ESD server.
 Existing applications may communicate with PulseAudio server as if it 
were an ESound server.</p>
</li>
<li>
<p>Two modules that implement ESound autospawn conventions. An 
application may start PulseAudio server as if it were an ESound server, 
and PulseAudio will notify the application that it was successfully 
started with a signal of via a file descriptor.</p>
</li>
</ul>
<h2 id="partially-or-completely-disable-pulseaudio">
  Partially or completely disable PulseAudio
  <a class="anchor" href="#partially-or-completely-disable-pulseaudio">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are several methods of running applications that need 
PulseAudio on systems where PulseAudio is not the primary sound system 
or even is not installed.</p>
<ul>
<li>
<p><strong>Configuring PulseAudio to use JACK backend</strong></p>
<p>It’s possible to configure PulseAudio to use JACK backend (that has 
limited functionality and larger latency) instead of ALSA backend.</p>
<p>The typical use case for this method is to run PulseAudio 
applications that don’t support JACK (like Skype) on a system that uses 
JACK, without switching the entire system to PulseAudio.</p>
</li>
<li>
<p><strong>Configuring PulseAudio as a “dumb pipe” for ALSA</strong></p>
<p><a href="http://alsa.opensrc.org/Dmix">dmix</a> and <a href="http://alsa.opensrc.org/Dsnoop">dsnoop</a> are ALSA virtual devices that implement software mixing and support sharing the same device between multiple applications.</p>
<p>It’s possible to configure PulseAudio to provide single sink and 
source attached to dmix and dsnoop devices without creating sinks and 
sources for hardware ALSA devices. See instructions <a href="https://wiki.archlinux.org/index.php/PulseAudio/Examples#PulseAudio_as_a_minimal_unintrusive_dumb_pipe_to_ALSA">here</a> and <a href="https://wiki.archlinux.org/index.php/PulseAudio#ALSA.2Fdmix_without_grabbing_hardware_device">here</a>.</p>
<p>The typical use case for this method is to run PulseAudio 
applications that don’t support ALSA (like Skype) on a system that uses 
ALSA, without switching the entire system to PulseAudio.</p>
</li>
<li>
<p><strong>Emulating PulseAudio on top of ALSA</strong></p>
<p>The <a href="https://github.com/i-rinat/apulse">apulse</a> wrapper tool uses the <code>LD_PRELOAD</code> trick to implement libpulse and libpulse-simple API directly on top of ALSA (libasound).</p>
<p>The typical use case for this method is to run PulseAudio 
applications that don’t support ALSA on a system that uses ALSA, without
 even installing PulseAudio.</p>
</li>
<li>
<p><strong>Bluetooth without PulseAudio</strong></p>
<p>The <a href="https://github.com/Arkq/bluez-alsa">BlueALSA</a> 
(bluez-alsa) project implements virtual ALSA device that uses Bluez5 as a
 backend. This allows to play and record audio from Bluetooth devices 
with any software that supports ALSA.</p>
</li>
</ul>
<h2 id="temporary-suspend-pulseaudio">
  Temporary suspend PulseAudio
  <a class="anchor" href="#temporary-suspend-pulseaudio">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p><a href="http://manpages.ubuntu.com/manpages/en/man1/pasuspender.1.html">pasuspender</a> is a wrapper tool for applications that need exclusive access to ALSA devices.</p>
<p>It uses the device reservation API to ask PulseAudio server to 
release ownership of devices, runs the wrapped application, and returns 
the ownership when the application exits.</p>
<p>The typical use case for this method is to run JACK applications on a system that uses PulseAudio.</p>
<hr>
<h1 id="server-internals">
  Server internals
  <a class="anchor" href="#server-internals">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>This section provides a brief overview of PulseAudio server internals.</p>
<h2 id="components">
  Components
  <a class="anchor" href="#components">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio server consists of several logical components:</p>
<ul>
<li>
<p><strong>daemon</strong></p>
<p>Daemon is a top-level component that configures core and modules and starts the core main loop.</p>
</li>
<li>
<p><strong>core</strong></p>
<p>Core provides building blocks and shared environment for modules. It 
is implemented in the libpulsecore library, which also uses libpulse and
 libpulsecommon.</p>
</li>
<li>
<p><strong>modules</strong></p>
<p>Modules are dynamically loaded libraries that extend server and 
implement many actual features, including network protocols, device 
drivers, sound processing tools, audio routing, etc. Modules use the 
libpulsecore library.</p>
</li>
</ul>
<h2 id="core">
  Core
  <a class="anchor" href="#core">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The core provides building blocks and shared environment for modules:</p>
<ul>
<li>
<p><strong>environment for modules</strong></p>
<p>Shared environment for modules:</p>
<ul>
<li>module management</li>
<li>name registry</li>
<li>main loop</li>
<li>hooks</li>
</ul>
</li>
<li>
<p><strong>fundamental objects</strong></p>
<p>Skeleton implementation of the fundamental objects:</p>
<ul>
<li>module</li>
<li>client</li>
<li>card</li>
<li>device port</li>
<li>source</li>
<li>source output</li>
<li>sink</li>
<li>sink input</li>
</ul>
</li>
<li>
<p><strong>common functionality</strong></p>
<p>Reusable parts common for multiple modules:</p>
<ul>
<li>audio processing</li>
<li>stream management</li>
<li>parsers and formatters</li>
<li>codecs</li>
<li>protocols</li>
</ul>
</li>
<li>
<p><strong>utility functions</strong></p>
<p>Numerous general-purpose utility functions:</p>
<ul>
<li>memory management</li>
<li>collections</li>
<li>message queues</li>
<li>event loops</li>
<li>threading</li>
<li>I/O</li>
<li>platform wrappers</li>
<li>OOP helpers</li>
</ul>
</li>
</ul>
<h2 id="modules">
  Modules
  <a class="anchor" href="#modules">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>A module is a dynamically loadable server extension. Usually, it is a
 shared library loaded at run-time, but it’s also possible to build it 
as a static library and link into the server at compile time.</p>
<p>There are several ways how a module may extend the server:</p>
<ul>
<li>
<p><strong>register new objects</strong></p>
<p>A module can get a skeleton implementation from the core (e.g. source
 or sink), extend it, create an instance, and register the instance in 
the name registry. The core and other modules may then use registered 
objects.</p>
<p>Such a module is usually loaded multiple times. The module arguments 
define the name and parameters of the object to create. For every new 
object, a new instance of the module is loaded.</p>
<p>This approach is used to implement hardware and network devices and sound processing tools.</p>
</li>
<li>
<p><strong>register event loop handlers</strong></p>
<p>A module can monitor various external events, like a Udev event, a 
D-Bus signal, or socket I/O. Module registers I/O or timer event handler
 in the core event loop, and core invokes handlers when the event is 
fired.</p>
<p>Such a module may be loaded either once or multiple times depending 
on the implementation, e.g. one module instance for every socket address
 to be monitored.</p>
<p>This approach is used to implement device hotplug, network publishing
 and discovery, and servers for various protocols, including the D-Bus 
API, the “native” protocol used in the C API, and the CLI protocol used 
in command line tools.</p>
</li>
<li>
<p><strong>register hooks and subscriptions</strong></p>
<p>A module can register hooks or subscribe events. The module monitors 
created, removed, or modified objects or other events, and implements 
some behavior.</p>
<p>Such a module is usually loaded only once.</p>
<p>This approach is used to implement various automatic actions like 
routing new streams, automatically saving and restoring object 
parameters to the database, autoloading filters, etc.</p>
</li>
<li>
<p><strong>register protocol and API extensions</strong></p>
<p>A module can register extensions for the “native” protocol and the C 
API, as well as for the D-Bus API. Clients may then use these extensions
 to communicate with the module.</p>
<p>This approach is used to provide an API to manage the restoration 
database and setup custom parameters for some sound processing tools.</p>
</li>
</ul>
<h2 id="objects">
  Objects
  <a class="anchor" href="#objects">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are two base types that are used to implement objects:</p>
<ul>
<li>
<p><strong>object</strong></p>
<p>Object (<code>pa_object</code>) is the base type for 
reference-countable objects. It implements reference counting, virtual 
destructor, and dynamic type checks and casts.</p>
<p>Usage:</p>
<ul>
<li>message object</li>
<li>device port</li>
</ul>
</li>
<li>
<p><strong>message object</strong></p>
<p>Message object (<code>pa_msgobject</code>) is the base type for objects that can receive messages. It extends <code>pa_object</code>, so provides all its features, and adds message handling.</p>
<p>Usage:</p>
<ul>
<li>core</li>
<li>source</li>
<li>source output</li>
<li>sink</li>
<li>sink input</li>
<li>network connections</li>
<li>streams</li>
</ul>
</li>
</ul>
<h2 id="registries">
  Registries
  <a class="anchor" href="#registries">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The core provides two global registries accessible in modules:</p>
<ul>
<li>
<p><strong>name registry</strong></p>
<p>The name registry (<code>pa_namereg</code>) is a global hashmap that contains objects of the following types:</p>
<ul>
<li>card</li>
<li>source</li>
<li>sink</li>
<li>sample cache entry</li>
</ul>
<p>These objects are always added to the name registry when they’re created, and modules can access them by name.</p>
</li>
<li>
<p><strong>shared properties</strong></p>
<p>The shared property subsystem (<code>pa_shared</code>) is a global 
hashmap that contains arbitrary data. Modules use it to register objects
 that may be accessed by name in related modules or other instances of 
the same module.</p>
</li>
</ul>
<h2 id="hooks-and-subscriptions">
  Hooks and subscriptions
  <a class="anchor" href="#hooks-and-subscriptions">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Hooks are an internal notification mechanism. Hooks may be provided both by core and modules:</p>
<ul>
<li>the core provides hooks for registered objects (e.g. sources and sinks)</li>
<li>the core provides hooks for the “native” and D-Bus protocol events</li>
<li>Bluetooth backend provides hooks for driver events</li>
</ul>
<p>Object hooks can be roughly divided into the four categories:</p>
<ul>
<li>object is created or removed</li>
<li>object is fully initialized</li>
<li>object state or properties are changed, e.g. the property list or volume is updated</li>
<li>object connections are changed, e.g. a stream is moved to another device</li>
</ul>
<p>Subscription events are an alternative notification mechanism for 
registered objects (e.g. sources and sinks). They may be used both 
internally and externally via the C API:</p>
<ul>
<li>module or client subscribes events by a mask</li>
<li>core triggers event when an object is created, removed, or modified</li>
</ul>
<h2 id="properties">
  Properties
  <a class="anchor" href="#properties">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio defines numerous properties that may be set for objects registered in the core. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/ApplicationProperties/">ApplicationProperties</a> wiki page and the <a href="https://freedesktop.org/software/pulseaudio/doxygen/proplist_8h.html">Doxygen documentation</a> with the full list of properties.</p>
<p>A property has a textual name and textual or binary value. The value 
format depends on the property name. Properties are organized into 
property lists. The following objects have a property list:</p>
<ul>
<li>module</li>
<li>client</li>
<li>card</li>
<li>device port</li>
<li>source</li>
<li>source output</li>
<li>sink</li>
<li>sink input</li>
<li>sample cache entry</li>
<li>format info</li>
</ul>
<p>The properties are accessible in the core, in modules, and in clients
 through the C API and D-Bus API. They are set both by the server and 
applications. When an application that uses libpulse connects to the 
server or creates a stream, libpulse automatically sets some client and 
stream properties from environment variables and process attributes.</p>
<p>Applications typically specify properties when creating a context or 
stream object. Context properties are used for the corresponding 
server-side client object, and stream properties are used for the 
corresponding server-side source output or sink input object.</p>
<p>There are also two cases when properties are inherited:</p>
<ul>
<li>
<p>when a sink input or source output is created for a “native” protocol
 stream, it inherits properties of the client that owns the stream</p>
</li>
<li>
<p>when a sink input is created for to play a sample cache entry, it inherits properties of the entry</p>
</li>
</ul>
<p>Most properties are used to provide various auxiliary 
meta-information to applications, like description and icons that may be
 displayed in GUI. Some properties are used for automatic actions like 
filter autoloading and routing.</p>
<p>Properties are grouped into property classes. The table below summarizes them.</p>
<table class="hdr_table">
  <tbody><tr>
    <th>class</th>
    <th>description</th>
    <th>set for</th>
    <th>set by</th>
    <th>used by</th>
  </tr>
  <tr>
    <td><strong>"module.*"</strong></td>
    <td>module meta-information</td>
    <td>module</td>
    <td>module loader</td>
    <td>GUIs</td>
  </tr>
  <tr>
    <td><strong>"application.*"</strong></td>
    <td>application and process attributes</td>
    <td>client</td>
    <td>applications, libpulse</td>
    <td>GUIs, autodetecting properties, automatic actions, routing</td>
  </tr>
  <tr>
    <td><strong>"window.*"</strong></td>
    <td>desktop window attributes</td>
    <td>client</td>
    <td>applications, libpulse</td>
    <td>position event sounds module</td>
  </tr>
  <tr>
    <td><strong>"device.*"</strong></td>
    <td>card, device, and device port attributes</td>
    <td>card, device port, source, sink</td>
    <td>device backends, sound processing modules</td>
    <td>GUIs, automatic actions, routing</td>
  </tr>
  <tr>
    <td><strong>"media.*"</strong></td>
    <td>multimedia attributes of a stream</td>
    <td>source output, sink input</td>
    <td>applications, sound processing modules</td>
    <td>GUIs, automatic actions, routing, device backends</td>
  </tr>
  <tr>
    <td><strong>"event.*"</strong></td>
    <td>event sound stream attributes</td>
    <td>source output, sink input</td>
    <td>applications, sample cache</td>
    <td>sample cache, position event sounds module</td>
  </tr>
  <tr>
    <td><strong>"filter.*"</strong></td>
    <td>sound processing filters for a stream</td>
    <td>source output, sink input</td>
    <td>applications</td>
    <td>filter autoloading</td>
  </tr>
  <tr>
    <td><strong>"format.*"</strong></td>
    <td>sample format attributes</td>
    <td>format info of source, sink, source output, or sink input</td>
    <td>applications, device backends</td>
    <td>applications, device backends</td>
  </tr>
</tbody></table>
<h2 id="threads">
  Threads
  <a class="anchor" href="#threads">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are two types of threads used in PulseAudio server:</p>
<ul>
<li>
<p><strong>event loop threads</strong></p>
<p>These threads run an event loop (<code>pa_mainloop</code>) which handles I/O and timer events, including asynchronous messages from other threads.</p>
<p>The mainloop API hides the execution flow control from the thread 
implementer, which just registers event handlers and starts the loop.</p>
</li>
<li>
<p><strong>IO threads</strong></p>
<p>These threads run a select/poll-like loop (<code>pa_rtpoll</code>) which handles I/O and timer events, including asynchronous messages from other threads.</p>
<p>The rtpoll API leaves the execution flow control to the thread 
implementer, who may decide how much and when to sleep, what file 
descriptors to handle and when, when to handle pending messages, etc.</p>
</li>
</ul>
<p>PulseAudio server creates the following threads:</p>
<ul>
<li>
<p><strong>core event loop</strong></p>
<p>The main thread runs the core main loop. Most modules register 
handlers in the core main loop. For example, the core main loop is used 
in hotplug and network modules to listen to Udev events (to detect ALSA 
cards), to listen to D-Bus events (to detect Bluetooth devices or 
receive events from JACK), to listen to broadcast announcements (to 
detect RTP receivers), and to handle client connections.</p>
</li>
<li>
<p><strong>avahi event loop</strong></p>
<p>PulseAudio creates a separate threaded event loop which runs an Avahi
 client and handles asynchronous messages from other threads.</p>
</li>
<li>
<p><strong>client event loops</strong></p>
<p>When PulseAudio server acts as a client to another remote PulseAudio 
server, it runs a separate thread with a client main loop. This is used 
in the tunnel source and sink implementation.</p>
</li>
<li>
<p><strong>device IO threads</strong></p>
<p>Every source and sink, except filter sources and sinks, have its own 
IO thread which reads or writes samples, implements clocking and 
maintains latency.</p>
</li>
</ul>
<p>The communication between threads is done via messages. Each thread installs a thread message queue (<code>pa_thead_mq</code> which uses <code>pa_asyncmsgq</code>) to handle asynchronous messages for message objects (<code>pa_msgobject</code>).</p>
<p>There are three types of message objects:</p>
<ul>
<li>
<p>Sources and sinks, except filter sources and sinks, are message 
objects with a dedicated IO thread, which handles both IO and 
asynchronous messages.</p>
</li>
<li>
<p>Filter sources and sinks don’t have a dedicated thread. They are running inside the IO thread of the master source or sink.</p>
</li>
<li>
<p>Other message objects, including source outputs and sink inputs, 
don’t have a dedicated thread as well. They all share the core event 
loop thread which handles asynchronous messages sent to them.</p>
</li>
</ul>
<h2 id="memory">
  Memory
  <a class="anchor" href="#memory">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The memory management in PulseAudio is based on the following concepts:</p>
<ul>
<li>
<p><strong>reference counting</strong></p>
<p>PulseAudio extensively uses reference counting. In particular, all objects (<code>pa_object</code>), message objects (<code>pa_msgobject</code>), memory blocks (<code>pa_memblock</code>), and many other types are reference-counted.</p>
</li>
<li>
<p><strong>blocks</strong></p>
<p>A block (<code>pa_memblock</code>) is a fixed-size reference-counted 
array in memory, usually, but not necessary, allocated from a memory 
pool. Usually, blocks are used indirectly via chunks.</p>
</li>
<li>
<p><strong>chunks</strong></p>
<p>A chunk (<code>pa_memchunk</code>) is a variable-size slice of a 
block. It consists of a pointer to a block, a starting offset in the 
block, and a length. Chunks are not reference-counted. They are usually 
allocated on stack or inside other objects. Most of the sample 
processing and exchange in done using chunks.</p>
</li>
<li>
<p><strong>block queues</strong></p>
<p>A block queue (<code>ps_memblockq</code>) is a FIFO of chunks. It 
allows to push and pop chunks and implements various flow control 
operations. Sample streams are based on the block queues.</p>
</li>
<li>
<p><strong>pools</strong></p>
<p>A pool (<code>pa_mempool</code>) is the most common way to allocate blocks.</p>
<p>Pools may use either private memory, <a href="http://man7.org/linux/man-pages/man3/shm_open.3.html">POSIX shared memory</a>, or <a href="http://man7.org/linux/man-pages/man2/memfd_create.2.html">memfd shared memory</a>. The shared memory pools combined with the memory exports and imports are used to implement the zero-copy mode.</p>
<p>Pools may be either global or per-client. The per-client pools 
guarantee that only the owner client is able to access its memory.</p>
</li>
<li>
<p><strong>exports and imports</strong></p>
<p>Memory exports (<code>pa_memexport</code>) allow to make blocks accessible from other processes. Memory imports (<code>pa_memimport</code>) allow to access the exported blocks.</p>
<p>The exporting process allocates a block in a shared memory pool, 
fills it, and communicates the shared memory id and the block id to the 
importing process. The importing process opens the shared memory, finds 
the block, and uses it.</p>
<p>When either the exporting process <em>revokes</em> the block or the importing process <em>releases</em> the block, it is returned to the memory pool.</p>
</li>
</ul>
<h2 id="io">
  I/O
  <a class="anchor" href="#io">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio server uses the following I/O APIs:</p>
<ul>
<li>
<p><strong>mainloop</strong></p>
<p>The event loop API (<code>pa_mainloop_api</code>) is used to handle I/O and timer events using callbacks. The user registers callbacks and runs the loop.</p>
<p>There are three implementations of the mainloop API: a regular event loop (<code>pa_mainloop</code>), a threaded event loop (<code>pa_threaded_mainloop</code>), which runs an event loop in a separate thread, and a Glib event loop (<code>pa_glib_mainloop</code>), which runs an event loop on top of the Glib event loop.</p>
<p>Most of the I/O, with a notable exception of the device IO threads, is based on the mainloop API.</p>
</li>
<li>
<p><strong>rtpoll</strong></p>
<p>The rtpoll API (<code>pa_rtpoll</code>) is used to handle I/O and timer events using a select/poll-like loop.</p>
<p>The user registers descriptors and manually runs the loop iterations.
 The user fully controls the flow and decides when to perform I/O and 
when and how much to sleep.</p>
<p>The device threads are based on the rtpoll API.</p>
</li>
<li>
<p><strong>srbchannel</strong></p>
<p>A shared ring buffer channel (<code>pa_srbchannel</code>) is a 
bidirectional byte stream on top of two ring buffers in the shared 
memory, two file descriptor-based semaphores (using POSIX <a href="http://man7.org/linux/man-pages/man2/pipe.2.html">pipe</a> or Linux-specific <a href="http://man7.org/linux/man-pages/man2/eventfd.2.html">eventfd</a>), and an event loop.</p>
<p>The user registers a callback that is called when the descriptor is ready, and performs non-blocking read or write operations.</p>
<p>The “native” protocol streams employ this channel to exchange control
 commands and chunk identifiers in the zero-copy mode, when the client 
and server are running on the same host and use a shared memory pool.</p>
</li>
<li>
<p><strong>iochannel</strong></p>
<p>An I/O channel (<code>pa_iochannel</code>) is a bidirectional byte stream on top of a socket file descriptor and an event loop.</p>
<p>The user registers a callback that is called when the descriptor is 
ready, and performs non-blocking read or write operations. In addition 
to the regular data, the user may also send and receive file descriptors
 and user credentials, if a Unix domain socket is used.</p>
<p>Binary client streams, like the “native”, “simple”, and ESound 
streams, are based on this channel, except when using the zero-copy mode
 of the “native” protocol.</p>
</li>
<li>
<p><strong>ioline</strong></p>
<p>An I/O line (<code>pa_ioline</code>) is a bidirectional line-oriented text stream on top of the I/O channel (<code>pa_iochannel</code>). The user registers callback that is called for every received line and sends and receives data line-by-line.</p>
<p>Text client streams, like the CLI and HTTP streams, are based on this channel.</p>
</li>
</ul>
<h2 id="packets">
  Packets
  <a class="anchor" href="#packets">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The “native” protocol is implemented on top of packets, packet stream, and packet dispatcher:</p>
<ul>
<li>
<p><strong>packet</strong></p>
<p>Packet (<code>pa_packet</code>) is a fixed-size reference-countable blob with data. Packets are usually allocated from a global per-process pool.</p>
</li>
<li>
<p><strong>packet stream</strong></p>
<p>Packet stream (<code>pa_pstream</code>) is a bidirectional message stream on top of an event loop (<code>pa_mainloop</code>), an I/O channel (<code>pa_iochannel</code>), and a memory pool (<code>pa_mempool</code>).</p>
<p>The packet stream may optionally employ memory exports (<code>pa_memexport</code>), memory imports (<code>pa_memimport</code>), and a shared ring buffer channel (<code>pa_srbchannel</code>) if the zero-copy mode is enabled.</p>
<p>The user may send and receive packets (<code>pa_packet</code>), chunks (<code>pa_memchunk</code>), and control messages (<code>shmrelease</code>, <code>shmrevoke</code>). All these message types are used by the “native” protocol.</p>
</li>
<li>
<p><strong>packet dispatcher</strong></p>
<p>Packet dispatcher (<code>pa_pdispatch</code>) looks up and invokes a 
callback for a packet. The user first registers callbacks for commands 
in the dispatcher, and then passes incoming packets to it.</p>
</li>
</ul>
<h2 id="audio-files">
  Audio files
  <a class="anchor" href="#audio-files">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Normally, it’s up to the client application to read or write audio 
files, and both PulseAudio server and client libraries deal only with 
sample streams. However, there are two cases when the server can read an
 audio file directly:</p>
<ul>
<li>the client may ask the server to load an audio file to the sample cache</li>
<li>the client may ask the server to play an audio file</li>
</ul>
<p>In the latter case, the server creates a new sink input that reads 
samples from the file. Both features are available through the CLI 
protocol. The server uses <a href="http://www.mega-nerd.com/libsndfile/">libsndfile</a> to read audio files.</p>
<h2 id="optimizations">
  Optimizations
  <a class="anchor" href="#optimizations">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Depending on the target platform, PulseAudio may employ various 
optimized versions of the sample conversion and software volume 
functions.</p>
<p>They include several functions written in the GCC inline assembly and
 employing MMX (x86), SSE (x86), or NEON (arm) instructions, and several
 functions written in the <a href="https://gstreamer.freedesktop.org/data/doc/orc/">Orc</a> assembly. The latter may be compiled at run-time for the current CPU.</p>
<h2 id="watchdog">
  Watchdog
  <a class="anchor" href="#watchdog">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio server has a built-in watchdog based on the POSIX <a href="http://man7.org/linux/man-pages/man2/getrlimit.2.html">rlimit</a>
 feature. It terminates the server process if it consumes too much CPU 
load and doesn’t respond in time. The server may be then automatically 
started by a client if the autospawn feature is enabled.</p>
<p>The server configures the <code>RLIMIT_CPU</code> timer which has the soft and hard limits:</p>
<ul>
<li>
<p>When the CPU time of the server process reaches the soft limit, the kernel sends to the process the <code>SIGXCPU</code> signal.</p>
<p>The signal handler checks if the server consumed too high percent of 
the CPU load since the previous signal handler invocation. If so, the 
signal handler terminates the server. Otherwise, it restarts the <code>RLIMIT_CPU</code> timer.</p>
</li>
<li>
<p>When the CPU time of the server process reaches the hard limit, the kernel sends to the process the <code>SIGKILL</code> signal.</p>
<p>This signal can’t be handled and unconditionally terminates the 
process. This happens only if the server failed to handle the previous <code>SIGXCPU</code> signal and didn’t restart the <code>RLIMIT_CPU</code> timer before it reached the hard limit.</p>
</li>
</ul>
<hr>
<h1 id="module-list">
  Module list
  <a class="anchor" href="#module-list">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>The tables below provide a brief summary of modules available out of 
the box, grouped by categories. Further details may be found on the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/Modules/">Modules</a> page on wiki.</p>
<h2 id="protocols-and-networking-1">
  Protocols and networking
  <a class="anchor" href="#protocols-and-networking-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<table class="hdr_table pa_table">
 <tbody><tr>
  <th>module</th>
  <th>usage</th>
  <th>description</th>
 </tr>
 <tr>
  <td>module-cli-protocol-{unix,tcp}<br>module-cli</td>
  <td>enabled by default</td>
  <td>Starts the CLI protocol server over a Unix domain socket, TCP socket, or the controlling TTY of the daemon.</td>
 </tr>
 <tr>
  <td>module-dbus-protocol</td>
  <td>enabled by default</td>
  <td>Starts the D-Bus protocol server.</td>
 </tr>
 <tr>
  <td>module-native-protocol-{fd,unix,tcp}</td>
  <td>enabled by default</td>
  <td>Starts the "native" protocol server over a preopened file descriptor, Unix domain socket, or TCP socket.</td>
 </tr>
 <tr>
  <td>module-simple-protocol-{unix,tcp}</td>
  <td>rarely used</td>
  <td>Starts the "simple" protocol server over a Unix domain socket or TCP socket.</td>
 </tr>
 <tr>
  <td>module-esound-protocol-{unix,tcp}</td>
  <td></td>
  <td>Starts the ESound protocol server over a Unix domain socket or TCP socket.</td>
 </tr>
 <tr>
  <td>module-tunnel-{source,sink}</td>
  <td>loaded by another module</td>
  <td>Creates a source or sink connected to a remote source or sink via the "native" protocol (implements client from scratch).</td>
 </tr>
 <tr>
  <td>module-tunnel-{source,sink}-new</td>
  <td>work in progress</td>
  <td>Creates a source or sink connected to a remote source or sink via the "native" protocol (reuses client from libpulse).</td>
 </tr>
 <tr>
  <td>module-zeroconf-discover</td>
  <td>enabled in paprefs</td>
  <td>Listens to mDNS announcements and automatically loads module-tunnel-{source,sink} for every remote source or sink (uses Avahi).</td>
 </tr>
 <tr>
  <td>module-zeroconf-publish</td>
  <td>enabled in paprefs</td>
  <td>Sends mDNS announcements for all local sources and sinks (uses Avahi).</td>
 </tr>
 <tr>
  <td>module-bonjour-publish</td>
  <td>for MacOS</td>
  <td>Sends mDNS announcements for all local sources and sinks (uses Apple Bonjour).</td>
 </tr>
 <tr>
  <td>module-raop-sink</td>
  <td>loaded by another module</td>
  <td>Creates a sink that forwards audio to a remote AirPlay1 device.</td>
 </tr>
 <tr>
  <td>module-raop-discover</td>
  <td>enabled in paprefs</td>
  <td>Listens to mDNS announcements and automatically loads module-raop-sink for every remote AirPlay1 device (uses Avahi).</td>
 </tr>
 <tr>
  <td>module-rtp-recv</td>
  <td>enabled in paprefs</td>
  <td>Listens to SDP/SAP announcements and automatically creates an RTP sink input for every detected RTP sender.</td>
 </tr>
 <tr>
  <td>module-rtp-send</td>
  <td>enabled in paprefs</td>
  <td>Creates RTP source output that sends samples to a preconfigured address, and broadcasts SDP/SAP announcements for it.</td>
 </tr>
 <tr>
  <td>module-http-protocol-{unix,tcp}</td>
  <td>used by other modules</td>
  <td>Starts an HTTP server over a Unix domain socket or TCP socket. 
Implements a web interface and HTTP streaming for sources and sink 
monitors.</td>
 </tr>
 <tr>
  <td>module-rygel-media-server</td>
  <td>enabled in paprefs</td>
  <td>Registers a plugin for the Rygel DLNA / UpNP server. Publishes HTTP streams of sources and sink monitors.</td>
 </tr>
</tbody></table>
<h2 id="device-drivers-1">
  Device drivers
  <a class="anchor" href="#device-drivers-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<table class="hdr_table pa_table">
 <tbody><tr>
  <th>module</th>
  <th>usage</th>
  <th>description</th>
 </tr>
 <tr>
  <td>module-udev-detect</td>
  <td>enabled if available</td>
  <td>Listens to Udev events and automatically loads module-alsa-card for every ALSA card.</td>
 </tr>
 <tr>
  <td>module-hal-detect</td>
  <td>deprecated</td>
  <td>Loads module-udev-detect.</td>
 </tr>
 <tr>
  <td>module-detect</td>
  <td>for systems without Udev</td>
  <td>On start, detects ALSA, OSS, Solaris, and WaveOut devices and loads appropriate source and sink modules.</td>
 </tr>
 <tr>
  <td>module-alsa-card</td>
  <td>loaded by another module</td>
  <td>Creates a card for an ALSA card, and automatically creates sources and sinks for inner ALSA devices.</td>
 </tr>
 <tr>
  <td>module-alsa-{source,sink}</td>
  <td></td>
  <td>Creates a source or sink for an ALSA device.</td>
 </tr>
 <tr>
  <td>module-bluetooth-discover</td>
  <td>enabled by default</td>
  <td>Loads either module-bluez5-discover or module-bluez4-discover.</td>
 </tr>
 <tr>
  <td>module-bluetooth-policy</td>
  <td>enabled by default</td>
  <td>Automatically switches card profiles of Bluetooth cards and loads 
module-loopback to route and play music from new Bluetooth sources.</td>
 </tr>
 <tr>
  <td>module-{bluez5,bluez4}-discover</td>
  <td>loaded by another module</td>
  <td>Listens to Bluez and oFono events on D-Bus and automatically loads module-{bluez5,bluez4}-device for every device.</td>
 </tr>
 <tr>
  <td>module-{bluez5,bluez4}-device</td>
  <td>loaded by another module</td>
  <td>Creates a card, source, and sink for a Bluetooth device.</td>
 </tr>
 <tr>
  <td>module-jackdbus-detect</td>
  <td></td>
  <td>Listens to JACK events on D-Bus and automatically loads module-jack-{source,sink} when JACK is started.</td>
 </tr>
 <tr>
  <td>module-jack-{source,sink}</td>
  <td>loaded by another module</td>
  <td>Creates a source or sink that read and write samples to JACK.</td>
 </tr>
 <tr>
  <td>module-oss</td>
  <td>for systems with OSS</td>
  <td>Creates a source and sink for an OSS device (/dev/dspN).</td>
 </tr>
 <tr>
  <td>module-solaris</td>
  <td>for Solaris and some *BSD</td>
  <td>Creates a source and sink for a Sun audio device (/dev/audio).</td>
 </tr>
 <tr>
  <td>module-coreaudio-detect</td>
  <td>for MacOS</td>
  <td>Listens to CoreAudio events and automatically loads module-coreaudio-device for every CoreAudio device.</td>
 </tr>
 <tr>
  <td>module-coreaudio-device</td>
  <td>for MacOS</td>
  <td>Creates a source and sink for a CoreAudio device.</td>
 </tr>
 <tr>
  <td>module-waveout</td>
  <td>for Windows</td>
  <td>Creates a source and sink for Win32 WaveIn/WaveOut devices.</td>
 </tr>
 <tr>
  <td>module-esound-sink</td>
  <td>has latency issues</td>
  <td>Creates a sink connected to the ESound daemon.</td>
 </tr>
 <tr>
  <td>module-lirc</td>
  <td></td>
  <td>Listens to LIRC events from an IR remote control and forward volume up/down and mute requests to a preconfigured sink.</td>
 </tr>
 <tr>
  <td>module-mmkbd-evdev</td>
  <td></td>
  <td>Listens to evdev events from the multimedia keyboard and forward volume up/down and mute requests to a preconfigured sink.</td>
 </tr>
</tbody></table>
<h2 id="sound-processing-1">
  Sound processing
  <a class="anchor" href="#sound-processing-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<table class="hdr_table pa_table">
 <tbody><tr>
  <th>module</th>
  <th>usage</th>
  <th>description</th>
 </tr>
 <tr>
  <td>module-loopback</td>
  <td>used by other modules</td>
  <td>Creates a pair of virtual source output and sink input connected with a queue, may be used as a source-to-sink adapter.</td>
 </tr>
 <tr>
  <td>module-null-source</td>
  <td></td>
  <td>Creates a virtual source that always produces silence.</td>
 </tr>
 <tr>
  <td>module-null-sink</td>
  <td></td>
  <td>Creates a virtual sink that silently drops all data. Together with
 its monitor, may be used as a source-output-to-sink-input adapter.</td>
 </tr>
 <tr>
  <td>module-combine-sink</td>
  <td>enabled in paprefs</td>
  <td>Creates a virtual sink that duplicates data to several other sinks.</td>
 </tr>
 <tr>
  <td>module-combine</td>
  <td>deprecated</td>
  <td>Loads module-combine-sink.</td>
 </tr>
 <tr>
  <td>module-sine-source<br>module-sine</td>
  <td></td>
  <td>Creates a virtual source or sink input that generates a sine wave with the preconfigured frequency.</td>
 </tr>
 <tr>
  <td>module-pipe-{source,sink}</td>
  <td></td>
  <td>Creates a virtual source or sink that reads or writes data to a preconfigured file or named pipe.</td>
 </tr>
 <tr>
  <td>module-remap-{source,sink}</td>
  <td>autoloadable filter</td>
  <td>Creates a filter source or sink that performs channel remapping on top of the master source or sink.</td>
 </tr>
 <tr>
  <td>module-equalizer-sink</td>
  <td>autoloadable filter</td>
  <td>Creates a filter sink that implements a digital equalizer on top 
of the master source or sink. The equalizer may be controlled via D-Bus.</td>
 </tr>
 <tr>
  <td>module-virtual-surround-sink</td>
  <td>autoloadable filter</td>
  <td>Creates a filter sink that performs a convolution with a HRIR WAV file on top of the master source or sink.</td>
 </tr>
 <tr>
  <td>module-virtual-{source,sink}</td>
  <td>autoloadable filter</td>
  <td>Creates a filter source or sink that just reads or writes data to the master source or sink.</td>
 </tr>
 <tr>
  <td>module-echo-cancel</td>
  <td>autoloadable filter</td>
  <td>Creates a paired filter source and sink that perform acoustic echo cancellation on top of the master source and sink.</td>
 </tr>
 <tr>
  <td>module-ladspa-sink</td>
  <td>autoloadable filter</td>
  <td>Creates a filter sink that applies an audio filter from an 
external LADSPA plugin on top of the master source or sink. The plugin 
may be controlled via D-Bus.</td>
 </tr>
</tbody></table>
<h2 id="power-saving-1">
  Power saving
  <a class="anchor" href="#power-saving-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<table class="hdr_table pa_table">
 <tbody><tr>
  <th>module</th>
  <th>usage</th>
  <th>description</th>
 </tr>
 <tr>
  <td>module-suspend-on-idle</td>
  <td>enabled by default</td>
  <td>Monitors sources and sinks and automatically suspends them when there are no connected streams for some period of time.</td>
 </tr>
</tbody></table>
<h2 id="automatic-setup-and-routing-1">
  Automatic setup and routing
  <a class="anchor" href="#automatic-setup-and-routing-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<table class="hdr_table pa_table">
 <tbody><tr>
  <th>module</th>
  <th>usage</th>
  <th>description</th>
 </tr>
 <tr>
  <td>module-default-device-restore</td>
  <td>enabled by default</td>
  <td>Saves and restores the fallback source and sink.</td>
 </tr>
 <tr>
  <td>module-card-restore</td>
  <td>enabled by default</td>
  <td>Saves and restores active profiles of card.</td>
 </tr>
 <tr>
  <td>module-device-restore</td>
  <td>enabled by default</td>
  <td>Saves and restores volume/mute settings and active ports of devices.</td>
 </tr>
 <tr>
  <td>module-stream-restore</td>
  <td>enabled by default</td>
  <td>Saves and restores volume/mute settings and routing rules of stream groups. Routes new streams according to stored device name.</td>
 </tr>
 <tr>
  <td>module-volume-restore</td>
  <td>deprecated</td>
  <td>Loads module-stream-restore.</td>
 </tr>
 <tr>
  <td>module-device-manager</td>
  <td>enabled in KDE</td>
  <td>Stores per-role and global priority lists of devices. Routes new streams according to the stored priority lists.</td>
 </tr>
 <tr>
  <td>module-intended-roles</td>
  <td>enabled by default</td>
  <td>Routes new streams according to the stream role and device intended role list.</td>
 </tr>
 <tr>
  <td>module-augment-properties</td>
  <td>enabled by default</td>
  <td>Computes client properties from the desktop file of the application.</td>
 </tr>
 <tr>
  <td>module-filter-heuristics</td>
  <td>enabled by default</td>
  <td>Adjusts the filters requested via the stream properties.</td>
 </tr>
 <tr>
  <td>module-filter-apply</td>
  <td>enabled by default</td>
  <td>Loads the filters requested via the stream properties.</td>
 </tr>
 <tr>
  <td>module-switch-on-port-available</td>
  <td>enabled by default</td>
  <td>When the availability of device ports changes, automatically switch the active device port and card profile.</td>
 </tr>
 <tr>
  <td>module-switch-on-connect</td>
  <td></td>
  <td>When a new device appears, automatically sets it as the fallback device and move all active streams to it.</td>
 </tr>
 <tr>
  <td>module-rescue-streams</td>
  <td>enabled by default</td>
  <td>When a device disappears, automatically moves streams connected to it to another working device.</td>
 </tr>
 <tr>
  <td>module-always-sink</td>
  <td>enabled by default</td>
  <td>When all sinks disappear, automatically loads the null sink.</td>
 </tr>
 <tr>
  <td>module-role-{ducking,cork}</td>
  <td>enabled by default</td>
  <td>When an important stream is started, automatically ducks (lower the volume) or corks (mute and request a pause) active streams.</td>
 </tr>
 <tr>
  <td>module-allow-passthrough</td>
  <td></td>
  <td>When a new passthrough stream is moved to an existing sink, 
automatically creates a null sink and moves all other running streams to
 that null sink.</td>
 </tr>
 <tr>
  <td>module-position-event-sounds</td>
  <td>enabled by default</td>
  <td>When a stream is created or its properties are changed, adjusts 
the volume balance of the stream depending on its on-screen 2-D 
coordinated provided by the application.</td>
 </tr>
 <tr>
  <td>module-match</td>
  <td></td>
  <td>When a new stream appears, automatically updates its volume based on its name and a preconfigured match table.</td>
 </tr>
</tbody></table>
<h2 id="desktop-integrations-1">
  Desktop integrations
  <a class="anchor" href="#desktop-integrations-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<table class="hdr_table pa_table">
 <tbody><tr>
  <th>module</th>
  <th>usage</th>
  <th>description</th>
 </tr>
 <tr>
  <td>module-systemd-login</td>
  <td>enabled by default</td>
  <td>Listens to logind events on D-Bus and creates a fake PulseAudio 
client for every new login session of the current user to prevent server
 from exiting until the user logouts.</td>
 </tr>
 <tr>
  <td>module-console-kit</td>
  <td>for systems w/o systemd</td>
  <td>Listens to ConsoleKit events on D-Bus and creates a fake 
PulseAudio client for every new login session of the current user to 
prevent server from exiting the user logouts.</td>
 </tr>
 <tr>
  <td>module-x11-xsmp</td>
  <td>enabled by default</td>
  <td>Listens to X session manager events and creates a fake PulseAudio 
client for the current login session to prevent server from exiting the 
user logouts.</td>
 </tr>
 <tr>
  <td>module-x11-publish</td>
  <td>enabled by default</td>
  <td>Publishes server address and credentials via the X11 root window 
properties, which may be read by clients running on the same X display, 
including remote clients that use SSH X forwarding.</td>
 </tr>
 <tr>
  <td>module-x11-bell</td>
  <td>enabled by default</td>
  <td>Intercepts XKB bell events and plays a preconfigured sample from the sample cache instead of the default X11 bell.</td>
 </tr>
 <tr>
  <td>module-x11-cork-request</td>
  <td>enabled by default</td>
  <td>When an (un)cork is requested for a stream, synthesizes X11 media 
key event and sends it to the application. A workaround for applications
 that don't handle cork requests, but do handle multimedia keys.</td>
 </tr>
 <tr>
  <td>module-gconf</td>
  <td>enabled by default</td>
  <td>Monitors GConf directory and automatically loads PulseAudio modules listed there.</td>
 </tr>
</tbody></table>
<h2 id="compatibility-layers-1">
  Compatibility layers
  <a class="anchor" href="#compatibility-layers-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<table class="hdr_table pa_table">
 <tbody><tr>
  <th>module</th>
  <th>usage</th>
  <th>description</th>
 </tr>
 <tr>
  <td>module-esound-compat-spawnpid<br>module-esound-compat-spawnfd</td>
  <td>loaded by esdcompat</td>
  <td>Notifies the process that started the server that the server was successfully started with a signal or via a file descriptor.</td>
 </tr>
</tbody></table>
<hr>
<h1 id="gui-tools">
  GUI tools
  <a class="anchor" href="#gui-tools">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>This section provides a brief summary of the three standard GUI tools
 for PulseAudio. Besides these tools, most desktop environments also 
provide their own tools or applets.</p>
<h2 id="pavucontrol">
  pavucontrol
  <a class="anchor" href="#pavucontrol">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p><a href="https://freedesktop.org/software/pulseaudio/pavucontrol/">pavucontrol</a> (PulseAudio Volume Control) provides the following features:</p>
<ul>
<li>setup volumes or mute sources, source outputs, sinks, and sink inputs</li>
<li>connect sink inputs to sinks</li>
<li>connect source outputs to sources</li>
<li>set fallback source and sink</li>
<li>set active device port of a source or sink</li>
<li>set active card profile of a card</li>
<li>configure latency offset and passthrough mode of a source or sink</li>
</ul>
<p>This tool uses the C API to communicate with the server. PulseAudio 
automatically saves most of these setting to the restoration database, 
so they are persistent.</p>
<div class="tile">
  <a data-lightbox="pavucontrol" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/pavucontrol_playback_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/pavucontrol_playback_ann.png">
  </a>
  <a data-lightbox="pavucontrol" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/pavucontrol_recording_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/pavucontrol_recording_ann.png">
  </a>
  <a data-lightbox="pavucontrol" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/pavucontrol_output_devices_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/pavucontrol_output_devices_ann.png">
  </a>
  <a data-lightbox="pavucontrol" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/pavucontrol_input_devices_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/pavucontrol_input_devices_ann.png">
  </a>
  <a data-lightbox="pavucontrol" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/pavucontrol_configuration_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/pavucontrol_configuration_ann.png">
  </a>
</div>
<h2 id="paprefs">
  paprefs
  <a class="anchor" href="#paprefs">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p><a href="https://freedesktop.org/software/pulseaudio/paprefs/">paprefs</a> (PulseAudio Preferences) provides methods to enable modules or module options that are disabled by default.</p>
<p>This tool just writes module names and arguments to the GNOME 
registry (GConf) and PulseAudio automatically loads the modules. These 
settings are persistent.</p>
<div class="tile">
  <a data-lightbox="paprefs" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/paprefs_network_access_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/paprefs_network_access_ann.png">
  </a>
  <a data-lightbox="paprefs" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/paprefs_network_server_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/paprefs_network_server_ann.png">
  </a>
  <a data-lightbox="paprefs" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/paprefs_multicast_rtp_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/paprefs_multicast_rtp_ann.png">
  </a>
  <a data-lightbox="paprefs" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/paprefs_simultaneous_output_ann.png">
    <img src="PulseAudio%20under%20the%20hood_files/paprefs_simultaneous_output_ann.png">
  </a>
</div>
<h2 id="qpaeq">
  qpaeq
  <a class="anchor" href="#qpaeq">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p><a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/Equalizer/">qpaeq</a> (Qt PulseAudio Equalizer) is a frontend for the equalizer sink.</p>
<p>This tool communicates with the server through the D-Bus API. It 
first creates an equalizer sink connected to a selected master sink, and
 then uses the D-Bus API extension registered by the equalized sink.</p>
<p>The number of sliders depends on the window size, so a larger window gives a higher precision.</p>
<div class="tile">
  <a data-lightbox="qpaeq" href="https://gavv.net/articles/pulseaudio-under-the-hood/screenshots/qpaeq.png">
    <img src="PulseAudio%20under%20the%20hood_files/qpaeq.png">
  </a>
</div>
<hr>
<h1 id="command-line-tools">
  Command line tools
  <a class="anchor" href="#command-line-tools">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>PulseAudio package comes with several command line tools.</p>
<h2 id="server">
  Server
  <a class="anchor" href="#server">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>pulseaudio</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/pulseaudio.1.html">pulseaudio</a> tool starts or kills the server.</p>
<p>The user may specify what configuration files to use, what modules to
 load and from where, configure log levels, and some other options.</p>
</li>
</ul>
<h2 id="clients">
  Clients
  <a class="anchor" href="#clients">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>pacmd</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/pacmd.1.html">pacmd</a> tool starts an interactive session for server configuration.</p>
<p>It connects to the server via the <a href="http://manpages.ubuntu.com/manpages/en/man5/pulse-cli-syntax.5.html">CLI</a>
 protocol over a Unix domain socket. This text protocol provides a 
variety of commands to inspect and configure the server. The tool 
redirects its stdin and stdout to the socket so that the user directly 
communicates with the server.</p>
<p>The pacmd tool doesn’t work over the network or when the server is 
running in system mode and doesn’t support autospawn. Users are 
encouraged to use the pactl tool instead.</p>
</li>
<li>
<p><strong>pactl</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/pactl.1.html">pactl</a> tool implements non-interactive commands for server configuration.</p>
<p>It communicates with the server via the C API, which uses the 
“native” protocol internally. The tool understands a number of commands 
which should be specified via the command line arguments. It supports 
most of the features available in the CLI protocol.</p>
</li>
<li>
<p><strong>pacat</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/pacat.1.html">pacat</a> tool implements a playback and recording client.</p>
<p>The paplay, parecord, parec, and pamon tools are symlinks to the 
pacat tool. The tool has four operation modes. The mode is determined by
 the symlink used to invoke the tool:</p>
<ul>
<li>
<p><strong>paplay</strong></p>
<p>Create a playback stream, read and decode samples from an audio file using libsndfile, and send samples to the stream.</p>
</li>
<li>
<p><strong>parecord</strong></p>
<p>Create a recording stream, receive samples from the stream, and encode and write them to a file using libsndfile.</p>
</li>
<li>
<p><strong>pacat</strong></p>
<p>Create a playback stream, read raw samples from stdin, and send samples to the stream.</p>
</li>
<li>
<p><strong>parec or pamon</strong></p>
<p>Create a recording stream, receive samples from the stream, and write raw samples to stdout.</p>
</li>
</ul>
</li>
</ul>
<h2 id="desktop">
  Desktop
  <a class="anchor" href="#desktop">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>start-pulseaudio-x11</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/start-pulseaudio-x11.1.html">start-pulseaudio-x11</a> tool starts the server for the current X11 session.</p>
<p>It relies on the autospawn feature to start the server. The tool loads several desktop-related modules, passing them the <code>$DISPLAY</code> and <code>$SESSION_MANAGER</code> environment variables.</p>
</li>
<li>
<p><strong>pax11publish</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/pax11publish.1.html">pax11publish</a> tool publishes the server address and credentials via the X11 root window properties.</p>
<p>The tool sets X11 properties that may be used by PulseAudio clients 
to connect to the server. The server address and credentials should be 
manually provided by the user via command line arguments. The tool is 
now superseded by the module-x11-publish and start-pulseaudio-x11 tool.</p>
</li>
</ul>
<h2 id="compatibility">
  Compatibility
  <a class="anchor" href="#compatibility">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>pasuspender</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/pasuspender.1.html">pasuspender</a> tool is a wrapper for applications that require an exclusive access to devices.</p>
<p>The tool connects to the server via the C API and asks the server to 
suspend all sinks and sources. Then it runs a child process, that may 
freely use any devices, typically ALSA devices. When the child exits, 
the tool asks the server to resume sinks and sources.</p>
</li>
<li>
<p><strong>padsp</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/padsp.1.html">padsp</a> tool is a wrapper for OSS applications.</p>
<p>The tool intercepts standard library functions of an OSS application using the <code>LD_PRELOAD</code>
 trick and redirects sound to PulseAudio. This approach works for some 
applications but is known to be incomplete. The ossp daemon with the 
ossp-padsp backend may be a better alternative for this tool.</p>
</li>
<li>
<p><strong>esdcompat</strong></p>
<p><a href="http://manpages.ubuntu.com/manpages/man1/esdcompat.1.html">esdcompat</a> emulates ESound autospawn feature.</p>
<p>The tool uses PulseAudio autospawn to start PulseAudio server and 
loads ESound compatibility modules that emulate ESound autospawn. The 
tool takes the same arguments as the ESound daemon so it can be used as a
 drop-in replacement for the esd tool.</p>
</li>
</ul>
<hr>
<h1 id="configuration">
  Configuration
  <a class="anchor" href="#configuration">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<div class="flex_table">
  <div class="flex_th">component</div>
  <div>libpulse</div>
  <div>libpulsecore</div>
</div>
<p>PulseAudio configuration is documented on the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/">User</a> and the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/FAQ/">FAQ</a> pages on wiki.</p>
<p>The Arch Linux wiki may be also useful: <a href="https://wiki.archlinux.org/index.php/PulseAudio">1</a>, <a href="https://wiki.archlinux.org/index.php/PulseAudio/Configuration">2</a>, <a href="https://wiki.archlinux.org/index.php/PulseAudio/Examples">3</a>, <a href="https://wiki.archlinux.org/index.php/PulseAudio/Troubleshooting">4</a>.</p>
<h2 id="system-and-user-modes">
  System and user modes
  <a class="anchor" href="#system-and-user-modes">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are two ways to run the server:</p>
<ul>
<li>one instance per-user</li>
<li>one system-wide instance</li>
</ul>
<p>The system-wide mode is not recommended but is useful in some cases like an embedded system. See the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/SystemWide/">SystemWide</a> and <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/WhatIsWrongWithSystemWide/">WhatIsWrongWithSystemWide</a> pages on wiki.</p>
<p>The major differences between the two modes are the following:</p>
<ul>
<li>
<p>In the system-wide mode, the most of the desktop integration 
features, like session management and X11 publishing, are not necessary 
and can’t be used since the server is not bound to a login session or 
X11 display.</p>
</li>
<li>
<p>In the per-user mode, server instances use the device reservation API
 to acquire exclusive access on ALSA devices. In the system-wide mode, 
this API is not used.</p>
</li>
<li>
<p>In the per-user mode, PulseAudio system directories are shared 
between server instances, and PulseAudio user directories belong to the 
user. In the system-wide mode, PulseAudio user directories belong to the
 “pulse” user.</p>
</li>
</ul>
<p>From the usability and security points of view, the server is not 
designed to be shared by multiple users. There is no user separation 
inside the server. In particular:</p>
<ul>
<li>
<p>Any connected client can read and control streams or devices used by 
another client. Any connected client can affect global configuration, 
like device hotplug.</p>
</li>
<li>
<p>All connected clients share the same persistent state, including the default source and sink, the restoration database, etc.</p>
</li>
</ul>
<h2 id="system-directories">
  System directories
  <a class="anchor" href="#system-directories">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio uses the following system-wide directories:</p>
<ul>
<li>
<p><strong>config directory</strong></p>
<p>Holds system-wide configuration files.</p>
<p>Usually set to <code>"/etc/pulse"</code>.</p>
<p>Contains <code>.conf</code> and <code>.pa</code> configuration files.</p>
</li>
<li>
<p><strong>module directory</strong></p>
<p>Holds dynamically loadable libraries for server modules.</p>
<p>Usually set to <code>"/usr/lib/pulse-{VERSION}/modules"</code>.</p>
<p>Contains <code>"module-{NAME}.so"</code> files.</p>
</li>
<li>
<p><strong>data directory</strong></p>
<p>Holds platform-independent server data.</p>
<p>Usually set to <code>"/usr/share/pulseaudio"</code>.</p>
<p>Contains <code>"alsa-mixer"</code> directory with ALSA profiles for non-UCM ALSA cards. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Backends/ALSA/Profiles/">Profiles</a> page on wiki.</p>
</li>
</ul>
<h2 id="user-directories">
  User directories
  <a class="anchor" href="#user-directories">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio uses the following per-user directories (belonging to the “pulse” user in the system-wide mode):</p>
<ul>
<li>
<p><strong>home directory</strong></p>
<p>Default or parent directory for the configuration files, persistent state, and runtime state of the user’s server instance.</p>
<p>Usually set to <code>"$XDG_CONFIG_HOME/pulse"</code> (typically <code>"~/.config/pulse"</code>). However, if the <code>"$HOME/.pulse"</code> directory already exists, it’s used otherwise.</p>
<p>Contains:</p>
<ul>
<li>a cookie file (for the “native” protocol authentication)</li>
<li>usually, contains configuration files, if the config directory is the same as the home directory</li>
<li>usually, contains persistent state, if the state directory is the same as the home directory</li>
<li>may contain a symlink to the runtime directory</li>
</ul>
</li>
<li>
<p><strong>config directory</strong></p>
<p>Holds per-user configuration files that override global configuration.</p>
<p>Usually, this is the same directory as the home directory. However, another path may be specified via the <code>$PULSE_CONFIG_PATH</code> environment variable.</p>
<p>Contains <code>.conf</code> and <code>.pa</code> configuration files.</p>
</li>
<li>
<p><strong>state directory</strong></p>
<p>Holds per-user persistent state that is changed dynamically and should be kept across reboots.</p>
<p>Usually, this is the same directory as the home directory. However, another path may be specified via the <code>$PULSE_STATE_PATH</code> environment variable.</p>
<p>Contains:</p>
<ul>
<li>default source and sink names</li>
<li>restoration database</li>
<li>device manager database</li>
<li>equalizer sink database</li>
</ul>
</li>
<li>
<p><strong>runtime directory</strong></p>
<p>Holds per-user runtime state that should be cleared when the server restarts.</p>
<p>If the <code>$PULSE_RUNTIME_PATH</code> environment variable is set, it specifies the path of the runtime directory. Otherwise, if the <code>$XDG_RUNTIME_DIR</code> environment variable is set, the runtime directory path is set to <code>"$XDG_RUNTIME_DIR/pulse"</code> instead (typically somewhere in <code>"/run"</code>).</p>
<p>If non of these environment variables are set, the runtime directory is created in <code>"/tmp"</code>, and a symlink to it is created in the home directory. The symlink name includes the <a href="http://man7.org/linux/man-pages/man5/machine-id.5.html">machine id</a>.</p>
<p>Contains:</p>
<ul>
<li>sockets files</li>
<li>pid file</li>
<li>lock files</li>
</ul>
</li>
</ul>
<h2 id="configuration-files">
  Configuration files
  <a class="anchor" href="#configuration-files">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio uses two types of configuration files:</p>
<ul>
<li><code>.conf</code> - client or server options in a simple key-value format</li>
<li><code>.pa</code> - server initialization commands in the <a href="http://manpages.ubuntu.com/manpages/zesty/en/man5/pulse-cli-syntax.5.html">CLI</a> protocol format</li>
</ul>
<p>Four files are used by default:</p>
<ul>
<li><code>daemon.conf</code> - server options</li>
<li><code>client.conf</code> - client options</li>
<li><code>default.pa</code> - server initialization for the per-user mode</li>
<li><code>system.pa</code> - server initialization for the system-wide mode</li>
</ul>
<p>If the user config directory contains <code>.conf</code> files, the system <code>.conf</code> files with the same name are ignored.</p>
<h2 id="sockets">
  Sockets
  <a class="anchor" href="#sockets">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio uses two types of sockets:</p>
<ul>
<li>Unix domain sockets (for local clients)</li>
<li>TCP sockets (for remote clients)</li>
</ul>
<p>The table below lists the non-standard TCP ports used by PulseAudio.</p>
<table class="hdr_table">
 <tbody><tr>
  <th>module</th>
  <th>port</th>
 </tr>
 <tr>
  <td>module-native-protocol-tcp</td>
  <td>4713</td>
 </tr>
 <tr>
  <td>module-simple-protocol-tcp</td>
  <td>4711</td>
 </tr>
 <tr>
  <td>module-http-protocol-tcp</td>
  <td>4714</td>
 </tr>
 <tr>
  <td>module-cli-protocol-tcp</td>
  <td>4712</td>
 </tr>
</tbody></table>
<h2 id="client-startup">
  Client startup
  <a class="anchor" href="#client-startup">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Every client that uses libpulse performs the following steps at startup:</p>
<ul>
<li>
<p><strong>initialize</strong></p>
<p>First, the client reads the server address, the authentication cookie, and other options from:</p>
<ul>
<li>environment variables</li>
<li>X11 root window properties</li>
<li>per-user and system-wide client configuration files</li>
</ul>
</li>
<li>
<p><strong>connect</strong></p>
<p>When the initialization is done, the client tries to connect to the following addresses:</p>
<ul>
<li>a Unix socket for the “native” protocol in the per-user and system-wide runtime directories</li>
<li>a TCP socket on localhost</li>
<li>a TCP socket on the host defined by the <code>$DISPLAY</code> environment variable</li>
</ul>
</li>
<li>
<p><strong>autospawn</strong></p>
<p>If the client can’t connect to the server, it automatically starts the server if all of the following is true:</p>
<ul>
<li>the autospawn is not disabled in the client configuration file</li>
<li>the client is not running under the <code>root</code> user</li>
<li>the server address is not set, or it’s set and belongs to the same login session as the client</li>
</ul>
</li>
<li>
<p><strong>authenticate</strong></p>
<p>When the client has connected to the server, it tries to read the authentication cookie from:</p>
<ul>
<li>environment variables</li>
<li>X11 root window properties</li>
<li>a cookie file explicitly provided by the application</li>
<li>per-user and system-wide client configuration files</li>
<li>per-user home directory</li>
</ul>
</li>
</ul>
<hr>
<h1 id="portability">
  Portability
  <a class="anchor" href="#portability">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>PulseAudio may work on several POSIX-compatible platforms. See <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/About/">About</a> and <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Ports/">Ports</a> pages on wiki.</p>
<p>Currently supported operating systems are:</p>
<ul>
<li>Linux</li>
<li>Android</li>
<li>*BSD</li>
<li>Solaris</li>
<li>Mac OSX</li>
<li>Windows</li>
</ul>
<p>PulseAudio core implements platform wrappers for low-level stuff like
 threading, networking, I/O, and shared memory. The rest code is mostly 
cross-platform.</p>
<p>However, some important features rely on platform-specific external components:</p>
<ul>
<li>
<p>The only full-featured hardware backends are ALSA and Bluetooth, which a both Linux-specific.</p>
</li>
<li>
<p>The timer-based scheduler is implemented only for ALSA cards, hence it’s also Linux-specific.</p>
</li>
<li>
<p>Most desktop integration features depend on freedesktop and Unix or 
Linux-specific components which are not used on Android, Mac OSX, and 
Windows.</p>
</li>
<li>
<p>Hardware controls handling also depend on Unix or Linux-specific components which are not used on Mac OSX and Windows.</p>
</li>
</ul>
<p>In result, Linux is the only platform on which all of the important 
features are supported. Other Unix desktops are supported but have 
limited functionality. Some features rely on the Linux and Unix desktop 
stack, therefore non-Unix desktop support is even more limited.</p>
<hr>
<h1 id="example-setups">
  Example setups
  <a class="anchor" href="#example-setups">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>This section demonstrates example PulseAudio configurations for several common and advanced use cases.</p>
<h2 id="playback-and-recording">
  Playback and recording
  <a class="anchor" href="#playback-and-recording">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>Connect a recording application to a source</strong></p>
<p>An application connects to the server via the “native” protocol and 
creates a recording stream. The server creates a source output for the 
stream. The source output is connected to a source.</p>
  <img src="PulseAudio%20under%20the%20hood_files/pnr_app_to_source.png" width="409px">
<p>Configuration:</p>
<ol>
<li>
<p>Start the recording application:</p>
<pre><code> $ parecord output.wav
</code></pre>
</li>
<li>
<p>Connect the parecord source output to the source using the pavucontrol tool.</p>
</li>
</ol>
</li>
<li>
<p><strong>Connect a playback application to a sink</strong></p>
<p>An application connects to the server via the “native” protocol and 
creates a playback stream. The server creates a sink input for the 
stream. The sink input is connected to a sink.</p>
  <img src="PulseAudio%20under%20the%20hood_files/pnr_app_to_sink.png" width="409px">
<p>Configuration:</p>
<ol>
<li>
<p>Start the playback application:</p>
<pre><code> $ paplay input.wav
</code></pre>
</li>
<li>
<p>Connect the paplay sink input to the sink using the pavucontrol tool.</p>
</li>
</ol>
</li>
<li>
<p><strong>Connect a playback application to a filter sink</strong></p>
<p>An application connects to the server via the “native” protocol and 
creates a playback stream. The server creates a sink input for the 
stream. The sink input is connected to a filter sink, which in turn is 
connected to the master sink.</p>
  <img src="PulseAudio%20under%20the%20hood_files/pnr_app_to_filter.png" width="681px">
<p>Configuration:</p>
<ol>
<li>
<p>Create an equalizer sink and sink input:</p>
<pre><code> $ pactl load-module module-dbus-protocol
 $ qpaeq
</code></pre>
</li>
<li>
<p>Connect the equalizer sink input to the master sink using the pavucontrol tool.</p>
</li>
<li>
<p>Start the playback application:</p>
<pre><code> $ paplay input.wav
</code></pre>
</li>
<li>
<p>Connect the paplay sink input to the equalizer sink using the pavucontrol tool.</p>
</li>
</ol>
</li>
</ul>
<h2 id="capturing-sound">
  Capturing sound
  <a class="anchor" href="#capturing-sound">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>Connect a sink to a file</strong></p>
<p>The monitor source of a sink is connected to a loopback, which in 
turn is connected to a pipe sink (because it’s not possible to connect a
 source to a sink directly). The pipe sink writes samples to a file.</p>
  <img src="PulseAudio%20under%20the%20hood_files/capture_sink_to_file.png" width="817px">
<p>Configuration:</p>
<ol>
<li>
<p>Create the pipe sink:</p>
<pre><code> $ pactl load-module module-pipe-sink file="$(pwd)/output.pcm"
</code></pre>
</li>
<li>
<p>Create the loopback sink input and source output:</p>
<pre><code> $ pactl load-module module-loopback
</code></pre>
</li>
<li>
<p>Connect the loopback sink input to the pipe sink using the pavucontrol tool.</p>
</li>
<li>
<p>Connect the loopback source output to the sink monitor of a sink using the pavucontrol tool.</p>
</li>
<li>
<p>When the capture is done, play the recorded file:</p>
<pre><code> $ pacat output.pcm
</code></pre>
</li>
</ol>
</li>
<li>
<p><strong>Connect a playback application to a file</strong></p>
<p>An application connects to the server via the “native” protocol and 
creates a playback stream. The server creates a sink input for the 
stream. The sink input is connected to a pipe sink. The pipe sink writes
 samples to a file.</p>
  <img src="PulseAudio%20under%20the%20hood_files/capture_app_to_file.png" width="545px">
<p>Configuration:</p>
<ol>
<li>
<p>Create the pipe sink:</p>
<pre><code> $ pactl load-module module-pipe-sink file="$(pwd)/output.pcm"
</code></pre>
</li>
<li>
<p>Start the playback application:</p>
<pre><code> $ paplay input.wav
</code></pre>
</li>
<li>
<p>Connect the paplay sink input to the pipe sink using the pavucontrol tool.</p>
</li>
<li>
<p>When the capture is done, play the recorded file:</p>
<pre><code> $ pacat output.pcm
</code></pre>
</li>
</ol>
</li>
<li>
<p><strong>Connect a playback application to a recording application</strong></p>
<p>Two applications connect to the server via the “native” protocol and 
create playback and recording streams. The server creates sink input and
 source output for the streams. The sink input is connected to the 
source output via a null sink (because it’s not possible to connect a 
sink input to a source output directly).</p>
  <img src="PulseAudio%20under%20the%20hood_files/capture_app_to_app.png" width="817px">
<p>Configuration:</p>
<ol>
<li>
<p>Create a null sink:</p>
<pre><code> $ pactl load-module module-null-sink
</code></pre>
</li>
<li>
<p>Start the playback application:</p>
<pre><code> $ paplay input.wav
</code></pre>
</li>
<li>
<p>Start the recording application:</p>
<pre><code> $ parecord output.wav
</code></pre>
</li>
<li>
<p>Connect the paplay sink input to the null sink using the pavucontrol tool.</p>
</li>
<li>
<p>Connect the parecord source output to the monitor of the null sink using the pavucontrol tool.</p>
</li>
</ol>
</li>
</ul>
<h2 id="native-protocol-1">
  Native protocol
  <a class="anchor" href="#native-protocol-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>Connect a local playback application to a remote sink</strong></p>
<p>An application connects to the local server via the “native” protocol
 and creates a playback stream. The local server creates a sink input 
for the stream. The local sink input is connected to a tunnel sink. The 
tunnel sink connects to the remote server via the “native” protocol and 
creates a playback stream. The remote server creates a sink input for 
the stream. The remote sink input is connected to a sink.</p>
  <img src="PulseAudio%20under%20the%20hood_files/native_app_to_sink.png" width="409px">
<p>Configuration:</p>
<ol>
<li>
<p>Enable Zeroconf publishing on the remote server using the paprefs tool or the following commands:</p>
<pre><code> $ (start avahi daemon)
 $ pactl load-module module-native-protocol-tcp auth-anonymous=1
 $ pactl load-module module-zeroconf-publish
</code></pre>
</li>
<li>
<p>Enable Zeroconf discovery on the local server using the paprefs tool or the following commands:</p>
<pre><code> $ (start avahi daemon)
 $ pactl load-module module-zeroconf-discover
</code></pre>
</li>
<li>
<p>Wait until the tunnel sinks and sources appear on the local server.</p>
</li>
<li>
<p>Start the playback application on the local server:</p>
<pre><code> $ paplay input.wav
</code></pre>
</li>
<li>
<p>Connect the local sink input to the tunnel sink using the pavucontrol tool on the local server.</p>
</li>
<li>
<p>Connect the remote sink input to a sink using the pavucontrol tool on the remote server.</p>
</li>
</ol>
</li>
<li>
<p><strong>Connect a local playback application to a remote recording application</strong></p>
<p>The local application connects to the local server via the “native” 
protocol and creates a playback stream. The local server creates a sink 
input for the stream. The remote application connects to the remote 
server via the “native” protocol and creates a recording stream. The 
remote server creates a source output for the stream.</p>
<p>The local sink input is connected to a tunnel sink. The tunnel sink 
connects to the remote server via the “native” protocol and creates a 
playback stream. The remote server creates a sink input for the stream. 
The remote sink input is connected to the remote source output via a 
null sink (because it’s not possible to connect a sink input to a source
 output directly).</p>
  <img src="PulseAudio%20under%20the%20hood_files/native_app_to_app.png" width="681px">
<p>Configuration:</p>
<ol>
<li>
<p>Enable Zeroconf publishing on the remote server using the paprefs tool or the following commands:</p>
<pre><code> $ (start avahi daemon)
 $ pactl load-module module-native-protocol-tcp auth-anonymous=1
 $ pactl load-module module-zeroconf-publish
</code></pre>
</li>
<li>
<p>Enable Zeroconf discovery on the local server using the paprefs tool or the following commands:</p>
<pre><code> $ (start avahi daemon)
 $ pactl load-module module-zeroconf-discover
</code></pre>
</li>
<li>
<p>Wait until the tunnel sinks and sources appear on the local server.</p>
</li>
<li>
<p>Start the playback application on the local server:</p>
<pre><code> $ paplay input.wav
</code></pre>
</li>
<li>
<p>Start the recording application on the remote server:</p>
<pre><code> $ parecord output.wav
</code></pre>
</li>
<li>
<p>Create the null sink on the remote server:</p>
<pre><code> $ pactl load-module module-null-sink
</code></pre>
</li>
<li>
<p>Connect the local sink input to the tunnel sink using the pavucontrol tool on the local server.</p>
</li>
<li>
<p>Connect the remote sink input to the null sink using the pavucontrol tool on the remote server.</p>
</li>
<li>
<p>Connect the remote source output to the sink monitor of the null sink using the pavucontrol tool on the remote server.</p>
</li>
</ol>
</li>
<li>
<p><strong>Connect a local sink to a remote sink</strong></p>
<p>The monitor source of a sink is connected to a loopback, which in 
turn is connected to a tunnel sink (because it’s not possible to connect
 a source to a sink directly). The tunnel sink connects to the remote 
server via the “native” protocol and creates a playback stream. The 
remote server creates a sink input for the stream. The remote sink input
 is connected to a sink.</p>
  <img src="PulseAudio%20under%20the%20hood_files/native_sink_to_sink.png" width="681px">
<p>Configuration:</p>
<ol>
<li>
<p>Enable Zeroconf publishing on the remote server using the paprefs tool or the following commands:</p>
<pre><code> $ (start avahi daemon)
 $ pactl load-module module-native-protocol-tcp auth-anonymous=1
 $ pactl load-module module-zeroconf-publish
</code></pre>
</li>
<li>
<p>Enable Zeroconf discovery on the local server using the paprefs tool or the following commands:</p>
<pre><code> $ (start avahi daemon)
 $ pactl load-module module-zeroconf-discover
</code></pre>
</li>
<li>
<p>Wait until the tunnel sinks and sources appear on the local server.</p>
</li>
<li>
<p>Create the loopback on the local server:</p>
<pre><code> $ pactl load-module module-loopback
</code></pre>
</li>
<li>
<p>Connect the loopback sink input to the tunnel sink using the pavucontrol tool.</p>
</li>
<li>
<p>Connect the loopback source output to the sink monitor of a sink using the pavucontrol tool.</p>
</li>
<li>
<p>Connect the remote sink input to the sink using the pavucontrol tool on the remote server.</p>
</li>
</ol>
</li>
</ul>
<h2 id="rtp">
  RTP
  <a class="anchor" href="#rtp">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<ul>
<li>
<p><strong>Connect a local playback application to a remote sink</strong></p>
<p>An application connects to the local server via the “native” protocol
 and creates a playback stream. The local server creates a sink input 
for the stream. The local sink input is connected to the RTP source 
output via a null sink (because it’s not possible to connect a sink 
input to a source output directly). The local RTP source output is 
connected to the remote RTP sink input via RTP. The remote RTP sink 
input is connected to a sink.</p>
  <img src="PulseAudio%20under%20the%20hood_files/rtp_app_to_sink.png" width="681px">
<p>Configuration:</p>
<ol>
<li>
<p>Enable RTP receiver on the remote server using the paprefs tool or the following command:</p>
<pre><code> $ pactl load-module module-rtp-recv sap_address=&lt;IP&gt;
</code></pre>
</li>
<li>
<p>Enable RTP sender on the local server using the paprefs tool or the following command:</p>
<pre><code> $ pactl load-module module-rtp-send destionation_ip=&lt;IP&gt;
</code></pre>
</li>
<li>
<p>Create the null sink on the local server:</p>
<pre><code> $ pactl load-module module-null-sink
</code></pre>
</li>
<li>
<p>Connect the local sink input to the null sink using the pavucontrol tool on the local server.</p>
</li>
<li>
<p>Connect the sink monitor of the null sink to the local RTP source output using the pavucontrol tool on the local server.</p>
</li>
<li>
<p>Connect the remote RTP sink input to a sink using the pavucontrol tool on the remote server.</p>
</li>
</ol>
</li>
<li>
<p><strong>Connect a local playback application to a remote recording application</strong></p>
<p>The local application connects to the local server via the “native” 
protocol and creates a playback stream. The local server creates a sink 
input for the stream. The remote application connects to the remote 
server via the “native” protocol and creates a recording stream. The 
remote server creates a source output for the stream.</p>
<p>The local sink input is connected to the RTP source output via a null
 sink (because it’s not possible to connect a sink input to a source 
output directly). The local RTP source output is connected to the remote
 RTP sink input via RTP. The remote RTP sink input is connected to the 
remote source output via a null sink (because it’s not possible to 
connect a sink input to a source output directly).</p>
  <img src="PulseAudio%20under%20the%20hood_files/rtp_app_to_app.png" width="681px">
<p>Configuration:</p>
<ol>
<li>
<p>Enable RTP receiver on the remote server using the paprefs tool or the following command:</p>
<pre><code> $ pactl load-module module-rtp-recv sap_address=&lt;IP&gt;
</code></pre>
</li>
<li>
<p>Enable RTP sender on the local server using the paprefs tool or the following command:</p>
<pre><code> $ pactl load-module module-rtp-send destionation_ip=&lt;IP&gt;
</code></pre>
</li>
<li>
<p>Create the null sink on the local server:</p>
<pre><code> $ pactl load-module module-null-sink
</code></pre>
</li>
<li>
<p>Create the null sink on the remote server:</p>
<pre><code> $ pactl load-module module-null-sink
</code></pre>
</li>
<li>
<p>Connect the local sink input to the null sink using the pavucontrol tool on the local server.</p>
</li>
<li>
<p>Connect the sink monitor of the null sink to the local RTP source output using the pavucontrol tool on the local server.</p>
</li>
<li>
<p>Connect the remote RTP sink input to the null sink using the pavucontrol tool on the remote server.</p>
</li>
<li>
<p>Connect the remote source output to the sink monitor of the null sink using the pavucontrol tool on the remote server.</p>
</li>
</ol>
</li>
<li>
<p><strong>Connect a local sink to a remote sink</strong></p>
<p>The sink monitor of a local sink is connected to the local RTP source
 output. The local RTP source output is connected to the remote RTP sink
 input via RTP. The remote RTP sink input is connected to a sink.</p>
  <img src="PulseAudio%20under%20the%20hood_files/rtp_sink_to_sink.png" width="409px">
<p>Configuration:</p>
<ol>
<li>
<p>Enable RTP receiver on the remote server using the paprefs tool or the following command:</p>
<pre><code> $ pactl load-module module-rtp-recv sap_address=&lt;IP&gt;
</code></pre>
</li>
<li>
<p>Enable RTP sender on the local server using the paprefs tool or the following command:</p>
<pre><code> $ pactl load-module module-rtp-send destionation_ip=&lt;IP&gt;
</code></pre>
</li>
<li>
<p>Connect the sink monitor of a sink to the local RTP source output using the pavucontrol tool on the local server.</p>
</li>
<li>
<p>Connect the remote RTP sink input to a sink using the pavucontrol tool on the remote server.</p>
</li>
</ol>
</li>
</ul>
<hr>
<h1 id="example-clients-and-modules">
  Example clients and modules
  <a class="anchor" href="#example-clients-and-modules">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>This section provides several examples of client applications and 
server modules. The source code and usage instructions are available <a href="https://github.com/gavv/snippets/tree/master/pa">on GitHub</a>.</p>
<p>Also, some analysis of the client examples is available in <a href="https://gavv.net/articles/decode-play/">this post</a>.</p>
<h2 id="documentation">
  Documentation
  <a class="anchor" href="#documentation">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>The following official documentation is available:</p>
<ul>
<li>
<p>Overview:</p>
<ul>
<li><a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/">Clients</a> - application developer documentation</li>
<li><a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/">Developer</a> - module developer documentation</li>
</ul>
</li>
<li>
<p>API documentation:</p>
<ul>
<li><a href="https://freedesktop.org/software/pulseaudio/doxygen/">C API</a></li>
<li><a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/DBus/">D-Bus API</a></li>
</ul>
</li>
<li>
<p>Client examples:</p>
<ul>
<li><a href="https://freedesktop.org/software/pulseaudio/doxygen/examples.html">Doxygen</a></li>
<li><a href="https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/Developer/Clients/Samples/">Wiki</a></li>
</ul>
</li>
<li>
<p>Module examples:</p>
<ul>
<li><a href="https://github.com/pulseaudio/pulseaudio/blob/master/src/modules/module-virtual-source.c">module-virtual-source</a></li>
<li><a href="https://github.com/pulseaudio/pulseaudio/blob/master/src/modules/module-virtual-sink.c">module-virtual-sink</a></li>
</ul>
</li>
</ul>
<h2 id="d-bus-api-1">
  D-Bus API
  <a class="anchor" href="#d-bus-api-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>This example is quite straightforward, so just look at the code.</p>
<ul>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_dbus_print.py"><strong><code>pa_dbus_print</code></strong></a></p>
<p>Python3 script that prints various server-side objects using the D-Bus API.</p>
</li>
</ul>
<h2 id="c-api-1">
  C API
  <a class="anchor" href="#c-api-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>These examples are described in details in another <a href="https://gavv.net/articles/decode-play/#pulseaudio">article</a>.</p>
<ul>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_play_simple.c"><strong><code>pa_play_simple</code></strong></a></p>
<p>Minimal playback client using the Simple API.</p>
</li>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_play_async_cb.c"><strong><code>pa_play_async_cb</code></strong></a></p>
<p>Playback client using the Asynchronous API, based on callbacks.</p>
</li>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_play_async_poll.c"><strong><code>pa_play_async_poll</code></strong></a></p>
<p>Playback client using the Asynchronous API, based on polling.</p>
</li>
</ul>
<h2 id="modules-1">
  Modules
  <a class="anchor" href="#modules-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>These examples have comments in the source code.</p>
<ul>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_module_source.c"><strong><code>pa_module_source</code></strong></a></p>
<p>This module implements a source. The module runs a thread that takes 
care of the timing and periodically generates samples and writes them to
 the connected source outputs.</p>
</li>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_module_source_output.c"><strong><code>pa_module_source_output</code></strong></a></p>
<p>This module implements a source output. The module provides callbacks invoked when a source generates more samples.</p>
</li>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_module_sink.c"><strong><code>pa_module_sink</code></strong></a></p>
<p>This module implements a sink. The module runs a thread that takes 
care of the timing and periodically requests samples from the connected 
sink inputs.</p>
</li>
<li>
<p><a href="https://github.com/gavv/snippets/blob/master/pa/pa_module_sink_input.c"><strong><code>pa_module_sink_input</code></strong></a></p>
<p>This module implements a sink input. The module provides callbacks invoked when a sink needs more samples.</p>
</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="http://brainyfeed.blogspot.ru/2012/06/part-i-writing-simple-module.html">Writing a simple PulseAudio module.</a></li>
</ul>
<hr>
<h1 id="critique">
  Critique
  <a class="anchor" href="#critique">
    <i class="fa fa-hashtag"></i>
  </a>
</h1>
<p>Finally, I’d like to discuss some problems in the PulseAudio design 
and implementation that I’ve gathered while writing this document.</p>
<p>We’ll discuss only problems that are essential but yet solvable and 
could be avoided while still providing the same functionality to the 
user.</p>
<p>We <em>won’t</em> discuss several kinds of issues:</p>
<ul>
<li>
<p>The rationale for implementing or not implementing some features in a
 sound server. An ideal feature set of a sound server deserves a deeper 
analysis than I can provide here.</p>
</li>
<li>
<p>Fundamental costs of the provided features that are unavoidable. It’s
 up to the user to decide whether to pay them or not, depending on the 
requirements and available alternatives.</p>
</li>
<li>
<p>Bugs and limitations of the existing code that can be just fixed at some point. There is a bug tracker for such things.</p>
</li>
</ul>
<h2 id="documentation-1">
  Documentation
  <a class="anchor" href="#documentation-1">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>A comprehensive documentation is a starting point for detecting 
problems and improving things. PulseAudio has a good documentation for 
public interfaces and troubleshooting. However, the following official 
documentation is partially or completely missing:</p>
<ul>
<li>a detailed feature overview</li>
<li>a design overview</li>
<li>a detailed description of the key abstractions and their relations</li>
<li>a high-level description of the key algorithms</li>
<li>a high-level description of the protocols</li>
<li>a detailed documentation for internal APIs</li>
<li>a rationale</li>
</ul>
<h2 id="abstraction-level">
  Abstraction level
  <a class="anchor" href="#abstraction-level">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>PulseAudio is built around the four fundamental object types: sources, sinks, source outputs, and sink inputs.</p>
<p>These object types are low-level enough. They operate with streams of
 samples. It’s up to the implementation how to handle the samples: write
 them to a sound card, send them over the network, perform sound 
processing or whatever else.</p>
<p>The good thing with this approach is the flexibility it gives. All of
 the numerous PulseAudio features are implemented in terms of the four 
objects. The problem, however, is that the features implemented on top 
of them are much higher level.</p>
<p>Because of this distance, modules often need to implement some 
intermediate layers. Similar modules need similar intermediate layers. 
However, the lack of the appropriate core abstractions causes problems 
with code reuse and consistency.</p>
<p>Here are some candidates of the high-level abstractions that are currently missing:</p>
<ul>
<li>
<p><strong>publishing and discovery</strong></p>
<p>PulseAudio implements Zeroconf publishing and discovery for 
PulseAudio servers, SAP/SDP publishing and discovery for RTP, Zeroconf 
discovery for AirPlay, Udev hotplugging for ALSA cards, and BlueZ 
hotplugging for Bluetooth devices.</p>
<p>All of these mechanisms are implemented independently and are bound 
to a concrete type of transport or device. It’s not possible to reuse 
existing publishing and discovery alone. For example, it’s not possible 
to write a module that improves the RTP support without reimplementing 
SAP/SDP support or reuse the Zeroconf discovery for an alternative 
transport.</p>
</li>
<li>
<p><strong>transport</strong></p>
<p>PulseAudio implements various transport protocols, including the 
“native” protocol, RTP, RAOP, HTTP, and several Bluetooth transports.</p>
<p>Conceptually, a transport could implement just encoding and I/O. 
Bluetooth transports are close to this. RTP transport is implemented as a
 source output and sink input, which is a bit more complicated. The 
“native” protocol and RAOP transports are implemented as a source and 
sink, which is the most complicated.</p>
<p>Every network transport is implemented from scratch. A network source
 or sink should start an IO thread, run the rtpoll loop, implement 
clocking, handle asynchronous events, etc. It’s not possible to reuse 
this code for a new transport.</p>
</li>
<li>
<p><strong>protocol extension</strong></p>
<p>PulseAudio is an extensible server, and it supports several 
extensible protocols: the D-Bus API, the “native” protocol, and the RTP.</p>
<p>The D-Bus API is a good example. It provides an abstraction of the protocol extension which may be registered by a module.</p>
<p>Modules can also implement custom commands for the “native” protocol.
 However, the core does not provide an explicit abstraction of the 
“native” protocol extension. All modules that implement custom commands 
are hardcoded in the protocol implementation. It’s not possible to add a
 new extension without modifying the core.</p>
<p>The RTP is designed to be an extremely extensible protocol. However, 
PulseAudio doesn’t support RTP extensions. It’s not possible to add 
support for a new payload type or add forward error correction support 
to the RTP transport.</p>
</li>
<li>
<p><strong>filter</strong></p>
<p>PulseAudio supports several filters, like channel remapping and echo cancellation.</p>
<p>All filter modules follow the same convention. They accept the same arguments and create a pair of virtual device and stream.</p>
<p>Every filter module is implemented from scratch. It should parse the 
module arguments, start an IO thread, run the rtpoll loop, implement 
clocking, handle asynchronous events, etc. It’s not possible to reuse 
this code for a new filter.</p>
<p>Although it’s possible to implement filters as LADSPA plugins, all filters available out of the box don’t use this possibility.</p>
</li>
<li>
<p><strong>priority list</strong></p>
<p>PulseAudio implements an extensible routing algorithm, which is spread across several modules.</p>
<p>Every routing step is implemented in a separate module. These modules
 are quite isolated because the core doesn’t provide a generic routing 
abstraction. Every module just installs a hook that tries to route a 
stream in its own way if it wasn’t routed by another module yet. The 
routing rules are implemented independently in every module, which may 
lead to inconsistency.</p>
<p>This problem is addressed by the <a href="https://www.freedesktop.org/wiki/Software/PulseAudio/RFC/PriorityRouting/">PriorityRouting</a> proposal which is currently not implemented.</p>
</li>
<li>
<p><strong>scheduler</strong></p>
<p>PulseAudio implements an advanced timer-based scheduler for ALSA 
device. The scheduler is implemented inside the ALSA source and sink.</p>
<p>There are two problems. First, the implementation is duplicated in 
the source and in the sink. Second, the implementation is pretty 
complicated, and mixing it with the source or sink housekeeping makes 
things even more complicated. It would be much simpler to understand and
 improve it if it was a standalone component.</p>
</li>
</ul>
<h2 id="mechanism-and-policy">
  Mechanism and policy
  <a class="anchor" href="#mechanism-and-policy">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Lack of the appropriate high-level abstractions leads to violation of the <a href="https://en.wikipedia.org/wiki/Separation_of_mechanism_and_policy">separation of mechanism and policy</a> principle.</p>
<p>If a mechanism is not encapsulated by an abstraction that is generic 
enough, its implementation tends to be merged with the concrete policy 
or a set of policies. It makes it hard to modify, replace, or reuse 
mechanism and policy independently.</p>
<p>When this happens, and several modules need the same mechanism, two scenarios are possible:</p>
<ul>
<li>
<p>The implementation of the mechanism may be reimplemented in every 
module, but a bit differently, fitted for the concrete policy that the 
module needs.</p>
<p>In this case, we get increased code duplication. We also get an 
increased coupling between modules, because cooperating modules rely on 
the concrete policies implemented in other modules, instead of a generic
 mechanism implemented in core.</p>
<p>This happened with network transports, sound processing filters, and routing modules.</p>
</li>
<li>
<p>The implementation of the mechanism together with the implementation of all necessary policies may be moved to the core.</p>
<p>In this case, we get defective modularity. We also get an increased 
coupling between modules and core, because the modules rely on the 
concrete policies implemented in core, instead of a generic mechanism.</p>
<p>This happened with the network protocols and protocol extensions. 
Actually, the modularity is only an illusion in this particular case, 
because the “native”, the “simple”, the CLI, the HTTP, and the ESound 
protocol modules are just thin wrappers that use the functionality 
implemented completely in the core.</p>
</li>
</ul>
<h2 id="code-quality">
  Code quality
  <a class="anchor" href="#code-quality">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are some usual problems with the code quality, that could be resolved by introducing stricter code style guidelines:</p>
<ul>
<li>
<p>Complicated sharing of responsibilities between the “base” part of an
 object from the core and the “derived” part from a module, which 
recursively calls each other.</p>
</li>
<li>
<p>Complicated sharing of an object state between threads and 
non-obvious jumps from one thread to another. Different methods of the 
same object are called on different threads and use different subsets of
 the object fields.</p>
</li>
<li>
<p>Wide and coupled internal interfaces. Maybe it’s just me, but it took
 me about a week to figure it out what’s going on in the ALSA Mixer and 
UCM related code.</p>
</li>
<li>
<p>Mixing low-level code like memory management and string manipulation 
with the high-level logic, which becomes hard to extract and understand.</p>
</li>
<li>
<p>Mixing generic utilities and domain-specific components in core. 
Utilities are boring and vast, and it would be helpful to separate them 
from the really important code.</p>
</li>
<li>
<p>Handmade serialization for the network protocols and restoration database, handmade formatters and parsers.</p>
</li>
<li>
<p>Custom implementation of the collections, event loops, threading 
primitives, and platform wrappers, instead of using a general purpose 
library like Glib, which is an acceptable dependency on the desktop.</p>
</li>
<li>
<p>Long function bodies, short variable names, reusing the same variable for several purposes, <code>#ifdef</code> madness in some modules.</p>
</li>
</ul>
<h2 id="service-quality">
  Service quality
  <a class="anchor" href="#service-quality">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>Two PulseAudio servers can be connected either using the “native” 
protocol or using RTP. Both implementations are not suited for 
unreliable networks like WiFi:</p>
<ul>
<li>
<p>the “native” protocol is based on TCP, and packet losses cause playback delays</p>
</li>
<li>
<p>the implementation of RTP sender and receiver in PulseAudio doesn’t 
employ any RTP extensions for error correction or retransmission, and 
packet losses cause playback holes</p>
</li>
</ul>
<p>This problem is addressed by the <a href="https://roc-streaming.org/">Roc Toolkit</a> project that I’m currently <a href="https://gavv.net/articles/new-network-transport/">working on</a>. A tutorial is <a href="https://gavv.net/articles/roc-tutorial/">available here</a>.</p>
<h2 id="usability">
  Usability
  <a class="anchor" href="#usability">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>There are several sources of confusion for users:</p>
<ul>
<li>
<p>Building transport chains is non-intuitive. The user just wants to 
select the source, destination, and transport. However, things are not 
that simple.</p>
<p>On the one hand, different transports are implemented using different
 object types. For example, the “native” protocol uses tunnel sources 
and sinks, but the RTP uses RTP source outputs and sink inputs. On the 
other hand, only certain object types may be connected directly.</p>
<p>Hence, in some cases, the user can just connect a device or a stream 
to a transport, while in other cases the user has to configure tricky 
adapters like the loopback and null sink.</p>
</li>
<li>
<p>Building sound processing chains is non-intuitive. The user just 
wants to apply one or several filters to a stream or device. But things 
are not that simple.</p>
<p>Every filter module creates a pair of a virtual device and stream. 
The user has to find both in the list of available devices and streams 
and connect the filter device to some stream and the filter stream to 
some device. When multiple filter chains are employed, the configuration
 becomes totally confusing.</p>
</li>
<li>
<p>Routing is non-intuitive. The user wants to select a device either 
for a stream or for a category of streams. Again, things are not that 
simple.</p>
<p>There are two independent routing rule databases. The fist one 
(module-stream-restore) is used when the user moves a single stream, but
 the server may move either a single stream or a category of streams, 
depending on the stream properties provided by the application.</p>
<p>The second one (module-device-manager) is used when the user 
configures preferred devices for a category of streams. However, this 
routing rules may be overridden by conflicting rules from the first 
database. Only KDE provides a GUI for this second database.</p>
</li>
<li>
<p>The autospawn feature is just weird. The user wants to kill the 
server, but the server is magically restarted by any background client. 
By the way, if the user disables autospawn, the server will not be 
automatically restarted after a crash, which still happens from time to 
time.</p>
</li>
</ul>
<h2 id="final-thoughts">
  Final thoughts
  <a class="anchor" href="#final-thoughts">
    <i class="fa fa-hashtag"></i>
  </a>
</h2>
<p>This section lists some downsides, but there are upsides too:</p>
<ul>
<li>
<p>With a few exceptions mentioned above, the module system is done well.</p>
</li>
<li>
<p>At the high level, the inter-thread communication is done well. 
Mutexes are rare, threads use event loops and exchange asynchronous 
messages.</p>
</li>
<li>
<p>The memory management is done well. It is based on pools and chunks and is flexible enough to support the zero-copy mode.</p>
</li>
<li>
<p>The function contracts are carefully covered with assertions.</p>
</li>
<li>
<p>There is a plenty of advanced and well-engineered features, including
 the timer-based scheduler, buffer rewinding, clocking, and latency 
management. There is a lot to learn from the implementation.</p>
</li>
<li>
<p>Finally, PulseAudio just works in many cases.</p>
</li>
</ul>
<p>Most of the problems listed in this section are not trivial but may 
be resolved with a thoughtful refactoring. In this regard, it’s worth 
mentioning some reasons to contribute to PulseAudio:</p>
<ul>
<li>
<p>The project solves real problems that are complex and interesting, so it offers a challenge.</p>
</li>
<li>
<p>The project employs or implements many different technologies, from 
various device backends to sound processing tools and scheduling, so 
there is something to learn from.</p>
</li>
<li>
<p>The project is already used on many Linux desktops, so it’s practical.</p>
</li>
</ul>
<p>Thanks for reading, and happy hacking!</p>

      </section>
    </article>
  </section>
  
    
  <footer id="footer">
    <div id="disqus_recommendations" style="margin-bottom: 12px;"><iframe id="dsq-app7692" name="dsq-app7692" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" style="width: 100% !important; border: medium !important; overflow: hidden !important; height: 271px !important; display: inline !important; box-sizing: border-box !important;" width="100%" src="PulseAudio%20under%20the%20hood_files/a_003.html" horizontalscrolling="no" verticalscrolling="no"></iframe></div><div id="disqus_thread" class="no-print"><iframe id="dsq-app6726" name="dsq-app6726" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" style="width: 1px !important; min-width: 100% !important; border: medium !important; overflow: hidden !important; height: 7019px !important;" width="100%" src="PulseAudio%20under%20the%20hood_files/a_004.html" horizontalscrolling="no" verticalscrolling="no"></iframe></div>
    <script>
      (function() {
        var d = document, s = d.createElement('script');
        s.src = '//gavv.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Enable JavaScript to view the
        <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>
  </footer>


  

      </div>
    <script defer="defer" src="PulseAudio%20under%20the%20hood_files/vcd15cbe7772f49c399c6a5babf22c1241717689176015.js" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon="{&quot;rayId&quot;:&quot;94d0f234ae1ebc2f&quot;,&quot;version&quot;:&quot;2025.5.0&quot;,&quot;r&quot;:1,&quot;token&quot;:&quot;5d0eb33de0fd427fa52d196ce203f394&quot;,&quot;serverTiming&quot;:{&quot;name&quot;:{&quot;cfExtPri&quot;:true,&quot;cfEdge&quot;:true,&quot;cfOrigin&quot;:true,&quot;cfL4&quot;:true,&quot;cfSpeedBrain&quot;:true,&quot;cfCacheStatus&quot;:true}}}" crossorigin="anonymous"></script>

  

<div id="lightboxOverlay" class="lightboxOverlay" style="display: none;"></div><div id="lightbox" class="lightbox" style="display: none;"><div class="lb-outerContainer"><div class="lb-container"><img class="lb-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAP///wAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw=="><div class="lb-nav"><a class="lb-prev" href=""></a><a class="lb-next" href=""></a></div><div class="lb-loader"><a class="lb-cancel"></a></div></div></div><div class="lb-dataContainer"><div class="lb-data"><div class="lb-details"><span class="lb-caption"></span><span class="lb-number"></span></div><div class="lb-closeContainer"><a class="lb-close"></a></div></div></div></div><iframe style="display: none;"></iframe><iframe style="display: none;"></iframe></body></html>